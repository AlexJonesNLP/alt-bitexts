{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook written and maintained by Alex Jones (alexander.g.jones.23@dartmouth.edu, alexjones1925@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For mining sentence pairs from comparable corpora for the purpose of training NMT systems (see https://github.com/AlexJonesNLP/alt-bitexts/tree/main/ComparableCorporaMaterials for data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### May be used for replicating NMT results from paper \"An Alternative to Thresholding for Margin-Based Bitext Mining\" (Alex Jones and Derry Tanti Wijaya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for margin-based similarity search based on LASER implementation by Facebook AI: https://github.com/facebookresearch/LASER/blob/master/source/mine_bitex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See https://github.com/facebookresearch/LASER for copyright and licensing specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import faiss\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from bs4 import BeautifulSoup\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer(\n",
       "    (auto_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(501153, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Pooling()\n",
       "  (2): Dense(\n",
       "    (activation_function): Tanh()\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (3): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'LaBSE'\n",
    "sentence_model = SentenceTransformer(model_name)\n",
    "sentence_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for embedding function\n",
    "def embed(sentences:list):\n",
    "    global i\n",
    "    i+=1\n",
    "    if i%1000==0:\n",
    "        print(\"Finished document {}\".format(i))\n",
    "    return sentence_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the GPU with FAISS\n",
    "GPU = faiss.StandardGpuResources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Params\n",
    "******\n",
    "src_emb: array of size number_of_source_sentences X embedding_dimension\n",
    "tgt_emb: array of size number_of_target_sentences X embedding_dimension\n",
    "k: number of neighbors to return\n",
    "batch_size: batch size\n",
    "\n",
    "Returns\n",
    "*******\n",
    "cos_sims: cosine similarity scores for each of k nearest neighbors for each source sentence\n",
    "inds: target indices of k nearest neighbors for each source sentence\n",
    "\n",
    "Modeled off of LASER source code: https://github.com/facebookresearch/LASER/blob/master/source/mine_bitexts.py\n",
    "\n",
    "'''\n",
    "\n",
    "def knnSearch(src_emb, tgt_emb, k=1, batch_size=1):\n",
    "    emb_dim = src_emb.shape[1] # Embedding dimension\n",
    "    num_src_sents = src_emb.shape[0]\n",
    "    num_tgt_sents = tgt_emb.shape[0]\n",
    "    cos_sims = np.zeros((num_src_sents, k), dtype=np.float32)\n",
    "    inds = np.zeros((num_src_sents, k), dtype=np.int64)\n",
    "    for s_min in range(0, num_src_sents, batch_size):\n",
    "        s_max = min(s_min + batch_size, num_src_sents)\n",
    "        src_sims = []\n",
    "        src_inds = []\n",
    "        for t_min in range(0, num_tgt_sents, batch_size):\n",
    "            t_max = min(t_min + batch_size, num_tgt_sents)\n",
    "            idx = faiss.IndexFlatIP(emb_dim)\n",
    "            idx = faiss.index_cpu_to_gpu(GPU, 0, idx)\n",
    "            idx.add(tgt_emb[t_min : t_max])\n",
    "            src_sim, src_ind = idx.search(src_emb[s_min : s_max], min(k, t_max-t_min))\n",
    "            src_sims.append(src_sim)\n",
    "            src_inds.append(src_ind + t_min)\n",
    "            del idx\n",
    "        src_sims = np.concatenate(src_sims, axis=1)\n",
    "        src_inds = np.concatenate(src_inds, axis=1)\n",
    "        sorted_inds = np.argsort(-src_sims, axis=1)\n",
    "        for i in range(s_min, s_max):\n",
    "            for j in range(k):\n",
    "                cos_sims[i, j] = src_sims[i-s_min, sorted_inds[i-s_min, j]]\n",
    "                inds[i, j] = src_inds[i-s_min, sorted_inds[i-s_min, j]]\n",
    "    return cos_sims, inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves k-nearest neighbor indices and similarity means for margin scoring\n",
    "# If forward: finds neearest neighbors and indices for all source sentences\n",
    "# If backward: finds nearest neighbors and indices for all target sentences\n",
    "# In the approach implemented in our paper, we perform both forward and backward search\n",
    "\n",
    "def directedMeansAndInds(src_emb, tgt_emb, forward=False, backward=False, k=1, batch_size=1):\n",
    "    assert forward != backward, \"Please choose either forward or backward\"\n",
    "    if forward:\n",
    "        cos_sims, inds = knnSearch(src_emb, tgt_emb, min(tgt_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds\n",
    "    elif backward:\n",
    "        cos_sims, inds = knnSearch(tgt_emb, src_emb, min(src_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Params\n",
    "******\n",
    "src_embs: array of size number_of_source_sentences X embedding_dimension\n",
    "tgt_embs: array of size number_of_source_sentences X embedding_dimension\n",
    "batch_size: batch size\n",
    "num_neighbors: number of neighbors\n",
    "\n",
    "Returns\n",
    "*******\n",
    "concat_pairs: list of mined sentence pairs\n",
    "margin_scores: list of scores corresponding to mined pairs\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def mineSentencePairs(src_embs, tgt_embs, batch_size=100, num_neighbors=4):\n",
    "\n",
    "    # Retrieve means and indices in the forward direction . . .\n",
    "    fwd_means, fwd_inds = directedMeansAndInds(src_embs, tgt_embs, forward=True, k=num_neighbors, batch_size=batch_size)\n",
    "    # . . . and in the backward direction\n",
    "    bwd_means, bwd_inds = directedMeansAndInds(src_embs, tgt_embs, backward=True, k=num_neighbors, batch_size=batch_size)\n",
    "\n",
    "    fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "    for i in range(fwd_inds.shape[0]):\n",
    "        for j in range(fwd_inds.shape[1]):\n",
    "            tgt_ind = fwd_inds[i,j]\n",
    "            # Compute ratio margin score between each source sentence and each of its k-nearest neighbors\n",
    "            margin_score = (src_embs[i].dot(tgt_embs[tgt_ind])) / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "            # Store the result\n",
    "            fwd_margin_scores[i,j] = margin_score\n",
    "    \n",
    "    # We will store the source index, target index, and margin score for the best\n",
    "    # pairs found using forward search\n",
    "    best = np.zeros((fwd_inds.shape[0], 3))\n",
    "    # Take pair that maximizes margin score for each source sentence\n",
    "    best_inds = fwd_inds[np.arange(src_embs.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(fwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "        best[i] = ((i+1, best_inds[i]+1, best_score)) # Assumption is that GROUND TRUTH VALUES ARE 1-INDEXED!!!\n",
    "\n",
    "    # Repeat process in backward direction (finding matches in source text for target sentences)\n",
    "    bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "    for i in range(bwd_inds.shape[0]):\n",
    "        for j in range(bwd_inds.shape[1]):\n",
    "            tgt_ind = bwd_inds[i,j]\n",
    "            margin_score = (tgt_embs[i].dot(src_embs[tgt_ind])) / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "            bwd_margin_scores[i,j] = margin_score\n",
    "            \n",
    "    bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "    best_inds = bwd_inds[np.arange(tgt_embs.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(bwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "        bwd_best[i] = ((best_inds[i]+1, i+1, best_score))\n",
    "    \n",
    "    # Best triples (src_idx, tgt_idx, margin_score) from forward/backward searches\n",
    "    fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "    bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "    pairs_and_scores = []\n",
    "    # Take INTERSECTION of forward and backward searches\n",
    "    pairs_and_scores = list(set(fwd_best) & set(bwd_best))\n",
    "\n",
    "    pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "    concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores] # Store indices only\n",
    "    concat_pairs_int = []\n",
    "    for tup in concat_pairs:\n",
    "        concat_pairs_int.append((int(tup[0]), int(tup[1]))) # Ground-truth indices are ints, so change type\n",
    "    concat_pairs = concat_pairs_int\n",
    "\n",
    "    margin_scores = [triplet[2] for triplet in pairs_and_scores] # Store scores only\n",
    "                                    \n",
    "    return concat_pairs, margin_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in en-kk and en-gu comparable corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where you stored the comparable corpora files\n",
    "PATH = '/project/statnlp/ajones'\n",
    "\n",
    "# Original English sentences\n",
    "orig_en_kk = open('{}/orig.en-kk'.format(PATH), 'r').readlines()\n",
    "# Translated English sentences\n",
    "transl_en_kk = open('{}/translation.en-kk'.format(PATH), 'r').readlines()\n",
    "# English doc IDs\n",
    "en_doc_ids_1 = open('{}/id.en-kk'.format(PATH), 'r').readlines()\n",
    "\n",
    "# Original Kazakh sentences\n",
    "orig_kk_en = open('{}/orig.kk-en'.format(PATH), 'r').readlines()\n",
    "# Translated Kazakh sentences\n",
    "transl_kk_en = open('{}/translation.kk-en'.format(PATH), 'r').readlines()\n",
    "# Gujarati doc IDs\n",
    "kk_doc_ids = open('{}/id.kk-en'.format(PATH), 'r').readlines()\n",
    "    \n",
    "\n",
    "    \n",
    "# Original English sentences\n",
    "orig_en_gu = open('{}/orig.en-gu'.format(PATH), 'r').readlines()\n",
    "# Translated English sentences\n",
    "transl_en_gu = open('{}/translation.en-gu'.format(PATH), 'r').readlines()\n",
    "# English doc IDs\n",
    "en_doc_ids_2 = open('{}/id.en-gu'.format(PATH), 'r').readlines()\n",
    "\n",
    "# Original Gujarati sentences\n",
    "orig_gu_en = open('{}/orig.gu-en'.format(PATH), 'r').readlines()\n",
    "# Translated Gujarati sentences\n",
    "transl_gu_en = open('{}/translation.gu-en'.format(PATH), 'r').readlines()\n",
    "# Gujarati doc IDs\n",
    "gu_doc_ids = open('{}/id.gu-en'.format(PATH), 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting document IDs from strings to integers and getting rid of \n",
    "# new characters\n",
    "\n",
    "for i in range(len(en_doc_ids_1)):\n",
    "    en_doc_ids_1[i] = literal_eval(en_doc_ids_1[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(kk_doc_ids)):\n",
    "    kk_doc_ids[i] = literal_eval(kk_doc_ids[i].replace('\\n', ''))\n",
    "    \n",
    "for i in range(len(en_doc_ids_2)):\n",
    "    en_doc_ids_2[i] = literal_eval(en_doc_ids_2[i].replace('\\n', ''))\n",
    "    \n",
    "for i in range(len(gu_doc_ids)):\n",
    "    gu_doc_ids[i] = literal_eval(gu_doc_ids[i].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty lists of lists in which to group sentence by document\n",
    "# in order to prepare for document-level mining\n",
    "\n",
    "orig_en_sents_1 = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "orig_kk_sents = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "transl_en_sents_1 = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "transl_kk_sents = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "\n",
    "orig_en_sents_2 = [[] for _ in range(max(en_doc_ids_2)+1)]\n",
    "orig_gu_sents = [[] for _ in range(max(en_doc_ids_2)+1)]\n",
    "transl_en_sents_2 = [[] for _ in range(max(en_doc_ids_2)+1)]\n",
    "transl_gu_sents = [[] for _ in range(max(en_doc_ids_2)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping sentences into their original documents and removing newline characters\n",
    "\n",
    "for i in range(len(orig_en_kk)):\n",
    "    doc_idx = en_doc_ids_1[i]\n",
    "    orig_en_sents_1[doc_idx].append(orig_en_kk[i].replace('\\n', ''))\n",
    "    transl_en_sents_1[doc_idx].append(transl_en_kk[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(orig_kk_en)):\n",
    "    doc_idx = kk_doc_ids[i]\n",
    "    orig_kk_sents[doc_idx].append(orig_kk_en[i].replace('\\n', ''))\n",
    "    transl_kk_sents[doc_idx].append(transl_kk_en[i].replace('\\n', ''))\n",
    "    \n",
    "for i in range(len(orig_en_gu)):\n",
    "    doc_idx = en_doc_ids_2[i]\n",
    "    orig_en_sents_2[doc_idx].append(orig_en_gu[i].replace('\\n', ''))\n",
    "    transl_en_sents_2[doc_idx].append(transl_en_gu[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(orig_gu_en)):\n",
    "    doc_idx = gu_doc_ids[i]\n",
    "    orig_gu_sents[doc_idx].append(orig_gu_en[i].replace('\\n', ''))\n",
    "    transl_gu_sents[doc_idx].append(transl_gu_en[i].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying especially short documents in each corpus\n",
    "\n",
    "bad_en_idx_1 = [i for i in range(len(orig_en_sents_1)) if len(orig_en_sents_1[i])<30]\n",
    "bad_kk_idx = [i for i in range(len(orig_kk_sents)) if len(orig_kk_sents[i])<8]\n",
    "\n",
    "bad_en_idx_2 = [i for i in range(len(orig_en_sents_2)) if len(orig_en_sents_2[i])<21]\n",
    "bad_gu_idx = [i for i in range(len(orig_gu_sents)) if len(orig_gu_sents[i])<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extremely short documents from all corpora\n",
    "\n",
    "new_orig_en_sents_1, new_orig_kk_sents, new_transl_en_sents_1, new_transl_kk_sents = [],[],[],[]\n",
    "for i in range(len(orig_en_sents_1)):\n",
    "    if i not in bad_en_idx_1 and i not in bad_kk_idx:\n",
    "        new_orig_en_sents_1.append(orig_en_sents_1[i])\n",
    "        new_orig_kk_sents.append(orig_kk_sents[i])\n",
    "        new_transl_en_sents_1.append(transl_en_sents_1[i])\n",
    "        new_transl_kk_sents.append(transl_kk_sents[i])\n",
    "\n",
    "new_orig_en_sents_2, new_orig_gu_sents, new_transl_en_sents_2, new_transl_gu_sents = [],[],[],[]\n",
    "for i in range(len(orig_en_sents_2)):\n",
    "    if i not in bad_en_idx_2 and i not in bad_gu_idx:\n",
    "        new_orig_en_sents_2.append(orig_en_sents_2[i])\n",
    "        new_orig_gu_sents.append(orig_gu_sents[i])\n",
    "        new_transl_en_sents_2.append(transl_en_sents_2[i])\n",
    "        new_transl_gu_sents.append(transl_gu_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing variable names back to originals\n",
    "orig_en_sents_1, orig_kk_sents, transl_en_sents_1, transl_kk_sents = new_orig_en_sents_1, new_orig_kk_sents, new_transl_en_sents_1, new_transl_kk_sents\n",
    "orig_en_sents_2, orig_gu_sents, transl_en_sents_2, transl_gu_sents = new_orig_en_sents_2, new_orig_gu_sents, new_transl_en_sents_2, new_transl_gu_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding all sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "orig_en_embs_1 = [[embed(doc)] for doc in orig_en_sents_1]\n",
    "print(\"Finished orig_en_embs\")\n",
    "orig_kk_embs = [[embed(doc)] for doc in orig_kk_sents]\n",
    "print(\"Finished orig_kk_embs\")\n",
    "transl_en_embs_1 = [[embed(doc)] for doc in transl_en_sents_1]\n",
    "print(\"Finished transl_en_embs\")\n",
    "transl_kk_embs = [[embed(doc)] for doc in transl_kk_sents]\n",
    "print(\"Finished transl_kk_embs\")\n",
    "\n",
    "orig_en_embs_2 = [[embed(doc)] for doc in orig_en_sents_2]\n",
    "print(\"Finished orig_en_embs\")\n",
    "orig_gu_embs = [[embed(doc)] for doc in orig_gu_sents]\n",
    "print(\"Finished orig_gu_embs\")\n",
    "transl_en_embs_2 = [[embed(doc)] for doc in transl_en_sents_2]\n",
    "print(\"Finished transl_en_embs\")\n",
    "transl_gu_embs = [[embed(doc)] for doc in transl_gu_sents]\n",
    "print(\"Finished transl_gu_embs\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total embedding time: {:} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings in the correct format for similarity search\n",
    "\n",
    "for i in range(len(orig_en_embs_1)):\n",
    "    orig_en_embs_1[i], orig_kk_embs[i], transl_en_embs_1[i], transl_kk_embs[i] = np.asarray(orig_en_embs_1[i]), np.asarray(orig_kk_embs[i]), np.asarray(transl_en_embs_1[i]), np.asarray(transl_kk_embs[i])\n",
    "for i in range(len(orig_en_embs_1)):\n",
    "    orig_en_embs_1[i], orig_kk_embs[i], transl_en_embs_1[i], transl_kk_embs[i] = orig_en_embs_1[i][0,:,:], orig_kk_embs[i][0,:,:], transl_en_embs_1[i][0,:,:], transl_kk_embs[i][0,:,:]\n",
    "    \n",
    "for i in range(len(orig_en_embs_2)):\n",
    "    orig_en_embs_2[i], orig_gu_embs[i], transl_en_embs_2[i], transl_gu_embs[i] = np.asarray(orig_en_embs_2[i]), np.asarray(orig_gu_embs[i]), np.asarray(transl_en_embs_2[i]), np.asarray(transl_gu_embs[i])\n",
    "for i in range(len(orig_en_embs_2)):\n",
    "    orig_en_embs_2[i], orig_gu_embs[i], transl_en_embs_2[i], transl_gu_embs[i] = orig_en_embs_2[i][0,:,:], orig_gu_embs[i][0,:,:], transl_en_embs_2[i][0,:,:], transl_gu_embs[i][0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + original kk sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_kk_orig, all_margin_scores_en_kk_orig = [], []\n",
    "# Mine on a document level, rather than globally\n",
    "for en_doc, kk_doc in zip(orig_en_embs_1, orig_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, kk_doc)\n",
    "    all_sent_pairs_en_kk_orig.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_kk_orig.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method-1.06.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method-1.20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method-1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN_THRESHOLD = 1.06 # Set this to whichever margin you want to use\n",
    "orig_en_kk_pairs = []\n",
    "for i in range(len(all_sent_pairs_en_kk_orig)):\n",
    "    doc = all_sent_pairs_en_kk_orig[i]\n",
    "    for j in range(len(doc)):\n",
    "        if all_margin_scores_en_kk_orig[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "            kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "            orig_en_kk_pairs.append((en_sent, kk_sent))\n",
    "\n",
    "en_sents_1, kk_sents = [pair[0] for pair in orig_en_kk_pairs], [pair[1] for pair in orig_en_kk_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method_top20k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pairs_with_docidx_ms = []\n",
    "for i in range(len(all_sent_pairs_en_kk_orig)):\n",
    "    doc_pairs = []\n",
    "    for j in range(len(all_sent_pairs_en_kk_orig[i])):\n",
    "        doc_pairs.append((all_sent_pairs_en_kk_orig[i][j][0], all_sent_pairs_en_kk_orig[i][j][1], \n",
    "                         i, all_margin_scores_en_kk_orig[i][j]))\n",
    "    sent_pairs_with_docidx_ms.append(doc_pairs)\n",
    "    \n",
    "four_tuples = []\n",
    "for doc in sent_pairs_with_docidx_ms:\n",
    "    for tup in doc:\n",
    "        four_tuples.append(tup)\n",
    "        \n",
    "sorted_by_margins = sorted(four_tuples, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "top20k_en_sents = [orig_en_sents_1[tup[2]][tup[0]-1] for tup in top20k]\n",
    "top20k_kk_sents = [orig_kk_sents[tup[2]][tup[0]-1] for tup in top20k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_orig_pairs_1.06.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_orig_pairs_1.20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_orig_pairs_1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + original gu sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_gu_orig, all_margin_scores_en_gu_orig = [], []\n",
    "# Mine on a document level, rather than globally\n",
    "for en_doc, gu_doc in zip(orig_en_embs_2, orig_gu_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_2)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, gu_doc)\n",
    "    all_sent_pairs_en_gu_orig.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_gu_orig.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN_THRESHOLD = 1.06 # Set this to whichever margin you want to use\n",
    "orig_en_gu_pairs = []\n",
    "for i in range(len(all_sent_pairs_en_gu_orig)):\n",
    "    doc = all_sent_pairs_en_gu_orig[i]\n",
    "    for j in range(len(doc)):\n",
    "        if all_margin_scores_en_gu_orig[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_2[i][pair[0]-1]\n",
    "            gu_sent = orig_gu_sents[i][pair[1]-1]\n",
    "            orig_en_gu_pairs.append((en_sent, gu_sent))\n",
    "\n",
    "en_sents_2, gu_sents = [pair[0] for pair in orig_en_gu_pairs], [pair[1] for pair in orig_en_gu_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_kk_to_en_1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + translated kk sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_kk_to_en, all_margin_scores_kk_to_en = [], []\n",
    "for en_doc, transl_kk_doc in zip(orig_en_embs_1, transl_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, transl_kk_doc)\n",
    "    all_sent_pairs_kk_to_en.append(doc_sent_pairs)\n",
    "    all_margin_scores_kk_to_en.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN_THRESHOLD = 1.35 # Set this to whichever margin you want to use\n",
    "kk_to_en_pairs = []\n",
    "for i in range(len(all_sent_pairs_kk_to_en)):\n",
    "    doc = all_sent_pairs_kk_to_en[i]\n",
    "    for j in range(len(doc)):\n",
    "        if all_margin_scores_kk_to_en[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "            kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "            kk_to_en_pairs.append((en_sent, kk_sent))\n",
    "\n",
    "en_sents_3, kk_sents_2 = [pair[0] for pair in kk_to_en_pairs], [pair[1] for pair in kk_to_en_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_majority_vote_ALL.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_majority_vote_ALL_1.20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_majority_vote_ALL_1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original kk + translated en sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_to_kk, all_margin_scores_en_to_kk = [], []\n",
    "for transl_en_doc, kk_doc in zip(transl_en_embs_1, orig_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(transl_en_doc, kk_doc)\n",
    "    all_sent_pairs_en_to_kk.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_to_kk.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing majority vote retrieval\n",
    "\n",
    "mv_pairs_1, mv_margins_1 = [], []\n",
    "for i in range(len(all_sent_pairs_en_kk_orig)):\n",
    "    \n",
    "    # Grabbing all three sets of sentence pairs:\n",
    "    # 1. orig en + orig kk\n",
    "    # 2. orig en + translated kk\n",
    "    # 3. translated en + orig kk\n",
    "    doc1, doc2, doc3 = all_sent_pairs_en_kk_orig[i], all_sent_pairs_en_to_kk[i], all_sent_pairs_kk_to_en[i]\n",
    "    \n",
    "    # Taking pairwise intersections of these sets\n",
    "    int1, int2, int3 = set(doc1)&set(doc2), set(doc1)&set(doc3), set(doc2)&set(doc3)\n",
    "    # Taking union of pairwise intersections (voting step)\n",
    "    pairwise_int = list(set(int1|int2|int3))\n",
    "    \n",
    "    for j in range(len(pairwise_int)):\n",
    "        pair = pairwise_int[j]\n",
    "        en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "        kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "        mv_pairs_1.append((en_sent, kk_sent))\n",
    "        \n",
    "        # Storing margin scores associated with sentence pairs\n",
    "        if pair in doc1:\n",
    "            idx = all_sent_pairs_en_kk_orig[i].index(pair)\n",
    "            mv_margins_1.append(all_margin_scores_en_kk_orig[i][idx])\n",
    "        elif pair in doc2:\n",
    "            idx = all_sent_pairs_en_to_kk[i].index(pair)\n",
    "            mv_margins_1.append(all_margin_scores_en_to_kk[i][idx])\n",
    "        else:\n",
    "            idx = all_sent_pairs_kk_to_en[i].index(pair)\n",
    "            mv_margins_1.append(all_margin_scores_kk_to_en[i][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional threshold\n",
    "\n",
    "MARGIN_THRESHOLD = 1.06 # Set this to whichever margin you want to use\n",
    "new_mv_pairs = []\n",
    "for i in range(len(mv_pairs_1)):\n",
    "    doc = mv_pairs_1[i]\n",
    "    for j in range(len(doc)):\n",
    "        if mv_margins_1[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "            kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "            new_mv_pairs.append((en_sent, kk_sent))\n",
    "\n",
    "en_sents_4, kk_sents_3 = [pair[0] for pair in new_mv_pairs], [pair[1] for pair in new_mv_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_pairs_majority_vote.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + translated gu sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_gu_to_en, all_margin_scores_gu_to_en = [], []\n",
    "for en_doc, transl_gu_doc in zip(orig_en_embs_2, transl_gu_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_2)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, transl_gu_doc)\n",
    "    all_sent_pairs_gu_to_en.append(doc_sent_pairs)\n",
    "    all_margin_scores_gu_to_en.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + translated gu sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_to_gu, all_margin_scores_en_to_gu = [], []\n",
    "for transl_en_doc, gu_doc in zip(transl_en_embs_2, orig_gu_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_2)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(transl_en_doc, gu_doc)\n",
    "    all_sent_pairs_en_to_gu.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_to_gu.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_pairs_2, mv_margins_2 = [], []\n",
    "for i in range(len(all_sent_pairs_en_gu_orig)):\n",
    "    doc1, doc2, doc3 = all_sent_pairs_en_gu_orig[i], all_sent_pairs_en_to_gu[i], all_sent_pairs_gu_to_en[i]\n",
    "    int1, int2, int3 = set(doc1)&set(doc2), set(doc1)&set(doc3), set(doc2)&set(doc3)\n",
    "    pairwise_int = list(set(int1|int2|int3))\n",
    "    for j in range(len(pairwise_int)):\n",
    "        pair = pairwise_int[j]\n",
    "        en_sent = orig_en_sents_2[i][pair[0]-1]\n",
    "        gu_sent = orig_gu_sents[i][pair[1]-1]\n",
    "        mv_pairs_2.append((en_sent, gu_sent))\n",
    "        \n",
    "        if pair in doc1:\n",
    "            idx = all_sent_pairs_en_gu_orig[i].index(pair)\n",
    "            mv_margins_2.append(all_margin_scores_en_gu_orig[i][idx])\n",
    "        elif pair in doc2:\n",
    "            idx = all_sent_pairs_en_to_gu[i].index(pair)\n",
    "            mv_margins_2.append(all_margin_scores_en_to_gu[i][idx])\n",
    "        else:\n",
    "            idx = all_sent_pairs_gu_to_en[i].index(pair)\n",
    "            mv_margins_2.append(all_margin_scores_gu_to_en[i][idx])\n",
    "\n",
    "en_sents_5, gu_sents_2 = [pair[0] for pair in mv_pairs_2], [pair[1] for pair in mv_pairs_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2, majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where you stored the comparable corpora files\n",
    "PATH = '/project/statnlp/ajones'\n",
    "\n",
    "# Original English sentences\n",
    "orig_en_kk_iter1 = open('{}/orig-iter1.en-kk'.format(PATH), 'r').readlines()\n",
    "# Translated English sentences\n",
    "transl_en_kk_iter1 = open('{}/translation-iter1.en-kk'.format(PATH), 'r').readlines()\n",
    "# English doc IDs\n",
    "en_doc_ids_1_iter1 = open('{}/id-iter1.en-kk'.format(PATH), 'r').readlines()\n",
    "\n",
    "# Original Kazakh sentences\n",
    "orig_kk_en_iter1 = open('{}/orig-iter1.kk-en'.format(PATH), 'r').readlines()\n",
    "# Translated Kazakh sentences\n",
    "transl_kk_en_iter1 = open('{}/translation-iter1.kk-en'.format(PATH), 'r').readlines()\n",
    "# Gujarati doc IDs\n",
    "kk_doc_ids_iter1 = open('{}/id-iter1.kk-en'.format(PATH), 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(en_doc_ids_1_iter1)):\n",
    "    en_doc_ids_1_iter1[i] = literal_eval(en_doc_ids_1_iter1[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(kk_doc_ids_iter1)):\n",
    "    kk_doc_ids_iter1[i] = literal_eval(kk_doc_ids_iter1[i].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_en_sents_1_iter1 = [[] for _ in range(max(en_doc_ids_1_iter1)+1)]\n",
    "orig_kk_sents_iter1 = [[] for _ in range(max(en_doc_ids_1_iter1)+1)]\n",
    "transl_en_sents_1_iter1 = [[] for _ in range(max(en_doc_ids_1_iter1)+1)]\n",
    "transl_kk_sents_iter1 = [[] for _ in range(max(en_doc_ids_1_iter1)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping sentences into their original documents and removing newline characters\n",
    "\n",
    "for i in range(len(orig_en_kk_iter1)):\n",
    "    doc_idx = en_doc_ids_1_iter1[i]\n",
    "    orig_en_sents_1_iter1[doc_idx].append(orig_en_kk_iter1[i].replace('\\n', ''))\n",
    "    transl_en_sents_1_iter1[doc_idx].append(transl_en_kk_iter1[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(orig_kk_en_iter1)):\n",
    "    doc_idx = kk_doc_ids_iter1[i]\n",
    "    orig_kk_sents_iter1[doc_idx].append(orig_kk_en_iter1[i].replace('\\n', ''))\n",
    "    transl_kk_sents_iter1[doc_idx].append(transl_kk_en_iter1[i].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_en_idx_1_iter1 = [i for i in range(len(orig_en_sents_1_iter1)) if len(orig_en_sents_1_iter1[i])<30]\n",
    "bad_kk_idx_iter1 = [i for i in range(len(orig_kk_sents_iter1)) if len(orig_kk_sents_iter1[i])<8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extremely short documents from all corpora\n",
    "\n",
    "new_orig_en_sents_1_iter1, new_orig_kk_sents_iter1, new_transl_en_sents_1_iter1, new_transl_kk_sents_iter1 = [],[],[],[]\n",
    "for i in range(len(orig_en_sents_1_iter1)):\n",
    "    if i not in bad_en_idx_1_iter1 and i not in bad_kk_idx_iter1:\n",
    "        new_orig_en_sents_1_iter1.append(orig_en_sents_1_iter1[i])\n",
    "        new_orig_kk_sents_iter1.append(orig_kk_sents_iter1[i])\n",
    "        new_transl_en_sents_1_iter1.append(transl_en_sents_1_iter1[i])\n",
    "        new_transl_kk_sents_iter1.append(transl_kk_sents_iter1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing variable names back to originals\n",
    "orig_en_sents_1_iter1, orig_kk_sents_iter1, transl_en_sents_1_iter1, transl_kk_sents_iter1 = new_orig_en_sents_1_iter1, new_orig_kk_sents_iter1, new_transl_en_sents_1_iter1, new_transl_kk_sents_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14662\n",
      "Finished document 1000\n",
      "Finished document 2000\n",
      "Finished document 3000\n",
      "Finished document 4000\n",
      "Finished document 5000\n",
      "Finished document 6000\n",
      "Finished document 7000\n",
      "Finished document 8000\n",
      "Finished document 9000\n",
      "Finished document 10000\n",
      "Finished document 11000\n",
      "Finished document 12000\n",
      "Finished document 13000\n",
      "Finished document 14000\n",
      "Finished orig_en_embs\n",
      "Finished document 1000\n",
      "Finished document 2000\n",
      "Finished document 3000\n",
      "Finished document 4000\n"
     ]
    }
   ],
   "source": [
    "# Embedding all sentences\n",
    "\n",
    "i = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(len(orig_en_sents_1_iter1))\n",
    "orig_en_embs_1 = [[embed(doc)] for doc in orig_en_sents_1_iter1]\n",
    "print(\"Finished orig_en_embs\")\n",
    "\n",
    "i = 0\n",
    "orig_kk_embs = [[embed(doc)] for doc in orig_kk_sents_iter1]\n",
    "print(\"Finished orig_kk_embs\")\n",
    "\n",
    "i = 0\n",
    "transl_en_embs_1 = [[embed(doc)] for doc in transl_en_sents_1_iter1]\n",
    "print(\"Finished transl_en_embs\")\n",
    "\n",
    "i = 0\n",
    "transl_kk_embs = [[embed(doc)] for doc in transl_kk_sents_iter1]\n",
    "print(\"Finished transl_kk_embs\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total embedding time: {:} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(orig_en_embs_1)):\n",
    "    orig_en_embs_1[i], orig_kk_embs[i], transl_en_embs_1[i], transl_kk_embs[i] = np.asarray(orig_en_embs_1[i]), np.asarray(orig_kk_embs[i]), np.asarray(transl_en_embs_1[i]), np.asarray(transl_kk_embs[i])\n",
    "for i in range(len(orig_en_embs_1)):\n",
    "    orig_en_embs_1[i], orig_kk_embs[i], transl_en_embs_1[i], transl_kk_embs[i] = orig_en_embs_1[i][0,:,:], orig_kk_embs[i][0,:,:], transl_en_embs_1[i][0,:,:], transl_kk_embs[i][0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed document 1000 of 14662\n",
      "Completed document 2000 of 14662\n",
      "Completed document 3000 of 14662\n",
      "Completed document 4000 of 14662\n",
      "Completed document 5000 of 14662\n",
      "Completed document 6000 of 14662\n",
      "Completed document 7000 of 14662\n",
      "Completed document 8000 of 14662\n",
      "Completed document 9000 of 14662\n",
      "Completed document 10000 of 14662\n",
      "Completed document 11000 of 14662\n",
      "Completed document 12000 of 14662\n",
      "Completed document 13000 of 14662\n",
      "Completed document 14000 of 14662\n",
      "188.85500073432922\n"
     ]
    }
   ],
   "source": [
    "# Mining using original en + original kk sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_kk_orig_iter1, all_margin_scores_en_kk_orig_iter1 = [], []\n",
    "# Mine on a document level, rather than globally\n",
    "for en_doc, kk_doc in zip(orig_en_embs_1, orig_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1_iter1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, kk_doc)\n",
    "    all_sent_pairs_en_kk_orig_iter1.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_kk_orig_iter1.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed document 1000 of 14662\n",
      "Completed document 2000 of 14662\n",
      "Completed document 3000 of 14662\n",
      "Completed document 4000 of 14662\n",
      "Completed document 5000 of 14662\n",
      "Completed document 6000 of 14662\n",
      "Completed document 7000 of 14662\n",
      "Completed document 8000 of 14662\n",
      "Completed document 9000 of 14662\n",
      "Completed document 10000 of 14662\n",
      "Completed document 11000 of 14662\n",
      "Completed document 12000 of 14662\n",
      "Completed document 13000 of 14662\n",
      "Completed document 14000 of 14662\n",
      "188.7188708782196\n"
     ]
    }
   ],
   "source": [
    "# Mining using original en + translated kk sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_kk_to_en_iter1, all_margin_scores_kk_to_en_iter1 = [], []\n",
    "for en_doc, transl_kk_doc in zip(orig_en_embs_1, transl_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1_iter1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, transl_kk_doc)\n",
    "    all_sent_pairs_kk_to_en_iter1.append(doc_sent_pairs)\n",
    "    all_margin_scores_kk_to_en_iter1.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed document 1000 of 14662\n",
      "Completed document 2000 of 14662\n",
      "Completed document 3000 of 14662\n",
      "Completed document 4000 of 14662\n",
      "Completed document 5000 of 14662\n",
      "Completed document 6000 of 14662\n",
      "Completed document 7000 of 14662\n",
      "Completed document 8000 of 14662\n",
      "Completed document 9000 of 14662\n",
      "Completed document 10000 of 14662\n",
      "Completed document 11000 of 14662\n",
      "Completed document 12000 of 14662\n",
      "Completed document 13000 of 14662\n",
      "Completed document 14000 of 14662\n",
      "189.28361177444458\n"
     ]
    }
   ],
   "source": [
    "# Mining using original kk + translated en sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_to_kk_iter1, all_margin_scores_en_to_kk_iter1 = [], []\n",
    "for transl_en_doc, kk_doc in zip(transl_en_embs_1, orig_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1_iter1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(transl_en_doc, kk_doc)\n",
    "    all_sent_pairs_en_to_kk_iter1.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_to_kk_iter1.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing majority vote retrieval\n",
    "\n",
    "mv_pairs_3, mv_margins_3 = [], []\n",
    "for i in range(len(all_sent_pairs_en_kk_orig_iter1)):\n",
    "    \n",
    "    # Grabbing all three sets of sentence pairs:\n",
    "    # 1. orig en + orig kk\n",
    "    # 2. orig en + translated kk\n",
    "    # 3. translated en + orig kk\n",
    "    doc1, doc2, doc3 = all_sent_pairs_en_kk_orig_iter1[i], all_sent_pairs_en_to_kk_iter1[i], all_sent_pairs_kk_to_en_iter1[i]\n",
    "    \n",
    "    # Taking pairwise intersections of these sets\n",
    "    int1, int2, int3 = set(doc1)&set(doc2), set(doc1)&set(doc3), set(doc2)&set(doc3)\n",
    "    # Taking union of pairwise intersections (voting step)\n",
    "    pairwise_int = list(set(int1|int2|int3))\n",
    "    \n",
    "    for j in range(len(pairwise_int)):\n",
    "        pair = pairwise_int[j]\n",
    "        en_sent = orig_en_sents_1_iter1[i][pair[0]-1]\n",
    "        kk_sent = orig_kk_sents_iter1[i][pair[1]-1]\n",
    "        mv_pairs_3.append((en_sent, kk_sent))\n",
    "        \n",
    "        # Storing margin scores associated with sentence pairs\n",
    "        if pair in doc1:\n",
    "            idx = all_sent_pairs_en_kk_orig_iter1[i].index(pair)\n",
    "            mv_margins_3.append(all_margin_scores_en_kk_orig_iter1[i][idx])\n",
    "        elif pair in doc2:\n",
    "            idx = all_sent_pairs_en_to_kk_iter1[i].index(pair)\n",
    "            mv_margins_3.append(all_margin_scores_en_to_kk_iter1[i][idx])\n",
    "        else:\n",
    "            idx = all_sent_pairs_kk_to_en_iter1[i].index(pair)\n",
    "            mv_margins_3.append(all_margin_scores_kk_to_en_iter1[i][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = [pair[0] for pair in mv_pairs_3]\n",
    "kk_sents = [pair[1] for pair in mv_pairs_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'source': en_sents, 'target': kk_sents}).to_csv('Data/en_kk_mined_majority_vote_iter2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

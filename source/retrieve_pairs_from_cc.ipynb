{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook written and maintained by Alex Jones (alexander.g.jones.23@dartmouth.edu, alexjones1925@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For mining sentence pairs from comparable corpora for the purpose of training NMT systems (see https://github.com/AlexJonesNLP/alt-bitexts/tree/main/ComparableCorporaMaterials for data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### May be used for replicating NMT results from paper \"An Alternative to Thresholding for Margin-Based Bitext Mining\" (Alex Jones and Derry Tanti Wijaya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for margin-based similarity search based on LASER implementation by Facebook AI: https://github.com/facebookresearch/LASER/blob/master/source/mine_bitex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See https://github.com/facebookresearch/LASER for copyright and licensing specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import faiss\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Checks for GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0a467aefc89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LaBSE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m             raise RuntimeError(\n\u001b[1;32m    185\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             raise AssertionError(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "model_name = 'LaBSE'\n",
    "sentence_model = SentenceTransformer(model_name)\n",
    "sentence_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for embedding function\n",
    "def sentence_embed(sentences:list):\n",
    "    return sentence_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the GPU with FAISS\n",
    "GPU = faiss.StandardGpuResources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Params\n",
    "******\n",
    "src_emb: array of size number_of_source_sentences X embedding_dimension\n",
    "tgt_emb: array of size number_of_target_sentences X embedding_dimension\n",
    "k: number of neighbors to return\n",
    "batch_size: batch size\n",
    "\n",
    "Returns\n",
    "*******\n",
    "cos_sims: cosine similarity scores for each of k nearest neighbors for each source sentence\n",
    "inds: target indices of k nearest neighbors for each source sentence\n",
    "\n",
    "Modeled off of LASER source code: https://github.com/facebookresearch/LASER/blob/master/source/mine_bitexts.py\n",
    "\n",
    "'''\n",
    "\n",
    "def knnSearch(src_emb, tgt_emb, k=1, batch_size=1):\n",
    "    emb_dim = src_emb.shape[1] # Embedding dimension\n",
    "    num_src_sents = src_emb.shape[0]\n",
    "    num_tgt_sents = tgt_emb.shape[0]\n",
    "    cos_sims = np.zeros((num_src_sents, k), dtype=np.float32)\n",
    "    inds = np.zeros((num_src_sents, k), dtype=np.int64)\n",
    "    for s_min in range(0, num_src_sents, batch_size):\n",
    "        s_max = min(s_min + batch_size, num_src_sents)\n",
    "        src_sims = []\n",
    "        src_inds = []\n",
    "        for t_min in range(0, num_tgt_sents, batch_size):\n",
    "            t_max = min(t_min + batch_size, num_tgt_sents)\n",
    "            idx = faiss.IndexFlatIP(emb_dim)\n",
    "            idx = faiss.index_cpu_to_gpu(GPU, 0, idx)\n",
    "            idx.add(tgt_emb[t_min : t_max])\n",
    "            src_sim, src_ind = idx.search(src_emb[s_min : s_max], min(k, t_max-t_min))\n",
    "            src_sims.append(src_sim)\n",
    "            src_inds.append(src_ind + t_min)\n",
    "            del idx\n",
    "        src_sims = np.concatenate(src_sims, axis=1)\n",
    "        src_inds = np.concatenate(src_inds, axis=1)\n",
    "        sorted_inds = np.argsort(-src_sims, axis=1)\n",
    "        for i in range(s_min, s_max):\n",
    "            for j in range(k):\n",
    "                cos_sims[i, j] = src_sims[i-s_min, sorted_inds[i-s_min, j]]\n",
    "                inds[i, j] = src_inds[i-s_min, sorted_inds[i-s_min, j]]\n",
    "    return cos_sims, inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves k-nearest neighbor indices and similarity means for margin scoring\n",
    "# If forward: finds neearest neighbors and indices for all source sentences\n",
    "# If backward: finds nearest neighbors and indices for all target sentences\n",
    "# In the approach implemented in our paper, we perform both forward and backward search\n",
    "\n",
    "def directedMeansAndInds(src_emb, tgt_emb, forward=False, backward=False, k=1, batch_size=1):\n",
    "    assert forward != backward, \"Please choose either forward or backward\"\n",
    "    if forward:\n",
    "        cos_sims, inds = knnSearch(src_emb, tgt_emb, min(tgt_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds\n",
    "    elif backward:\n",
    "        cos_sims, inds = knnSearch(tgt_emb, src_emb, min(src_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Params\n",
    "******\n",
    "pred_tuples: predicted sentence pairs\n",
    "gold_tuples: ground-truth sentence pairs\n",
    "\n",
    "Returns\n",
    "*******\n",
    "Unweighted F1, precision, recall\n",
    "\n",
    "'''\n",
    "\n",
    "def computeF1(pred_tuples, gold_tuples):\n",
    "    tp = 0 # true positives\n",
    "    fp = 0 # false positives\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    epsilon = 1e-8 # To prevent division by zero\n",
    "    for pair in pred_tuples:\n",
    "        if pair in gold_tuples:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1 \n",
    "    prec = tp / (len(pred_tuples) + epsilon)\n",
    "    rec = tp / len(gold_tuples)\n",
    "    f1 = 2*prec*rec / (prec+rec+epsilon)\n",
    "    return f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Params\n",
    "******\n",
    "src_embs: array of size number_of_source_sentences X embedding_dimension\n",
    "tgt_embs: array of size number_of_source_sentences X embedding_dimension\n",
    "batch_size: batch size\n",
    "num_neighbors: number of neighbors\n",
    "\n",
    "Returns\n",
    "*******\n",
    "concat_pairs: list of mined sentence pairs\n",
    "margin_scores: list of scores corresponding to mined pairs\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def mineSentencePairs(src_embs, tgt_embs, batch_size=100, num_neighbors=4):\n",
    "\n",
    "    # Retrieve means and indices in forward direction . . .\n",
    "    fwd_means, fwd_inds = directedMeansAndInds(src_embs, tgt_embs, forward=True, k=num_neighbors, batch_size=batch_size)\n",
    "    # . . . and in backward direction\n",
    "    bwd_means, bwd_inds = directedMeansAndInds(src_embs, tgt_embs, backward=True, k=num_neighbors, batch_size=batch_size)\n",
    "\n",
    "    fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "    for i in range(fwd_inds.shape[0]):\n",
    "        for j in range(fwd_inds.shape[1]):\n",
    "            tgt_ind = fwd_inds[i,j]\n",
    "            # Compute ratio margin score between each source sentence and each of its k-nearest neighbors\n",
    "            margin_score = (src_embs[i].dot(tgt_embs[tgt_ind])) / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "            # Store the result\n",
    "            fwd_margin_scores[i,j] = margin_score\n",
    "    \n",
    "    # We will store the source index, target index, and margin score for the best\n",
    "    # pairs found using forward search\n",
    "    best = np.zeros((fwd_inds.shape[0], 3))\n",
    "    # Take pair that maximizes margin score for each source sentence\n",
    "    best_inds = fwd_inds[np.arange(src_embs.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(fwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "        best[i] = ((i+1, best_inds[i]+1, best_score)) # Assumption is that GROUND TRUTH VALUES ARE 1-INDEXED!!!\n",
    "\n",
    "    # Repeat process in backward direction (finding matches in source text for target sentences)\n",
    "    bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "    for i in range(bwd_inds.shape[0]):\n",
    "        for j in range(bwd_inds.shape[1]):\n",
    "            tgt_ind = bwd_inds[i,j]\n",
    "            margin_score = (tgt_embs[i].dot(src_embs[tgt_ind])) / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "            bwd_margin_scores[i,j] = margin_score\n",
    "            \n",
    "    bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "    best_inds = bwd_inds[np.arange(tgt_embs.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(bwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "        bwd_best[i] = ((best_inds[i]+1, i+1, best_score))\n",
    "    \n",
    "    # Best triples (src_idx, tgt_idx, margin_score) from forward/backward searches\n",
    "    fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "    bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "    pairs_and_scores = []\n",
    "    # Take INTERSECTION of forward and backward searches\n",
    "    pairs_and_scores = list(set(fwd_best) & set(bwd_best))\n",
    "\n",
    "    pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "    concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores] # Store indices only\n",
    "    concat_pairs_int = []\n",
    "    for tup in concat_pairs:\n",
    "        concat_pairs_int.append((int(tup[0]), int(tup[1]))) # Ground-truth indices are ints, so change type\n",
    "    concat_pairs = concat_pairs_int\n",
    "\n",
    "    margin_scores = [triplet[2] for triplet in pairs_and_scores] # Store scores only\n",
    "                                    \n",
    "    return concat_pairs, margin_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in en-kk and en-gu comparable corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where you stored the comparable corpora files\n",
    "PATH = '/project/statnlp/ajones'\n",
    "\n",
    "# Original English sentences\n",
    "orig_en_kk = open('{}/orig.en-kk'.format(PATH), 'r').readlines()\n",
    "# Translated English sentences\n",
    "transl_en_kk = open('{}/translation.en-kk'.format(PATH), 'r').readlines()\n",
    "# English doc IDs\n",
    "en_doc_ids_1 = open('{}/id.en-kk'.format(PATH), 'r').readlines()\n",
    "\n",
    "# Original Kazakh sentences\n",
    "orig_kk_en = open('{}/orig.gu-kk'.format(PATH), 'r').readlines()\n",
    "# Translated Kazakh sentences\n",
    "transl_kk_en = open('{}/translation.gu-kk'.format(PATH), 'r').readlines()\n",
    "# Gujarati doc IDs\n",
    "kk_doc_ids = open('{}/id.kk-en'.format(PATH), 'r').readlines()\n",
    "    \n",
    "\n",
    "    \n",
    "# Original English sentences\n",
    "orig_en_gu = open('{}/orig.en-gu'.format(PATH), 'r').readlines()\n",
    "# Translated English sentences\n",
    "transl_en_gu = open('{}/translation.en-gu'.format(PATH), 'r').readlines()\n",
    "# English doc IDs\n",
    "en_doc_ids_2 = open('{}/id.en-gu'.format(PATH), 'r').readlines()\n",
    "\n",
    "# Original Gujarati sentences\n",
    "orig_gu_en = open('{}/orig.gu-en'.format(PATH), 'r').readlines()\n",
    "# Translated Gujarati sentences\n",
    "transl_gu_en = open('{}/translation.gu-en'.format(PATH), 'r').readlines()\n",
    "# Gujarati doc IDs\n",
    "gu_doc_ids = open('{}/id.gu-en'.format(PATH), 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting document IDs from strings to integers and getting rid of \n",
    "# new characters\n",
    "\n",
    "for i in range(len(en_doc_ids_1)):\n",
    "    en_doc_ids_1[i] = literal_eval(en_doc_ids_1[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(kk_doc_ids)):\n",
    "    kk_doc_ids[i] = literal_eval(kk_doc_ids[i].replace('\\n', ''))\n",
    "    \n",
    "for i in range(len(en_doc_ids_2)):\n",
    "    en_doc_ids_2[i] = literal_eval(en_doc_ids_2[i].replace('\\n', ''))\n",
    "    \n",
    "for i in range(len(gu_doc_ids)):\n",
    "    gu_doc_ids[i] = literal_eval(gu_doc_ids[i].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty lists of lists in which to group sentence by document\n",
    "# in order to prepare for document-level mining\n",
    "\n",
    "orig_en_sents_1 = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "orig_kk_sents = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "transl_en_sents_1 = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "transl_kk_sents = [[] for _ in range(max(en_doc_ids_1)+1)]\n",
    "\n",
    "orig_en_sents_2 = [[] for _ in range(max(en_doc_ids_2)+1)]\n",
    "orig_gu_sents = [[] for _ in range(max(en_doc_ids_2)+1)]\n",
    "transl_en_sents_2 = [[] for _ in range(max(en_doc_ids_2)+1)]\n",
    "transl_gu_sents = [[] for _ in range(max(en_doc_ids_2)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping sentences into their original documents and removing newline characters\n",
    "\n",
    "for i in range(len(orig_en_kk)):\n",
    "    doc_idx = en_doc_ids_1[i]\n",
    "    orig_en_sents_1[doc_idx].append(orig_en_kk[i].replace('\\n', ''))\n",
    "    transl_en_sents_1[doc_idx].append(transl_en_kk[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(orig_kk_en)):\n",
    "    doc_idx = kk_doc_ids[i]\n",
    "    orig_kk_sents[doc_idx].append(orig_kk_en[i].replace('\\n', ''))\n",
    "    transl_kk_sents[doc_idx].append(transl_kk_en[i].replace('\\n', ''))\n",
    "    \n",
    "for i in range(len(orig_en_gu)):\n",
    "    doc_idx = en_doc_ids_2[i]\n",
    "    orig_en_sents_2[doc_idx].append(orig_en_gu[i].replace('\\n', ''))\n",
    "    transl_en_sents_2[doc_idx].append(transl_en_gu[i].replace('\\n', ''))\n",
    "\n",
    "for i in range(len(orig_gu_en)):\n",
    "    doc_idx = gu_doc_ids[i]\n",
    "    orig_gu_sents[doc_idx].append(orig_gu_en[i].replace('\\n', ''))\n",
    "    transl_gu_sents[doc_idx].append(transl_gu_en[i].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying especially short documents in each corpus\n",
    "\n",
    "bad_en_idx_1 = [i for i in range(len(orig_en_sents_1)) if len(orig_en_sents[i])<30]\n",
    "bad_kk_idx = [i for i in range(len(orig_kk_sents)) if len(orig_kk_sents[i])<8]\n",
    "\n",
    "bad_en_idx_2 = [i for i in range(len(orig_en_sents_2)) if len(orig_en_sents_2[i])<21]\n",
    "bad_gu_idx = [i for i in range(len(orig_gu_sents)) if len(orig_gu_sents[i])<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extremely short documents from all corpora\n",
    "\n",
    "new_orig_en_sents_1, new_orig_kk_sents, new_transl_en_sents_1, new_transl_kk_sents = [],[],[],[]\n",
    "for i in range(len(orig_en_sents_1)):\n",
    "    if i not in bad_en_idx_1 and i not in bad_kk_idx:\n",
    "        new_orig_en_sents_1.append(orig_en_sents_1[i])\n",
    "        new_orig_kk_sents.append(orig_kk_sents[i])\n",
    "        new_transl_en_sents_1.append(transl_en_sents_1[i])\n",
    "        new_transl_kk_sents.append(transl_kk_sents[i])\n",
    "\n",
    "new_orig_en_sents_2, new_orig_gu_sents, new_transl_en_sents_2, new_transl_gu_sents = [],[],[],[]\n",
    "for i in range(len(orig_en_sents_2)):\n",
    "    if i not in bad_en_idx_2 and i not in bad_gu_idx:\n",
    "        new_orig_en_sents_2.append(orig_en_sents_2[i])\n",
    "        new_orig_gu_sents.append(orig_gu_sents[i])\n",
    "        new_transl_en_sents_2.append(transl_en_sents_2[i])\n",
    "        new_transl_gu_sents.append(transl_gu_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing variable names back to originals\n",
    "orig_en_sents_1, orig_kk_sents, transl_en_sents_1, transl_kk_sents = new_orig_en_sents_1, new_orig_kk_sents, new_transl_en_sents_1, new_transl_kk_sents\n",
    "orig_en_sents_2, orig_gu_sents, transl_en_sents_2, transl_gu_sents = new_orig_en_sents_2, new_orig_gu_sents, new_transl_en_sents_2, new_transl_gu_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding all sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "orig_en_embs_1 = [[embed(doc)] for doc in orig_en_sents_1]\n",
    "print(\"Finished orig_en_embs\")\n",
    "orig_kk_embs = [[embed(doc)] for doc in orig_kk_sents]\n",
    "print(\"Finished orig_gu_embs\")\n",
    "transl_en_embs_1 = [[embed(doc)] for doc in transl_en_sents_1]\n",
    "print(\"Finished transl_en_embs\")\n",
    "transl_kk_embs = [[embed(doc)] for doc in transl_kk_sents]\n",
    "print(\"Finished transl_gu_embs\")\n",
    "\n",
    "orig_en_embs_2 = [[embed(doc)] for doc in orig_en_sents_2]\n",
    "print(\"Finished orig_en_embs\")\n",
    "orig_gu_embs = [[embed(doc)] for doc in orig_gu_sents]\n",
    "print(\"Finished orig_gu_embs\")\n",
    "transl_en_embs_2 = [[embed(doc)] for doc in transl_en_sents_2]\n",
    "print(\"Finished transl_en_embs\")\n",
    "transl_gu_embs = [[embed(doc)] for doc in transl_gu_sents]\n",
    "print(\"Finished transl_gu_embs\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total embedding time: {:} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings in the correct format for similarity search\n",
    "\n",
    "for i in range(len(orig_en_embs_1)):\n",
    "    orig_en_embs_1[i], orig_kk_embs[i], transl_en_embs_1[i], transl_kk_embs[i] = np.asarray(orig_en_embs_1[i]), np.asarray(orig_kk_embs[i]), np.asarray(transl_en_embs_1[i]), np.asarray(transl_kk_embs[i])\n",
    "for i in range(len(orig_en_embs_1)):\n",
    "    orig_en_embs_1[i], orig_kk_embs[i], transl_en_embs_1[i], transl_kk_embs[i] = orig_en_embs_1[i][0,:,:], orig_kk_embs[i][0,:,:], transl_en_embs_1[i][0,:,:], transl_kk_embs[i][0,:,:]\n",
    "    \n",
    "for i in range(len(orig_en_embs_2)):\n",
    "    orig_en_embs_2[i], orig_gu_embs[i], transl_en_embs_2[i], transl_gu_embs[i] = np.asarray(orig_en_embs_2[i]), np.asarray(orig_gu_embs[i]), np.asarray(transl_en_embs_2[i]), np.asarray(transl_gu_embs[i])\n",
    "for i in range(len(orig_en_embs_2)):\n",
    "    orig_en_embs_2[i], orig_gu_embs[i], transl_en_embs_2[i], transl_gu_embs[i] = orig_en_embs_2[i][0,:,:], orig_gu_embs[i][0,:,:], transl_en_embs_2[i][0,:,:], transl_gu_embs[i][0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + original kk sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_kk_orig, all_margin_scores_en_kk_orig = [], []\n",
    "# Mine on a document level, rather than globally\n",
    "for en_doc, kk_doc in zip(orig_en_embs_1, orig_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, kk_doc, '')\n",
    "    all_sent_pairs_en_kk_orig.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_kk_orig.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method-1.06.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method-1.20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method-1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN_THRESHOLD = 1.06 # Set this to whichever margin you want to use\n",
    "orig_en_kk_pairs = []\n",
    "for i in range(len(all_sent_pairs_en_kk_orig)):\n",
    "    doc = all_sent_pairs_en_kk_orig[i]\n",
    "    for j in range(len(doc)):\n",
    "        if all_margin_scores_en_kk_orig[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "            kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "            orig_en_kk_pairs.append((en_sent, kk_sent))\n",
    "\n",
    "en_sents_1, kk_sents = [pair[0] for pair in orig_en_kk_pairs], [pair[1] for pair in orig_en_kk_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_old_method_top20k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pairs_with_docidx_ms = []\n",
    "for i in range(len(all_sent_pairs_en_kk_orig)):\n",
    "    doc_pairs = []\n",
    "    for j in range(len(all_sent_pairs_en_kk_orig[i])):\n",
    "        doc_pairs.append((all_sent_pairs_en_kk_orig[i][j][0], all_sent_pairs_en_kk_orig[i][j][1], \n",
    "                         i, all_margin_scores_en_kk_orig[i][j]))\n",
    "    sent_pairs_with_docidx_ms.append(doc_pairs)\n",
    "    \n",
    "four_tuples = []\n",
    "for doc in sent_pairs_with_docidx_ms:\n",
    "    for tup in doc:\n",
    "        four_tuples.append(tup)\n",
    "        \n",
    "sorted_by_margins = sorted(four_tuples, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "top20k_en_sents = [orig_en_sents_1[tup[2]][tup[0]-1] for tup in top20k]\n",
    "top20k_kk_sents = [orig_kk_sents[tup[2]][tup[0]-1] for tup in top20k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_orig_pairs_1.06.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_orig_pairs_1.20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_orig_pairs_1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + original gu sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_gu_orig, all_margin_scores_en_gu_orig = [], []\n",
    "# Mine on a document level, rather than globally\n",
    "for en_doc, gu_doc in zip(orig_en_embs_2, orig_gu_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_2)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, gu_doc, '')\n",
    "    all_sent_pairs_en_gu_orig.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_gu_orig.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN_THRESHOLD = 1.06 # Set this to whichever margin you want to use\n",
    "orig_en_gu_pairs = []\n",
    "for i in range(len(all_sent_pairs_en_gu_orig)):\n",
    "    doc = all_sent_pairs_en_gu_orig[i]\n",
    "    for j in range(len(doc)):\n",
    "        if all_margin_scores_en_gu_orig[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_2[i][pair[0]-1]\n",
    "            gu_sent = orig_gu_sents[i][pair[1]-1]\n",
    "            orig_en_gu_pairs.append((en_sent, gu_sent))\n",
    "\n",
    "en_sents_2, gu_sents = [pair[0] for pair in orig_en_gu_pairs], [pair[1] for pair in orig_en_gu_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_kk_to_en_1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + translated kk sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_kk_to_en, all_margin_scores_kk_to_en = [], []\n",
    "for en_doc, transl_kk_doc in zip(orig_en_embs_1, transl_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, transl_kk_doc, '')\n",
    "    all_sent_pairs_kk_to_en.append(doc_sent_pairs)\n",
    "    all_margin_scores_kk_to_en.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN_THRESHOLD = 1.35 # Set this to whichever margin you want to use\n",
    "kk_to_en_pairs = []\n",
    "for i in range(len(all_sent_pairs_kk_to_en)):\n",
    "    doc = all_sent_pairs_kk_to_en[i]\n",
    "    for j in range(len(doc)):\n",
    "        if all_margin_scores_kk_to_en[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "            kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "            kk_to_en_pairs.append((en_sent, kk_sent))\n",
    "\n",
    "en_sents_3, kk_sents_2 = [pair[0] for pair in kk_to_en_pairs], [pair[1] for pair in kk_to_en_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_majority_vote_ALL.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_majority_vote_ALL_1.20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_kk_mined_majority_vote_ALL_1.35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original kk + translated en sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_to_kk, all_margin_scores_en_to_kk = [], []\n",
    "for transl_en_doc, kk_doc in zip(transl_en_embs_1, orig_kk_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_1)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(transl_en_doc, kk_doc, '')\n",
    "    all_sent_pairs_en_to_kk.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_to_kk.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing majority vote retrieval\n",
    "\n",
    "mv_pairs_1, mv_margins_1 = [], []\n",
    "for i in range(len(all_sent_pairs_en_kk_orig)):\n",
    "    \n",
    "    # Grabbing all three sets of sentence pairs:\n",
    "    # 1. orig en + orig kk\n",
    "    # 2. orig en + translated kk\n",
    "    # 3. translated en + orig kk\n",
    "    doc1, doc2, doc3 = all_sent_pairs_en_kk_orig[i], all_sent_pairs_en_to_kk[i], all_sent_pairs_kk_to_en[i]\n",
    "    \n",
    "    # Taking pairwise intersections of these sets\n",
    "    int1, int2, int3 = set(doc1)&set(doc2), set(doc1)&set(doc3), set(doc2)&set(doc3)\n",
    "    # Taking union of pairwise intersections (voting step)\n",
    "    pairwise_int = list(set(int1|int2|int3))\n",
    "    \n",
    "    for j in range(len(pairwise_int)):\n",
    "        pair = pairwise_int[j]\n",
    "        en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "        kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "        mv_pairs_1.append((en_sent, kk_sent))\n",
    "        \n",
    "        # Storing margin scores associated with sentence pairs\n",
    "        if pair in doc1:\n",
    "            idx = all_sent_pairs_en_kk_orig[i].index(pair)\n",
    "            mv_margins_1.append(all_margin_scores_en_kk_orig[i][idx])\n",
    "        elif pair in doc2:\n",
    "            idx = all_sent_pairs_en_to_kk[i].index(pair)\n",
    "            mv_margins_1.append(all_margin_scores_en_to_kk[i][idx])\n",
    "        else:\n",
    "            idx = all_sent_pairs_kk_to_en[i].index(pair)\n",
    "            mv_margins_1.append(all_margin_scores_kk_to_en[i][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional threshold\n",
    "\n",
    "MARGIN_THRESHOLD = 1.06 # Set this to whichever margin you want to use\n",
    "new_mv_pairs = []\n",
    "for i in range(len(mv_pairs_1)):\n",
    "    doc = mv_pairs_1[i]\n",
    "    for j in range(len(doc)):\n",
    "        if mv_margins_1[i][j] > MARGIN_THRESHOLD:\n",
    "            pair = doc[j]\n",
    "            en_sent = orig_en_sents_1[i][pair[0]-1]\n",
    "            kk_sent = orig_kk_sents[i][pair[1]-1]\n",
    "            new_mv_pairs.append((en_sent, kk_sent))\n",
    "\n",
    "en_sents_4, kk_sents_3 = [pair[0] for pair in new_mv_pairs], [pair[1] for pair in new_mv_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### en_gu_pairs_majority_vote.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + translated gu sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_gu_to_en, all_margin_scores_gu_to_en = [], []\n",
    "for en_doc, transl_gu_doc in zip(orig_en_embs_2, transl_gu_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_2)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, transl_gu_doc, '')\n",
    "    all_sent_pairs_gu_to_en.append(doc_sent_pairs)\n",
    "    all_margin_scores_gu_to_en.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using original en + translated gu sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_to_gu, all_margin_scores_en_to_gu = [], []\n",
    "for transl_en_doc, gu_doc in zip(transl_en_embs_2, orig_gu_embs):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(orig_en_sents_2)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(transl_en_doc, gu_doc, '')\n",
    "    all_sent_pairs_en_to_gu.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_to_gu.append(doc_margin_scores)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_pairs_2, mv_margins_2 = [], []\n",
    "for i in range(len(all_sent_pairs_en_gu_orig)):\n",
    "    doc1, doc2, doc3 = all_sent_pairs_en_gu_orig[i], all_sent_pairs_en_to_gu[i], all_sent_pairs_gu_to_en[i]\n",
    "    int1, int2, int3 = set(doc1)&set(doc2), set(doc1)&set(doc3), set(doc2)&set(doc3)\n",
    "    pairwise_int = list(set(int1|int2|int3))\n",
    "    for j in range(len(pairwise_int)):\n",
    "        pair = pairwise_int[j]\n",
    "        en_sent = orig_en_sents_2[i][pair[0]-1]\n",
    "        gu_sent = orig_gu_sents[i][pair[1]-1]\n",
    "        mv_pairs_2.append((en_sent, gu_sent))\n",
    "        \n",
    "        if pair in doc1:\n",
    "            idx = all_sent_pairs_en_gu_orig[i].index(pair)\n",
    "            mv_margins_2.append(all_margin_scores_en_gu_orig[i][idx])\n",
    "        elif pair in doc2:\n",
    "            idx = all_sent_pairs_en_to_gu[i].index(pair)\n",
    "            mv_margins_2.append(all_margin_scores_en_to_gu[i][idx])\n",
    "        else:\n",
    "            idx = all_sent_pairs_gu_to_en[i].index(pair)\n",
    "            mv_margins_2.append(all_margin_scores_gu_to_en[i][idx])\n",
    "\n",
    "en_sents_5, gu_sents_2 = [pair[0] for pair in mv_pairs_2], [pair[1] for pair in mv_pairs_2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

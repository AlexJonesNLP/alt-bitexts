{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usr2/collab/agjones/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install google-api-python client\n",
    "# !pip install -U sentence-transformers\n",
    "# !pip install faiss-gpu\n",
    "# !pip install --upgrade bleu\n",
    "# import bleu\n",
    "# from bleu import list_bleu\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !pip install laserembeddings\n",
    "# !python -m laserembeddings download-models\n",
    "# !pip install cython numpy\n",
    "# !pip install benepar\n",
    "# !pip install word2word\n",
    "# !pip install pylcs\n",
    "# !pip install google-cloud-translate\n",
    "# !pip install google-auth==1.6.3\n",
    "# pip install google-auth-oauthlib\n",
    "#import benepar\n",
    "import pylcs\n",
    "from word2word import Word2word as w2w\n",
    "import spacy\n",
    "from google.cloud import translate\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import faiss\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tag import StanfordNERTagger\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "#benepar.download('benepar_fr')\n",
    "#benepar.download('benepar_en2')\n",
    "nltk.download('stopwords')\n",
    "from bs4 import BeautifulSoup\n",
    "import getpass\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# Checking for a GPU\n",
    "\n",
    "class GPUError(Exception):\n",
    "    pass\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    if torch.cuda.get_device_name(0) == \"Tesla K40m\":\n",
    "        raise GPUError(\"GPU Error: No compatible GPU found\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer(\n",
       "    (auto_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(501153, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Pooling()\n",
       "  (2): Dense(\n",
       "    (activation_function): Tanh()\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (3): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building and configuring what we need for the multilingual sentence embeddings\n",
    "\n",
    "# word_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "# word_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "# word_model.cuda()\n",
    "\n",
    "model_name = 'LaBSE'\n",
    "sentence_model = SentenceTransformer(model_name)\n",
    "sentence_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed for averaged word embeddings (= sentence embeddings)\n",
    "\n",
    "# Tokenize with XLMRoBERTa\n",
    "def word_tokenize(sentence):\n",
    "    tokenized_sentence = torch.LongTensor([word_tokenizer.encode(sentence, add_special_tokens=False)]) #, add_special_tokens=True, padding='max_length', max_length=40, truncation=True)])\n",
    "    tokenized_sentence = tokenized_sentence.to(device)\n",
    "    return tokenized_sentence\n",
    "\n",
    "# Embed with SBERT/LaBSE\n",
    "# Returns SENTENCE embeddings of input sentences\n",
    "def embed(sentences):\n",
    "    sentence_embeddings = sentence_model.encode(sentences)\n",
    "    return sentence_embeddings\n",
    "\n",
    "# Embed with XLMRoBERTa\n",
    "# Returns WORD embeddings of input sentences\n",
    "def word_embed(tokenized_word):\n",
    "    word_embedding = word_model.embeddings(tokenized_word)\n",
    "    word_embedding = torch.mean(word_embedding, dim=1)\n",
    "    return word_embedding.detach().cpu()\n",
    "    \n",
    "\n",
    "    # NB: Already dividing by the norm to create embedding!!\n",
    "#     sentence_embedding = model.embeddings(tokenized_sentence)\n",
    "#     #sentence_embedding = torch.mean(sentence_embedding, dim=1) # Using mean of hidden states as sentence embeddings\n",
    "#     sentence_embedding = sentence_embedding[0][-1] # Using CLS token hidden state as sentence embedding\n",
    "#     return sentence_embedding.detach().cpu()\n",
    "\n",
    "# def getCosineSimilarity(embedding1, embedding2):\n",
    "#     pass\n",
    "# #     cos_sim_func = nn.CosineSimilarity(dim=0)\n",
    "# #     cosine_similarity = cos_sim_func(embedding1, embedding2)\n",
    "# #     return cosine_similarity.detach().cpu().numpy()\n",
    "\n",
    "# def getAvgCosineSimilarity(sentence1, sentence2):\n",
    "#     tokenized_sentence1 = tokenize_sentence(sentence1)\n",
    "#     tokenized_sentence2 = tokenize_sentence(sentence2)\n",
    "#     sentence1_embedding = embed(tokenized_sentence1)\n",
    "#     sentence2_embedding = embed(tokenized_sentence2)\n",
    "#     cosine_similarity = getCosineSimilarity(sentence1_embedding, sentence2_embedding)\n",
    "#     return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent1_emb = embed(tokenize_sentence(sent1))\n",
    "# sent2_emb = embed(tokenize_sentence(sent2))\n",
    "# print(sent1_emb.shape)\n",
    "# print(sent1_emb)\n",
    "# print(sent2_emb)\n",
    "# print(getCosineSimilarity(sent1_emb, sent2_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# PERFORMING SOME TESTS ON TRAINING DATA FROM THE BUCC'18 PARALLEL \n",
    "# SENTENCE EXTRACTION TASK\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing BUCC data\n",
    "\n",
    "bucc_column_names = ['ID', 'Text']\n",
    "\n",
    "# En/Fr sentences with IDs\n",
    "english_sents = pd.read_csv('Data/fr-en.training_en.txt', names=bucc_column_names, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "french_sents = pd.read_csv('Data/fr-en.training_fr.txt', names=bucc_column_names, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "gold_column_names = ['fr_id', 'en_id']\n",
    "\n",
    "# IDs for artificially-inserted gold-standard sentence pairs\n",
    "gold_pairs = pd.read_csv('Data/fr-en.training_gold.csv', names=gold_column_names, error_bad_lines=False)\n",
    "gold_pairs['fr_id'] = gold_pairs['fr_id'].map(lambda x: x.rstrip('\\t'))\n",
    "\n",
    "# Reading in the English training data that's been translated to French (using HuggingFace's \n",
    "# OPUS MT / Helsinki-NLP open-source NMT)\n",
    "with open('Data/translated_english_sentences_all.txt', 'r') as f:\n",
    "    transl_en_sents = f.read().splitlines()\n",
    "    \n",
    "fr_vocab = pd.read_csv('Data/fr_vocab.txt', names=['words'], encoding='latin_1', error_bad_lines=False)\n",
    "fr_vocab = [word for word in fr_vocab['words']]\n",
    "\n",
    "en_fr_dict = pd.read_csv('Dictionaries/en-fr_dict.txt', names=['entries'], error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_dict = [entry for entry in en_fr_dict['entries']]\n",
    "en_fr_dict_list = []\n",
    "for entry in en_fr_dict:\n",
    "    if type(entry)==str and len(entry.split())==2:\n",
    "        en_fr_dict_list.append((entry.split()[0], entry.split()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21710562705993652\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "en_fr_dict_dict = {en_fr_dict_list[0][0]:[en_fr_dict_list[0][1]]}\n",
    "for entry in en_fr_dict_list[1:]:\n",
    "    if entry[0] in en_fr_dict_dict:\n",
    "        en_fr_dict_dict[entry[0]].append(entry[1])\n",
    "    else:\n",
    "        en_fr_dict_dict[entry[0]] = [entry[1]]\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 D , much manifestement Mozartian , ecrit considérablement ultérieur concurrently Mozart 's sortie . \n"
     ]
    }
   ],
   "source": [
    "# transl = \"\"\n",
    "# nstp = [word for word in nltk.word_tokenize(en_sents_only[1500]) if word not in stopwords.words('english')]\n",
    "# for word in nstp:\n",
    "#     if word in en_fr_dict_dict:\n",
    "#         transl += en_fr_dict_dict[word][0] + \" \"\n",
    "#     else:\n",
    "#         transl += word + \" \"\n",
    "# print(transl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching English sentences from BUCC data\n",
    "en_sents_only = [en_sent for en_sent in english_sents['Text']]\n",
    "fr_sents_only = [fr_sent for fr_sent in french_sents['Text']]\n",
    "\n",
    "# en_sents_nostp = [[word for word in nltk.word_tokenize(sent)] for sent in en_sents_only]\n",
    "# fr_vocab = [entry[1] for entry in en_fr_dict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching gold-standard IDs for each language\n",
    "en_gold_ids = [id_ for id_ in gold_pairs['en_id']]\n",
    "fr_gold_ids = [id_ for id_ in gold_pairs['fr_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning the gold IDs\n",
    "def cleanID(id_list):\n",
    "    for i in range(len(id_list)):\n",
    "        id_list[i] = re.sub('[^0-9]', '', id_list[i])\n",
    "        id_list[i] = int(id_list[i].lstrip('0'))\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ids = cleanID(en_gold_ids)\n",
    "fr_ids = cleanID(fr_gold_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging the gold IDs into tuples\n",
    "# Format: gold_tuples[i] = (en_ids[i], fr_ids[i])\n",
    "\n",
    "gold_tuples = []\n",
    "for i in range(len(en_ids)):\n",
    "    gold_tuples.append((en_ids[i], fr_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to embed 271874 sentences: 275.17 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "fr_embeddings = embed(fr_sents_only)\n",
    "end = time.time()\n",
    "print(\"Time taken to embed {:} sentences: {:.2f} seconds\".format(len(fr_sents_only), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenization+embedding rate with SBERT/LaBSE (French): Approx. 988 sents/second\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence tokenization+embedding rate with SBERT/LaBSE (French): Approx. {:.0f} sents/second\".format(len(fr_sents_only)/(end-start)))\n",
    "#print(\"N.B. This rate includes the time it takes to take the embeddings off the GPU; the real tokenize/embed rate is faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to embed 369810 sentences: 357.61 seconds\n"
     ]
    }
   ],
   "source": [
    "# Embed the original English sentences . . . \n",
    "start = time.time()\n",
    "en_embeddings = embed(en_sents_only)\n",
    "end = time.time()\n",
    "print(\"Time taken to embed {:} sentences: {:.2f} seconds\".format(len(en_sents_only), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to embed 369810 sentences: 409.21 seconds\n"
     ]
    }
   ],
   "source": [
    "# OR embed the translated English sentences\n",
    "start = time.time()\n",
    "transl_en_embeddings = embed(transl_en_sents)\n",
    "end = time.time()\n",
    "print(\"Time taken to embed {:} sentences: {:.2f} seconds\".format(len(en_sents_only), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenization+embedding rate with SBERT/LaBSE (English): Approx. 904 sents/second\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence tokenization+embedding rate with SBERT/LaBSE (English): Approx. {:.0f} sents/second\".format(len(transl_en_sents)/(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding English sentences using XLM-RoBERTa\n",
    "start = time.time()\n",
    "en_word_embeddings = [[word_embed(word_tokenize(word)) for word in sent.split()] for sent in en_sents_only]\n",
    "# tok_en_sents = [word_tokenize(sent) for sent in en_sents_only]\n",
    "# en_word_embeddings = [word_embed(tok_sent) for tok_sent in tok_en_sents]\n",
    "end = time.time()\n",
    "print(\"Time taken to tokenize/embed {:} sentences with XLM-RoBERTa: {:.2f} seconds\".format(len(en_sents_only), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding French vocab words using XLM-RoBERTa\n",
    "start = time.time()\n",
    "fr_vocab_embeddings = [word_embed(word_tokenize(word)) for word in fr_vocab]\n",
    "end = time.time()\n",
    "print(\"Time taken to tokenize/embed {:} words with XLM-RoBERTa: {:.2f} seconds\".format(len(fr_vocab), (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#\n",
    "# Finding the nearest neighbor for each (translated) English sentence using margin-\n",
    "# based similarity search (https://www.aclweb.org/anthology/P19-1309.pdf)\n",
    "#\n",
    "# Courtesy of https://github.com/facebookresearch/LASER/blob/master/source/\n",
    "# mine_bitexts.py\n",
    "#\n",
    "# Inspiration for any derivative code found below is to be attributed to Mikel \n",
    "# Artetxe, Holger Schwenk, and Facebook AI, unless otherwise stated\n",
    "#\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the GPU with FAISS\n",
    "GPU = faiss.StandardGpuResources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent1 = 'This is a sentence'\n",
    "# sent2 = 'C\\'est une phrase'\n",
    "# sent3 = 'Le chien a mangé la tarte'\n",
    "# sent1_emb = embed(tokenize_sentence(sent1))\n",
    "# sent2_emb = embed(tokenize_sentence(sent2))\n",
    "# sent3_emb = embed(tokenize_sentence(sent3))\n",
    "# print(getCosineSimilarity(sent1_emb, sent2_emb))\n",
    "# print(getCosineSimilarity(sent1_emb, sent3_emb))\n",
    "# print('---------')\n",
    "# sent1_emb = normalize(np.array(sent1_emb.numpy()[0]))\n",
    "# sent2_emb = normalize(np.array(sent2_emb.numpy()[0]))\n",
    "# sent3_emb = normalize(np.array(sent3_emb.numpy()[0]))\n",
    "# print(sent1_emb.dot(sent2_emb))\n",
    "# print(sent1_emb.dot(sent3_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting French and translated English embeddings into matrices\n",
    "\n",
    "# # print(fr_embeddings[0].numpy()[0])\n",
    "\n",
    "# fr_emb_mat = np.zeros((len(fr_embeddings), fr_embeddings[0].shape[0]), dtype=np.float32)\n",
    "# fr_vecs_only = [emb for emb in fr_embeddings]\n",
    "# for i in range(fr_emb_mat.shape[0]):\n",
    "#     fr_emb_mat[i] = np.array(fr_vecs_only[i])\n",
    "    \n",
    "# en_emb_mat = np.zeros((len(en_embeddings), en_embeddings[0].shape[0]), dtype=np.float32)\n",
    "# en_vecs_only = [emb for emb in en_embeddings]\n",
    "# for i in range(en_emb_mat.shape[0]):\n",
    "#     en_emb_mat[i] = np.array(en_vecs_only[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing all the embeddings\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm != 0:\n",
    "        return v/norm\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "# for i in range(fr_emb_mat.shape[0]):\n",
    "#     fr_emb_mat[i] = normalize(fr_emb_mat[i])\n",
    "# for i in range(en_emb_mat.shape[0]):\n",
    "#     en_emb_mat[i] = normalize(en_emb_mat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEIGHBORS = 4 # Can be adjusted to give better margin score results\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "def knnSearch(src_emb, tgt_emb, k=1, batch_size=1):\n",
    "    emb_dim = src_emb.shape[1] # Embedding dimension\n",
    "    num_src_sents = src_emb.shape[0]\n",
    "    num_tgt_sents = tgt_emb.shape[0]\n",
    "    cos_sims = np.zeros((num_src_sents, k), dtype=np.float32)\n",
    "    inds = np.zeros((num_src_sents, k), dtype=np.int64)\n",
    "    for s_min in range(0, num_src_sents, batch_size):\n",
    "        #if s_min % 1e2 == 0 and s_min > 0:\n",
    "            #print(\"Found {} out of {} forward/backward inds\".format(s_min, src_emb.shape[0]))\n",
    "        s_max = min(s_min + batch_size, num_src_sents)\n",
    "        src_sims = []\n",
    "        src_inds = []\n",
    "        for t_min in range(0, num_tgt_sents, batch_size):\n",
    "            t_max = min(t_min + batch_size, num_tgt_sents)\n",
    "            idx = faiss.IndexFlatIP(emb_dim)\n",
    "            idx = faiss.index_cpu_to_gpu(GPU, 0, idx)\n",
    "            idx.add(tgt_emb[t_min : t_max])\n",
    "            src_sim, src_ind = idx.search(src_emb[s_min : s_max], min(k, t_max-t_min))\n",
    "            src_sims.append(src_sim)\n",
    "            src_inds.append(src_ind + t_min)\n",
    "            del idx\n",
    "        src_sims = np.concatenate(src_sims, axis=1)\n",
    "        src_inds = np.concatenate(src_inds, axis=1)\n",
    "        sorted_inds = np.argsort(-src_sims, axis=1)\n",
    "        for i in range(s_min, s_max):\n",
    "            for j in range(k):\n",
    "                cos_sims[i, j] = src_sims[i-s_min, sorted_inds[i-s_min, j]]\n",
    "                inds[i, j] = src_inds[i-s_min, sorted_inds[i-s_min, j]]\n",
    "    return cos_sims, inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# knnSearch(transl_en_emb_mat[:1], fr_emb_mat[:1], k=1, batch_size=1)\n",
    "# end = time.time()\n",
    "# print(end-start)\n",
    "\n",
    "# start = time.time()\n",
    "# transl_en_emb_mat[0].dot(fr_emb_mat[0])\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directedMeansAndInds(src_emb, tgt_emb, forward=False, backward=False, k=1, batch_size=1):\n",
    "    assert forward != backward, \"Please choose either forward or backward\"\n",
    "    if forward:\n",
    "        cos_sims, inds = knnSearch(src_emb, tgt_emb, min(tgt_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds\n",
    "    elif backward:\n",
    "        cos_sims, inds = knnSearch(tgt_emb, src_emb, min(src_emb.shape[0], k), batch_size)\n",
    "        return cos_sims.mean(axis=1), inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin(a, b, ratio=False, distance=False):\n",
    "    if ratio:\n",
    "        return a/b\n",
    "    elif distance:\n",
    "        return a-b\n",
    "    else:\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorePair(src_embedding, tgt_embedding, fwd_mean, bwd_mean, r=False, d=False):\n",
    "    if r:\n",
    "        return margin(src_embedding.dot(tgt_embedding), (fwd_mean + bwd_mean) / 2, ratio=True)\n",
    "    elif d:\n",
    "        return margin(src_embedding.dot(tgt_embedding), (fwd_mean + bwd_mean) / 2, distance=True)\n",
    "    else:\n",
    "        return margin(src_embedding.dot(tgt_embedding), _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreCandidates(src_emb, tgt_emb, candidate_inds, fwd_mean, bwd_mean, r=False, d=False):\n",
    "    scores = np.zeros(candidate_inds.shape)\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(scores.shape[1]):\n",
    "            k = candidate_inds[i,j]\n",
    "            scores[i,j] = scorePair(src_emb[i], tgt_emb[k], fwd_mean[i], bwd_mean[k], r, d)\n",
    "    max_scores = np.zeros((src_emb.shape[0], 3))\n",
    "    for i in range(scores.shape[0]):\n",
    "        argmax = np.argmax(scores[i])\n",
    "        max_ind = candidate_inds[i][argmax]\n",
    "        max_scores[i] = ((i+1, max_ind+1, np.max(scores[i])))\n",
    "    return max_scores  \n",
    "#     max_scores = np.zeros((src_emb.shape[0], 3))\n",
    "#     for i in range(src_emb.shape[0]):\n",
    "#         j_max = 0\n",
    "#         for j in range(tgt_emb.shape[0]):\n",
    "#             score = scorePair(src_emb[i], tgt_emb[j], fwd_mean[i], bwd_mean[j], r=r, d=d)\n",
    "#             if score > scorePair(src_emb[i], tgt_emb[j_max], fwd_mean[i], bwd_mean[j_max], r=r, d=d):\n",
    "#                 j_max = j\n",
    "#         max_scores[i] = ((i+1, j_max+1, scorePair(src_emb[i], tgt_emb[j_max], fwd_mean[i], bwd_mean[j_max], r=r, d=d)))\n",
    "# #                 scores[i,j] = scorePair(src_emb[i], tgt_emb[j], fwd_mean[i], bwd_mean[j], r=True)\n",
    "    \n",
    "#     # Returns a list of tuples (x_i, y_j) such that the margin score between x_i and y_j is maximized\n",
    "#     return max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatches(src_emb, \n",
    "               tgt_emb,  \n",
    "               k=1, \n",
    "               batch_size=1, \n",
    "               ratio=False, \n",
    "               distance=False):\n",
    "    \n",
    "    fwd_means, fwd_inds = directedMeansAndInds(src_emb, tgt_emb, forward=True, k=k, batch_size=batch_size)\n",
    "    bwd_means, bwd_inds = directedMeansAndInds(src_emb, tgt_emb, backward=True, k=k, batch_size=batch_size)\n",
    "    fwd_matches = scoreCandidates(src_emb, tgt_emb, fwd_inds, fwd_means, bwd_means, r=ratio, d=distance)\n",
    "    bwd_matches = scoreCandidates(tgt_emb, src_emb, bwd_inds, bwd_means, fwd_means, r=ratio, d=distance)\n",
    "    fwd_matches = [tuple(fwd_matches[i]) for i in range(fwd_matches.shape[0])]\n",
    "    bwd_matches = [tuple(bwd_matches[i]) for i in range(bwd_matches.shape[0])]\n",
    "#     matches = []\n",
    "#     for i in range(margin_scores.shape[0]):\n",
    "#         matches.append((i+1, np.argmax(margin_scores[i])+1, np.max(margin_scores[i]))) # Add 1 to the indices because the gold training pairs are 1-indexed\n",
    "    return fwd_matches, bwd_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeF1(pred_tuples, gold_tuples):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    f1 = 0\n",
    "    epsilon = 1e-8 # To prevent division by zero\n",
    "    for pair in pred_tuples:\n",
    "        if pair in gold_tuples:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1 \n",
    "    prec = tp / (len(pred_tuples) + epsilon)\n",
    "    rec = tp / len(gold_tuples)\n",
    "    f1 = 2*prec*rec / (prec+rec+epsilon)\n",
    "    return f1, prec, rec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for arr in en_inds:\n",
    "    for ind in arr:\n",
    "        ind_sims, ind_inds = knnSearch(fr_embeddings[ind:ind+1], en_embeddings, k=4, batch_size=1000)\n",
    "        ind_mean = ind_sims.mean(axis=1)\n",
    "        score = (sample_en_emb.dot(fr_embeddings[ind])) / np.average((ind_mean, en_sims.mean(axis=1)))\n",
    "        print(score, 342507, ind+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29881838\n",
      "0.45290685 245347 88157\n",
      "0.35613424\n",
      "0.5621042 302350 88157\n",
      "0.2507164\n",
      "0.39928344 282778 88157\n",
      "0.3007291\n",
      "0.4721996 284240 88157\n"
     ]
    }
   ],
   "source": [
    "for arr in fr_inds:\n",
    "    for ind in arr:\n",
    "        ind_sims, ind_inds = knnSearch(en_embeddings[ind:ind+1], fr_embeddings, k=4, batch_size=1000)\n",
    "        ind_mean = ind_sims.mean(axis=1)\n",
    "        score = (sample_fr_emb.dot(en_embeddings[ind])) / np.average((ind_mean, fr_sims.mean(axis=1)))\n",
    "        print(score, ind+1, 88157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_means, fwd_inds = directedMeansAndInds(en_embeddings, fr_embeddings, forward=True, k=4, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_means, bwd_inds = directedMeansAndInds(en_embeddings, fr_embeddings, backward=True, k=4, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "for i in range(fwd_inds.shape[0]):\n",
    "    for j in range(fwd_inds.shape[1]):\n",
    "        tgt_ind = fwd_inds[i,j]\n",
    "        margin_score = (en_embeddings[i].dot(fr_embeddings[tgt_ind])) / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "        fwd_margin_scores[i,j] = margin_score\n",
    "best = np.zeros((fwd_inds.shape[0], 3))\n",
    "best_inds = fwd_inds[np.arange(en_embeddings.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "for i in range(fwd_inds.shape[0]):\n",
    "    best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "    best[i] = ((i+1, best_inds[i]+1, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "for i in range(bwd_inds.shape[0]):\n",
    "    for j in range(bwd_inds.shape[1]):\n",
    "        tgt_ind = bwd_inds[i,j]\n",
    "        margin_score = (fr_embeddings[i].dot(en_embeddings[tgt_ind])) / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "        bwd_margin_scores[i,j] = margin_score\n",
    "bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "best_inds = bwd_inds[np.arange(fr_embeddings.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "for i in range(bwd_inds.shape[0]):\n",
    "    best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "    #best_ind = bwd_inds[i][ind]\n",
    "    bwd_best[i] = ((best_inds[i]+1, i+1, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.0398 Precision: 0.0203 Recall: 0.9941\n"
     ]
    }
   ],
   "source": [
    "fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "pairs_and_scores = []\n",
    "# pairs_and_scores = list(set(fwd_best) & set(bwd_best))\n",
    "for fwd_triplet, bwd_triplet in zip(fwd_best, bwd_best):\n",
    "    pairs_and_scores.append(fwd_triplet)\n",
    "    pairs_and_scores.append(bwd_triplet)\n",
    "\n",
    "pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "# #pairs_and_scores = list(set(pairs_and_scores))\n",
    "concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores]\n",
    "#concat_pairs = list(dict.fromkeys(concat_pairs))\n",
    "#concat_pairs = list(set(concat_pairs))\n",
    "concat_pairs_int = []\n",
    "for tup in concat_pairs:\n",
    "    concat_pairs_int.append((int(tup[0]), int(tup[1])))\n",
    "concat_pairs = concat_pairs_int\n",
    "f1, prec, rec = computeF1(concat_pairs, gold_tuples)\n",
    "\n",
    "margin_scores = [triplet[2] for triplet in pairs_and_scores]\n",
    "\n",
    "pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv('Data/sent_pairs_fwd_NEW.csv', index=False)\n",
    "pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv('Data/sent_pairs_bwd_NEW.csv', index=False)\n",
    "pd.DataFrame({'concat_pairs': concat_pairs}).to_csv('Data/concat_pairs_NEW.csv', index=False)\n",
    "pd.DataFrame({'margin_scores': margin_scores}).to_csv('Data/margin_scores_NEW.csv', index=False)\n",
    "\n",
    "print(\"F1: {:.4f}\".format(f1), \"Precision: {:.4f}\".format(prec), \"Recall: {:.4f}\".format(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.0396 Precision: 0.0202 Recall: 0.9938\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_sent_pairs_NEW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-03b58d4484ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mall_sent_pairs_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/concat_pairs_T.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mall_sent_pairs_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sent_pairs_T\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'concat_pairs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mall_sent_pairs_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sent_pairs_NEW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_sent_pairs_NEW' is not defined"
     ]
    }
   ],
   "source": [
    "# # USING TRANSLATED ENGLISH SENTENCES FOR MINING INSTEAD\n",
    "\n",
    "# fwd_means, fwd_inds = directedMeansAndInds(transl_en_embeddings, fr_embeddings, forward=True, k=4, batch_size=1000)\n",
    "# bwd_means, bwd_inds = directedMeansAndInds(transl_en_embeddings, fr_embeddings, backward=True, k=4, batch_size=1000)\n",
    "\n",
    "# fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "# for i in range(fwd_inds.shape[0]):\n",
    "#     for j in range(fwd_inds.shape[1]):\n",
    "#         tgt_ind = fwd_inds[i,j]\n",
    "#         margin_score = (transl_en_embeddings[i].dot(fr_embeddings[tgt_ind])) / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "#         fwd_margin_scores[i,j] = margin_score\n",
    "# best = np.zeros((fwd_inds.shape[0], 3))\n",
    "# best_inds = fwd_inds[np.arange(transl_en_embeddings.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "# for i in range(fwd_inds.shape[0]):\n",
    "#     best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "#     best[i] = ((i+1, best_inds[i]+1, best_score))\n",
    "    \n",
    "# bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "# for i in range(bwd_inds.shape[0]):\n",
    "#     for j in range(bwd_inds.shape[1]):\n",
    "#         tgt_ind = bwd_inds[i,j]\n",
    "#         margin_score = (fr_embeddings[i].dot(transl_en_embeddings[tgt_ind])) / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "#         bwd_margin_scores[i,j] = margin_score\n",
    "# bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "# best_inds = bwd_inds[np.arange(fr_embeddings.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "# for i in range(bwd_inds.shape[0]):\n",
    "#     best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "#     #best_ind = bwd_inds[i][ind]\n",
    "#     bwd_best[i] = ((best_inds[i]+1, i+1, best_score))\n",
    "    \n",
    "# fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "# bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "# pairs_and_scores = []\n",
    "# # pairs_and_scores = list(set(fwd_best) & set(bwd_best))\n",
    "# for fwd_triplet, bwd_triplet in zip(fwd_best, bwd_best):\n",
    "#     pairs_and_scores.append(fwd_triplet)\n",
    "#     pairs_and_scores.append(bwd_triplet)\n",
    "\n",
    "# pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "# # #pairs_and_scores = list(set(pairs_and_scores))\n",
    "# concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores]\n",
    "# #concat_pairs = list(dict.fromkeys(concat_pairs))\n",
    "# #concat_pairs = list(set(concat_pairs))\n",
    "# concat_pairs_int = []\n",
    "# for tup in concat_pairs:\n",
    "#     concat_pairs_int.append((int(tup[0]), int(tup[1])))\n",
    "# concat_pairs = concat_pairs_int\n",
    "# f1, prec, rec = computeF1(concat_pairs, gold_tuples)\n",
    "\n",
    "# margin_scores = [triplet[2] for triplet in pairs_and_scores]\n",
    "\n",
    "# pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv('Data/sent_pairs_fwd_T.csv', index=False)\n",
    "# pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv('Data/sent_pairs_bwd_T.csv', index=False)\n",
    "# pd.DataFrame({'concat_pairs': concat_pairs}).to_csv('Data/concat_pairs_T.csv', index=False)\n",
    "# pd.DataFrame({'margin_scores': margin_scores}).to_csv('Data/margin_scores_T.csv', index=False)\n",
    "\n",
    "# print(\"F1: {:.4f}\".format(f1), \"Precision: {:.4f}\".format(prec), \"Recall: {:.4f}\".format(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word-by-word translations of English sentences into French using knnSearch\n",
    "\n",
    "def translateWordByWord(src_sents,\n",
    "                        src_embs,\n",
    "                        dict_) -> list:\n",
    "    \n",
    "    emb_dim = src_embs[0][0].shape[1]\n",
    "    wbw_transl = []\n",
    "    for i in range(len(src_embs)):\n",
    "        sent_emb_mat = np.zeros((len(src_embs[i]), emb_dim), dtype=np.float32)\n",
    "        transl = \"\"\n",
    "        for j in range(sent_emb_mat.shape[0]):\n",
    "            sent_emb_mat[j] = np.array(src_embs[i][j])\n",
    "            word = src_sents[i].split()[j]\n",
    "            if word in dict_:\n",
    "                if len(dict_[word])==1:\n",
    "                    transl += dict_[word][0] + \" \"\n",
    "                else:\n",
    "                    candidates = np.zeros((len(dict_[word]), emb_dim), dtype=np.float32)\n",
    "                    for k in range(candidates.shape[0]):\n",
    "                        candidates[k] = word_embed(word_tokenize(dict_[word][k]))\n",
    "                    _, nn_ind = knnSearch(sent_emb_mat[j:j+1], candidates)\n",
    "                    transl += dict_[word][int(nn_ind)] + \" \"\n",
    "            else:\n",
    "                transl += word + \" \"\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Translated sentence {:} of {:}\".format(i, len(src_sents)))\n",
    "        wbw_transl.append(transl)\n",
    "    return wbw_transl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "transl_en_sents_wbw = translateWordByWord(en_sents_only, en_word_embeddings, en_fr_dict_dict)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/translated_english_sentences_wbw.txt', 'w') as f:\n",
    "    for sent in transl_en_sents_wbw:\n",
    "        f.write(\"%s\\n\" % sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/translated_english_sentences_wbw.txt', 'r') as f:\n",
    "    transl_en_sents_wbw = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to embed 369810 sentences: 366.30 seconds\n"
     ]
    }
   ],
   "source": [
    "# # Embedding wbw-translated English sentences\n",
    "start = time.time()\n",
    "wbw_transl_embeddings = embed(transl_en_sents_wbw)\n",
    "end = time.time()\n",
    "print(\"Time taken to embed {:} sentences: {:.2f} seconds\".format(len(transl_en_sents_wbw), end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.0396 Precision: 0.0202 Recall: 0.9924\n"
     ]
    }
   ],
   "source": [
    "# Mining sentence pairs and computing F1 score using \"code-switched\" English sentences\n",
    "\n",
    "fwd_means, fwd_inds = directedMeansAndInds(wbw_transl_embeddings, fr_embeddings, forward=True, k=4, batch_size=1000)\n",
    "bwd_means, bwd_inds = directedMeansAndInds(wbw_transl_embeddings, fr_embeddings, backward=True, k=4, batch_size=1000)\n",
    "\n",
    "fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "for i in range(fwd_inds.shape[0]):\n",
    "    for j in range(fwd_inds.shape[1]):\n",
    "        tgt_ind = fwd_inds[i,j]\n",
    "        margin_score = (wbw_transl_embeddings[i].dot(fr_embeddings[tgt_ind])) / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "        fwd_margin_scores[i,j] = margin_score\n",
    "best = np.zeros((fwd_inds.shape[0], 3))\n",
    "best_inds = fwd_inds[np.arange(wbw_transl_embeddings.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "for i in range(fwd_inds.shape[0]):\n",
    "    best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "    best[i] = ((i+1, best_inds[i]+1, best_score))\n",
    "    \n",
    "bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "for i in range(bwd_inds.shape[0]):\n",
    "    for j in range(bwd_inds.shape[1]):\n",
    "        tgt_ind = bwd_inds[i,j]\n",
    "        margin_score = (fr_embeddings[i].dot(wbw_transl_embeddings[tgt_ind])) / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "        bwd_margin_scores[i,j] = margin_score\n",
    "bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "best_inds = bwd_inds[np.arange(fr_embeddings.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "for i in range(bwd_inds.shape[0]):\n",
    "    best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "    #best_ind = bwd_inds[i][ind]\n",
    "    bwd_best[i] = ((best_inds[i]+1, i+1, best_score))\n",
    "    \n",
    "fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "pairs_and_scores = []\n",
    "# pairs_and_scores = list(set(fwd_best) & set(bwd_best))\n",
    "for fwd_triplet, bwd_triplet in zip(fwd_best, bwd_best):\n",
    "    pairs_and_scores.append(fwd_triplet)\n",
    "    pairs_and_scores.append(bwd_triplet)\n",
    "\n",
    "pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "# #pairs_and_scores = list(set(pairs_and_scores))\n",
    "concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores]\n",
    "#concat_pairs = list(dict.fromkeys(concat_pairs))\n",
    "#concat_pairs = list(set(concat_pairs))\n",
    "concat_pairs_int = []\n",
    "for tup in concat_pairs:\n",
    "    concat_pairs_int.append((int(tup[0]), int(tup[1])))\n",
    "concat_pairs = concat_pairs_int\n",
    "f1, prec, rec = computeF1(concat_pairs, gold_tuples)\n",
    "\n",
    "margin_scores = [triplet[2] for triplet in pairs_and_scores]\n",
    "\n",
    "pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv('Data/sent_pairs_fwd_wbw.csv', index=False)\n",
    "pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv('Data/sent_pairs_bwd_wbw.csv', index=False)\n",
    "pd.DataFrame({'concat_pairs': concat_pairs}).to_csv('Data/concat_pairs_wbw.csv', index=False)\n",
    "pd.DataFrame({'margin_scores': margin_scores}).to_csv('Data/margin_scores_wbw.csv', index=False)\n",
    "\n",
    "print(\"F1: {:.4f}\".format(f1), \"Precision: {:.4f}\".format(prec), \"Recall: {:.4f}\".format(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_margin_scores_wbw = pd.read_csv('Data/margin_scores_wbw.csv')\n",
    "all_margin_scores_wbw = [float(score) for score in all_margin_scores_wbw['margin_scores']]\n",
    "all_sent_pairs_wbw = pd.read_csv('Data/concat_pairs_wbw.csv')\n",
    "all_sent_pairs_wbw = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs_wbw['concat_pairs']]\n",
    "all_sent_pairs_wbw = [pair[0] for pair in all_sent_pairs_wbw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(computeF1(concat_pairs, gold_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# test_pairs_fwd = getMatches(x, y, k=NUM_NEIGHBORS, batch_size=BATCH_SIZE, ratio=True)\n",
    "# test_pairs_bwd = getMatches(y, x, k=NUM_NEIGHBORS, batch_size=BATCH_SIZE, ratio=True)\n",
    "# concat_pairs = []\n",
    "# for fwd_triplet, bwd_triplet in zip(test_pairs_fwd, test_pairs_bwd):\n",
    "#     concat_pairs.append(fwd_triplet[0:2])\n",
    "#     concat_pairs.append((bwd_triplet[1], bwd_triplet[0]))\n",
    "# concat_pairs = list(set(concat_pairs))\n",
    "# f1, prec, rec = computeF1(concat_pairs, gold_tuples)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Getting matches for real now\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# sent_pairs_fwd, sent_pairs_bwd = getMatches(en_emb_mat, fr_emb_mat, k=NUM_NEIGHBORS, batch_size=BATCH_SIZE, ratio=True)\n",
    "# pairs_and_scores = []\n",
    "# for fwd_triplet, bwd_triplet in zip(sent_pairs_fwd, sent_pairs_bwd):\n",
    "#     pairs_and_scores.append(fwd_triplet)\n",
    "#     pairs_and_scores.append((bwd_triplet[1], bwd_triplet[0], bwd_triplet[2]))\n",
    "#     #concat_pairs.append(fwd_triplet[0:2])\n",
    "#     #margin_scores.append(fwd_triplet[2])\n",
    "#     #concat_pairs.append((bwd_triplet[1], bwd_triplet[0]))\n",
    "#     #margin_scores.append(bwd_triplet[2])\n",
    "\n",
    "# pairs_and_scores = list(set(pairs_and_scores))\n",
    "# concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores]\n",
    "# concat_pairs = list(set(concat_pairs))\n",
    "# #concat_pairs = list(set(concat_pairs))\n",
    "# concat_pairs_int = []\n",
    "# for tup in concat_pairs:\n",
    "#     concat_pairs_int.append((int(tup[0]), int(tup[1])))\n",
    "# concat_pairs = concat_pairs_int\n",
    "# f1, prec, rec = computeF1(concat_pairs, gold_tuples)\n",
    "\n",
    "# margin_scores = [triplet[2] for triplet in pairs_and_scores]\n",
    "\n",
    "# pd.DataFrame({'sent_pairs_fwd': sent_pairs_fwd}).to_csv('Data/sent_pairs_fwd_NEW.csv', index=False)\n",
    "# pd.DataFrame({'sent_pairs_bwd': sent_pairs_bwd}).to_csv('Data/sent_pairs_bwd_NEW.csv', index=False)\n",
    "# pd.DataFrame({'concat_pairs': concat_pairs}).to_csv('Data/concat_pairs_NEW.csv', index=False)\n",
    "# pd.DataFrame({'margin_scores': margin_scores}).to_csv('Data/margin_scores_NEW.csv', index=False)\n",
    "\n",
    "# print(\"F1: {:.4f}\".format(f1), \"Precision: {:.4f}\".format(prec), \"Recall: {:.4f}\".format(rec))\n",
    "\n",
    "# end = time.time()\n",
    "# print(\"Time taken to find pairs: {:.2f} seconds\".format((end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445050\n",
      "445050\n"
     ]
    }
   ],
   "source": [
    "print(len(margin_scores))\n",
    "print(len(all_sent_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_margin_scores = pd.read_csv('Data/margin_scores.csv')\n",
    "all_margin_scores_NEW = pd.read_csv('Data/margin_scores_NEW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_margin_scores = [float(score) for score in all_margin_scores['margin_scores']]\n",
    "all_margin_scores_NEW = [float(score) for score in all_margin_scores_NEW['margin_scores']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the sentence pairs back in\n",
    "all_sent_pairs = pd.read_csv('Data/concat_pairs.csv')\n",
    "all_sent_pairs_NEW = pd.read_csv('Data/concat_pairs_NEW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting strings back into tuples\n",
    "all_sent_pairs = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs['concat_pairs']]\n",
    "all_sent_pairs_NEW = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs_NEW['concat_pairs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting just a list of tuples\n",
    "all_sent_pairs = [pair[0] for pair in all_sent_pairs]\n",
    "all_sent_pairs_NEW = [pair[0] for pair in all_sent_pairs_NEW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knnSearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5f67ba9bb172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ind\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mknnSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m237295\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m237296\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknnSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100352\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100353\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'knnSearch' is not defined"
     ]
    }
   ],
   "source": [
    "test_sim, test_ind =  knnSearch(fr_embeddings[237295:237296], en_embeddings, k=1, batch_size=1000)\n",
    "ts, ti = knnSearch(en_embeddings[100352:100353], fr_embeddings, k=1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ind, test_sim)\n",
    "print(ti, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5242267847061157, 0.5467867255210876, 0.5596277713775635, 0.5174039602279663, 0.5158339142799377, 0.5208346843719482, 0.6611602306365967, 0.5113794803619385, 0.4711534380912781, 0.5581844449043274]\n"
     ]
    }
   ],
   "source": [
    "print(all_margin_scores_NEW[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5495959\n"
     ]
    }
   ],
   "source": [
    "print(en_embeddings[142313].dot(fr_embeddings[260047]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.039763418495595414, 0.02028760813391754, 0.9937266123706802)\n",
      "Should print '(0.039763418495595414, 0.02028760813391754, 0.9937266123706802)'\n"
     ]
    }
   ],
   "source": [
    "# # Checking to see that the pairs we just read in match those we wrote out\n",
    "# print(computeF1(all_sent_pairs, gold_tuples))\n",
    "# print(\"Should print '(0.039763418495595414, 0.02028760813391754, 0.9937266123706802)'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# \n",
    "# Now, we try to optimize the F1 score using other features\n",
    "# of the sentence pairs we already have.\n",
    "#\n",
    "# Benchmark to beat: 92.5 (https://arxiv.org/pdf/2004.09813.pdf).\n",
    "#\n",
    "# After the F1 score has been optimized on Fr-En using these features, we can\n",
    "# test how these same features transfer to other languages in the BUCC training\n",
    "# sets, and then to low-resource languages not in the BUCC data (e.g. Kazakh, Gujarati,\n",
    "# Somali)\n",
    "#\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(all_margin_scores)==len(all_sent_pairs)) # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.605146272722705 1.0\n",
      "0.7669204422819055 0.29242923855781555\n",
      "1.0070340326908442 0.5864986017554328\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(max(all_margin_scores), max(all_margin_scores_NEW))\n",
    "print(min(all_margin_scores), min(all_margin_scores_NEW))\n",
    "print(np.average(all_margin_scores), np.average(all_margin_scores_NEW))\n",
    "print(all_margin_scores==all_margin_scores_NEW)\n",
    "_ = [(sim1, sim2) for sim1, sim2 in zip(all_margin_scores, all_margin_scores_NEW) if sim1==sim2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6222602317800019\n",
      "-0.05660247802734375\n"
     ]
    }
   ],
   "source": [
    "print(np.average(all_cosine_similarities))\n",
    "print(np.min(all_cosine_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9146169632529533, 0.9006615450267811, 0.9290116662998019)\n"
     ]
    }
   ],
   "source": [
    "# Tuning the margin score threshold to be the max value t such that recall > 0.99\n",
    "\n",
    "MARGIN_THRESHOLD = 1.17\n",
    "CS_THRESHOLD = 0.84\n",
    "updated_sent_pairs = [] # Where we'll keep the new sentence pairs filtered from the initial set\n",
    "\n",
    "# for i in range(len(all_sent_pairs)):\n",
    "#     if all_cosine_similarities[i] > CS_THRESHOLD:\n",
    "#         updated_sent_pairs.append(all_sent_pairs[i])\n",
    "\n",
    "# print(computeF1(updated_sent_pairs, gold_tuples))\n",
    "\n",
    "updated_sent_pairs_wbw = []\n",
    "\n",
    "for i in range(len(all_sent_pairs_wbw)):\n",
    "    if all_margin_scores_wbw[i] > MARGIN_THRESHOLD:\n",
    "        updated_sent_pairs_wbw.append(all_sent_pairs_wbw[i])\n",
    "        \n",
    "print(computeF1(updated_sent_pairs_wbw, gold_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_en = [en_sents_only[pair[0]-1] for pair in gold_tuples if pair not in updated_sent_pairs_NEW]\n",
    "fn_fr = [fr_sents_only[pair[1]-1] for pair in gold_tuples if pair not in updated_sent_pairs_NEW]\n",
    "fp_en = [en_sents_only[pair[0]-1] for pair in updated_sent_pairs_NEW if pair not in gold_tuples]\n",
    "fp_fr = [fr_sents_only[pair[1]-1] for pair in updated_sent_pairs_NEW if pair not in gold_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({\"en\": fn_en, \"fr\": fn_fr}).to_csv('Data/false_negatives.csv', index=False)\n",
    "# pd.DataFrame({\"en\": fp_en, \"fr\": fp_fr}).to_csv('Data/false_positives.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = pd.read_csv('Data/false_negatives.csv')\n",
    "false_positives = pd.read_csv('Data/false_positives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9419\n",
      "0.9740312135046183 0.9571780057002534\n"
     ]
    }
   ],
   "source": [
    "# Recomputing F1-score by estimating number of incorrectly-flagged false positives and\n",
    "# false negatives\n",
    "\n",
    "f1, prec, rec = computeF1(updated_sent_pairs_NEW, gold_tuples)\n",
    "\n",
    "NUM_TPS = int(len(updated_sent_pairs_NEW)*prec)\n",
    "NUM_FNS = len(gold_tuples) - int(len(gold_tuples)*rec)\n",
    "\n",
    "print(len(updated_sent_pairs_NEW))\n",
    "\n",
    "# Ratio of \"false\" false positive sentence pairs to \"true\" false positive sentence pairs\n",
    "# i.e. 72 out of the first 100 sentence pairs flagged as false positives were deemed to be\n",
    "# valid translations of one another\n",
    "FFP_RATIO = 0.72\n",
    "\n",
    "# Ratio of \"false\" false negative sentences pairs to \"true\" false negatives,\n",
    "# computed as above\n",
    "FFN_RATIO = 0\n",
    "\n",
    "NUM_FALSE_FPS = 0.72*len(false_positives)\n",
    "NUM_FALSE_FNS = 0\n",
    "UPDATED_PREC = (NUM_TPS + NUM_FALSE_FPS) / len(updated_sent_pairs_NEW)\n",
    "UPDATED_REC = 0\n",
    "UPDATED_F1 = 2*UPDATED_PREC*rec / (UPDATED_PREC + rec)\n",
    "print(UPDATED_PREC, UPDATED_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_ids = [pair for pair in updated_sent_pairs_NEW if pair not in gold_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tuples_new = [tup for tup in gold_tuples]\n",
    "for id_ in false_positive_ids:\n",
    "    k = random.uniform(0, 1)\n",
    "    if k <= FFP_RATIO:\n",
    "        gold_tuples_new.append(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n"
     ]
    }
   ],
   "source": [
    "print(len(gold_tuples_new)-len(gold_tuples)) # Number of false false positives in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9598286529958574, 0.9753689351300824, 0.9447758124228712)\n"
     ]
    }
   ],
   "source": [
    "print(computeF1(updated_sent_pairs_NEW, gold_tuples_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SENT\n",
      "  (VN (CLS-SUJ Il) (V peut))\n",
      "  (VPinf-OBJ\n",
      "    (VN (VINF être) (VPP divisé))\n",
      "    (PP-P_OBJ (P en) (NP (DET trois) (NC zones)))\n",
      "    (PP-MOD\n",
      "      (P+D+ (P en) (NC fonction) (P+D du))\n",
      "      (NP (NC relief))\n",
      "      (COORD (PONCT ,) (PP (P+D du) (NP (NC climat))))\n",
      "      (COORD (PONCT ,) (PP (P+D du) (NP (NC peuplement))))\n",
      "      (COORD (CC et) (PP (P de) (NP (NC l'économie))))))\n",
      "  (PONCT :)) Il peut être divisé en trois zones en fonction du relief, du climat, du peuplement et de l'économie:\n"
     ]
    }
   ],
   "source": [
    "parser = benepar.Parser(\"benepar_fr\")\n",
    "sample_tree = parser.parse(fr_sents_only[0])\n",
    "print(sample_tree, fr_sents_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENT\n",
      "['Il', 'peut', 'être', 'divisé', 'en', 'trois', 'zones', 'en', 'fonction', 'du', 'relief', ',', 'du', 'climat', ',', 'du', 'peuplement', 'et', 'de', \"l'économie\", ':']\n",
      "(SENT\n",
      "  Il\n",
      "  peut\n",
      "  être\n",
      "  divisé\n",
      "  en\n",
      "  trois\n",
      "  zones\n",
      "  en\n",
      "  fonction\n",
      "  du\n",
      "  relief\n",
      "  ,\n",
      "  du\n",
      "  climat\n",
      "  ,\n",
      "  du\n",
      "  peuplement\n",
      "  et\n",
      "  de\n",
      "  l'économie\n",
      "  :)\n",
      "8\n",
      "<bound method Tree.subtrees of Tree('SENT', [Tree('VN', [Tree('CLS-SUJ', ['Il']), Tree('V', ['peut'])]), Tree('VPinf-OBJ', [Tree('VN', [Tree('VINF', ['être']), Tree('VPP', ['divisé'])]), Tree('PP-P_OBJ', [Tree('P', ['en']), Tree('NP', [Tree('DET', ['trois']), Tree('NC', ['zones'])])]), Tree('PP-MOD', [Tree('P+D+', [Tree('P', ['en']), Tree('NC', ['fonction']), Tree('P+D', ['du'])]), Tree('NP', [Tree('NC', ['relief'])]), Tree('COORD', [Tree('PONCT', [',']), Tree('PP', [Tree('P+D', ['du']), Tree('NP', [Tree('NC', ['climat'])])])]), Tree('COORD', [Tree('PONCT', [',']), Tree('PP', [Tree('P+D', ['du']), Tree('NP', [Tree('NC', ['peuplement'])])])]), Tree('COORD', [Tree('CC', ['et']), Tree('PP', [Tree('P', ['de']), Tree('NP', [Tree('NC', [\"l'économie\"])])])])])]), Tree('PONCT', [':'])])>\n",
      "<bound method Tree.productions of Tree('SENT', [Tree('VN', [Tree('CLS-SUJ', ['Il']), Tree('V', ['peut'])]), Tree('VPinf-OBJ', [Tree('VN', [Tree('VINF', ['être']), Tree('VPP', ['divisé'])]), Tree('PP-P_OBJ', [Tree('P', ['en']), Tree('NP', [Tree('DET', ['trois']), Tree('NC', ['zones'])])]), Tree('PP-MOD', [Tree('P+D+', [Tree('P', ['en']), Tree('NC', ['fonction']), Tree('P+D', ['du'])]), Tree('NP', [Tree('NC', ['relief'])]), Tree('COORD', [Tree('PONCT', [',']), Tree('PP', [Tree('P+D', ['du']), Tree('NP', [Tree('NC', ['climat'])])])]), Tree('COORD', [Tree('PONCT', [',']), Tree('PP', [Tree('P+D', ['du']), Tree('NP', [Tree('NC', ['peuplement'])])])]), Tree('COORD', [Tree('CC', ['et']), Tree('PP', [Tree('P', ['de']), Tree('NP', [Tree('NC', [\"l'économie\"])])])])])]), Tree('PONCT', [':'])])>\n",
      "<bound method Tree.pos of Tree('SENT', [Tree('VN', [Tree('CLS-SUJ', ['Il']), Tree('V', ['peut'])]), Tree('VPinf-OBJ', [Tree('VN', [Tree('VINF', ['être']), Tree('VPP', ['divisé'])]), Tree('PP-P_OBJ', [Tree('P', ['en']), Tree('NP', [Tree('DET', ['trois']), Tree('NC', ['zones'])])]), Tree('PP-MOD', [Tree('P+D+', [Tree('P', ['en']), Tree('NC', ['fonction']), Tree('P+D', ['du'])]), Tree('NP', [Tree('NC', ['relief'])]), Tree('COORD', [Tree('PONCT', [',']), Tree('PP', [Tree('P+D', ['du']), Tree('NP', [Tree('NC', ['climat'])])])]), Tree('COORD', [Tree('PONCT', [',']), Tree('PP', [Tree('P+D', ['du']), Tree('NP', [Tree('NC', ['peuplement'])])])]), Tree('COORD', [Tree('CC', ['et']), Tree('PP', [Tree('P', ['de']), Tree('NP', [Tree('NC', [\"l'économie\"])])])])])]), Tree('PONCT', [':'])])>\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with syntax tree operations\n",
    "\n",
    "print(sample_tree.label()) # Returns the node label of the entire sentence (maximal projection)\n",
    "print(sample_tree.leaves()) # Returns leaves of tree as a list; should be identical to input sentence\n",
    "print(sample_tree.flatten()) # Returns Tree.label() + Tree.leaves()\n",
    "print(sample_tree.height()) # Returns the height of the tree\n",
    "print(sample_tree.subtrees) # Returns all subtrees within the tree, ordered sequentially\n",
    "print(sample_tree.productions) # Seems to return the same thing as Tree.subtrees??\n",
    "print(sample_tree.pos) # Returns pos-tagged sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENT', 'VN', 'CLS-SUJ', 'V', 'VPinf-OBJ', 'VN', 'VINF', 'VPP', 'PP-P_OBJ', 'P', 'NP', 'DET', 'NC', 'PP-MOD', 'P+D+', 'P', 'NC', 'P+D', 'NP', 'NC', 'COORD', 'PONCT', 'PP', 'P+D', 'NP', 'NC', 'COORD', 'PONCT', 'PP', 'P+D', 'NP', 'NC', 'COORD', 'CC', 'PP', 'P', 'NP', 'NC', 'PONCT']\n",
      "----------------------------------------------------------------\n",
      "['CLS-SUJ', 'V', 'VINF', 'VPP', 'P', 'DET', 'NC', 'P', 'NC', 'P+D', 'NC', 'PONCT', 'P+D', 'NC', 'PONCT', 'P+D', 'NC', 'CC', 'P', 'NC', 'PONCT']\n"
     ]
    }
   ],
   "source": [
    "# Storing a list of subtree nodes (projections) from the sentence\n",
    "subtree_nodes = []\n",
    "for subtree in sample_tree.subtrees():\n",
    "    subtree_nodes.append(subtree.label())\n",
    "print(subtree_nodes)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "# Storing a list of POS tags from the sentence\n",
    "pos = []\n",
    "for node in sample_tree.pos():\n",
    "    pos.append(node[1])\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = benepar.Parser(\"benepar_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENT', 'VN', 'VS', 'NP-SUJ', 'NPP', 'CC', 'NP-SUJ', 'NPP', 'NPP', 'VN', 'V', 'ADV', 'VPP', 'Ssub-OBJ', 'PROWH', 'PP', 'P+D', 'NP', 'PRO', 'ADV', 'ADJ', 'VPP']\n"
     ]
    }
   ],
   "source": [
    "sample_tree_2 = parser.parse([en2fr(en_word)[0] for en_word in re.findall(r'\\w+', en_sents_only[0])])\n",
    "stree_list = []\n",
    "for stree in sample_tree_2.subtrees():\n",
    "    stree_list.append(stree.label())\n",
    "print(stree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plaise', 'Pygmalion', 'ni', 'Frankenstein', 'humanité', 'doit', 'longtemps', 'fasciné', 'Qu', 'aux', 'Aucune', 'beaucoup', 'artificiel', 'sauvé']\n"
     ]
    }
   ],
   "source": [
    "print([en2fr(en_word)[0] for en_word in re.findall(r'\\w+', en_sents_only[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether Pygmalion or Frankenstein, humanity has been fascinated with the idea of artificial life.\n",
      "plaise Pygmalion ni Frankenstein humanité doit longtemps fasciné Qu aux Aucune beaucoup artificiel sauvé\n"
     ]
    }
   ],
   "source": [
    "print(en_sents_only[0])\n",
    "print(' '.join([en2fr(word)[0] for word in re.findall(r'\\w+', en_sents_only[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(pylcs.lcs2(''.join(stree_list), ''.join(subtree_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using constituent overlap as a filtering mechanism\n",
    "\n",
    "# parser = benepar.Parser('benepar_fr')\n",
    "# en2fr = w2w('en', 'fr')\n",
    "\n",
    "# # Returns length of longest common substring between joined list of constituents\n",
    "# # for each sentence\n",
    "# def getConstituentOverlaps(en_sents: list, fr_sents: list) -> list:\n",
    "#     print(\"Creating French constituent trees . . .\")\n",
    "#     fr_trees = parser.parse_sents(fr_sents)\n",
    "#     print(\"French trees complete!\")\n",
    "#     print(\"Creating word-by-word translations of English sentences . . .\")\n",
    "#     transl_en_sents = []\n",
    "#     i = 0\n",
    "#     for en_sent in en_sents:\n",
    "#         for en_word in re.findall(r'\\w+', en_sent):\n",
    "#             transl_en_sent = []\n",
    "#             try:\n",
    "#                 transl_en_sent.append(en2fr(en_word)[0])\n",
    "#             except:\n",
    "#                 transl_en_sent.append(en_word)\n",
    "#         transl_en_sents.append(transl_en_sent)\n",
    "#         i += 1\n",
    "#         if i % 1e5 == 0:\n",
    "#             print(\"Translated sentence {:} of {:}\".format(i, len(en_sents)), flush=True)\n",
    "#     print(\"Translations complete!\")\n",
    "#     print(\"Creating translated English constituent trees . . .\")\n",
    "#     transl_en_trees = parser.parse_sents([' '.join(sent) for sent in transl_en_sents])\n",
    "#     print(\"Translated English trees complete!\")\n",
    "#     print(\"Parsing French constituents . . .\")\n",
    "#     all_fr_constituents = []\n",
    "#     j = 0\n",
    "#     start1 = time.time()\n",
    "#     for tree in fr_trees:\n",
    "#         constituents = ''.join([stree.label() for stree in tree.subtrees()])\n",
    "#         all_fr_constituents.append(constituents)\n",
    "#         j += 1\n",
    "#         if j % 1e4 == 0:\n",
    "#             end1 = time.time()\n",
    "#             print(\"Parsed sentence {:} of {:}\".format(j, len(fr_sents)), flush=True)\n",
    "#             print(\"Time taken: {:.1f} seconds\".format(end1-start1))\n",
    "#             start1 = time.time()\n",
    "#     print(\"French constituent parsing complete!\")\n",
    "#     print(\"Parsing translated English constituents . . .\")\n",
    "#     all_transl_en_constituents = []\n",
    "#     k = 0\n",
    "#     start2 = time.time()\n",
    "#     for tree in transl_en_trees:\n",
    "#         constituents = ''.join([stree.label() for stree in tree.subtrees()])\n",
    "#         all_transl_en_constituents.append(constituents)\n",
    "#         k += 1\n",
    "#         if k % 1e4 == 0:\n",
    "#             end2 = time.time()\n",
    "#             print(\"Parsed sentence {:} of {:}\".format(k, len(en_sents)), flush=True)\n",
    "#             print(\"Time taken: {:.1f} seconds\".format(end2-start2))\n",
    "#     print(\"Translated English constituent parsing complete!\")\n",
    "#     print(\"Computing longest continuous constituent overlaps . . .\")\n",
    "#     overlap = [pylcs.lcs2(fr_consts, transl_en_consts) for fr_consts, transl_en_consts in zip(all_fr_constituents, all_transl_en_constituents)]\n",
    "#     print(\"Constituent overlap computations complete!\")\n",
    "#     return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_en_sents = [en_sents_only[pair[0]-1] for pair in all_sent_pairs_NEW]\n",
    "# all_fr_sents = [fr_sents_only[pair[1]-1] for pair in all_sent_pairs_NEW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating French constituent trees . . .\n",
      "French trees complete!\n",
      "Creating word-by-word translations of English sentences . . .\n",
      "Translated sentence 100000 of 445054\n",
      "Translated sentence 200000 of 445054\n",
      "Translated sentence 300000 of 445054\n",
      "Translated sentence 400000 of 445054\n",
      "Translations complete!\n",
      "Creating translated English constituent trees . . .\n",
      "Translated English trees complete!\n",
      "Parsing French constituents . . .\n",
      "Parsed sentence 10000 of 445054\n",
      "Time taken: 146.2 seconds\n",
      "Parsed sentence 20000 of 445054\n",
      "Time taken: 140.9 seconds\n",
      "Parsed sentence 30000 of 445054\n",
      "Time taken: 139.7 seconds\n",
      "Parsed sentence 40000 of 445054\n",
      "Time taken: 141.9 seconds\n"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "# all_constituent_overlaps = getConstituentOverlaps(all_en_sents, all_fr_sents)\n",
    "# end = time.time()\n",
    "# print(\"Time elapsed: {:.1f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Data/constituent_overlaps.txt', 'w') as f:\n",
    "#     for overlap in all_constituent_overlaps:\n",
    "#         f.write(str(overlap)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/constituent_overlaps.txt', 'r') as f:\n",
    "    all_constituent_overlaps = f.read().splitlines()\n",
    "all_constituent_overlaps = [int(val) for val in all_constituent_overlaps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the margin score threshold to be the max value t such that recall > 0.99\n",
    "\n",
    "MARGIN_THRESHOLD = 1.19\n",
    "CONST_THRESHOLD = 7\n",
    "\n",
    "updated_sent_pairs_NEW = []\n",
    "\n",
    "for i in range(len(all_sent_pairs_NEW)):\n",
    "    if all_margin_scores_NEW[i] > MARGIN_THRESHOLD and all_constituent_overlaps[i] > CONST_THRESHOLD:\n",
    "        updated_sent_pairs_NEW.append(all_sent_pairs_NEW[i])\n",
    "        \n",
    "print(computeF1(updated_sent_pairs_NEW, gold_tuples_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(max(all_constituent_overlaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9239083391429174, 0.9076236992982505, 0.9407880255337883)\n"
     ]
    }
   ],
   "source": [
    "# Using length ratio as a filtering feature\n",
    "\n",
    "LENGTH_RATIO = 1.1\n",
    "updated_sent_pairs = []\n",
    "\n",
    "for i in range(len(all_sent_pairs_NEW)):\n",
    "    en_ind, fr_ind = (all_sent_pairs_NEW[i][0]-1, all_sent_pairs_NEW[i][1]-1)\n",
    "    if all_margin_scores_NEW[i] > MARGIN_THRESHOLD and ( (1-LENGTH_RATIO)*len(en_sents_only[en_ind]) < len(fr_sents_only[fr_ind]) < (1+LENGTH_RATIO)*len(en_sents_only[en_ind]) ):\n",
    "        updated_sent_pairs.append(all_sent_pairs_NEW[i])\n",
    "\n",
    "print(computeF1(updated_sent_pairs, gold_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating English sentences to French in order to calculate BLEU score\n",
    "#\n",
    "#####################################################################################################\n",
    "# Using the Helsinki-NLP / OPUS NMT model from HuggingFace to translate En BUCC train data to French\n",
    "#####################################################################################################\n",
    "#\n",
    "#################### *** SKIP THIS if sentences have already been translated *** ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, installing/importing the necessary depenencies . . . \n",
    "\n",
    "# !pip install mosestokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2bbd3fe7574831b7cd4eab14ccb999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/305M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(61674, 512, padding_idx=61673)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(61674, 512, padding_idx=61673)\n",
       "      (embed_positions): SinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): Identity()\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(61674, 512, padding_idx=61673)\n",
       "      (embed_positions): SinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model_name = 'Helsinki-NLP/opus-mt-en-trk'\n",
    "marian_tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(translation_model_name)\n",
    "translation_model.cuda().half() # Puts translation model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateToFrench(sent):\n",
    "    tokenized = marian_tokenizer.prepare_seq2seq_batch(sent, return_tensors='pt')\n",
    "    tokenized.to(device)\n",
    "    translated = translation_model.generate(**tokenized)\n",
    "    decoded = [marian_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    tokenized.to('cpu')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2649881839752197\n"
     ]
    }
   ],
   "source": [
    "# transl_sents_test = []\n",
    "# start = time.time()\n",
    "# # for sent in en_sents_only[3000:3010]:\n",
    "# #     transl_sents_test.append(translateToFrench([sent]))\n",
    "# translated = translateToFrench(en_sents_only[5000:5100])\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ids_to_translate = [pair[0] for pair in all_sent_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents_to_translate = [en_sents_only[id_-1] for id_ in en_ids_to_translate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1 of 3699\n",
      "Completed batch 2 of 3699\n",
      "Completed batch 3 of 3699\n",
      "Completed batch 4 of 3699\n",
      "Completed batch 5 of 3699\n",
      "Completed batch 6 of 3699\n",
      "Completed batch 7 of 3699\n",
      "Completed batch 8 of 3699\n",
      "Completed batch 9 of 3699\n",
      "Completed batch 10 of 3699\n",
      "Completed batch 11 of 3699\n",
      "Completed batch 12 of 3699\n",
      "Completed batch 13 of 3699\n",
      "Completed batch 14 of 3699\n",
      "Completed batch 15 of 3699\n",
      "Completed batch 16 of 3699\n",
      "Completed batch 17 of 3699\n",
      "Completed batch 18 of 3699\n",
      "Completed batch 19 of 3699\n",
      "Completed batch 20 of 3699\n",
      "Completed batch 21 of 3699\n",
      "Completed batch 22 of 3699\n",
      "Completed batch 23 of 3699\n",
      "Completed batch 24 of 3699\n",
      "Completed batch 25 of 3699\n",
      "Completed batch 26 of 3699\n",
      "Completed batch 27 of 3699\n",
      "Completed batch 28 of 3699\n",
      "Completed batch 29 of 3699\n",
      "Completed batch 30 of 3699\n",
      "Completed batch 31 of 3699\n",
      "Completed batch 32 of 3699\n",
      "Completed batch 33 of 3699\n",
      "Completed batch 34 of 3699\n",
      "Completed batch 35 of 3699\n",
      "Completed batch 36 of 3699\n",
      "Completed batch 37 of 3699\n",
      "Completed batch 38 of 3699\n",
      "Completed batch 39 of 3699\n",
      "Completed batch 40 of 3699\n",
      "Completed batch 41 of 3699\n",
      "Completed batch 42 of 3699\n",
      "Completed batch 43 of 3699\n",
      "Completed batch 44 of 3699\n",
      "Completed batch 45 of 3699\n",
      "Completed batch 46 of 3699\n",
      "Completed batch 47 of 3699\n",
      "Completed batch 48 of 3699\n",
      "Completed batch 49 of 3699\n",
      "Completed batch 50 of 3699\n",
      "Completed batch 51 of 3699\n",
      "Completed batch 52 of 3699\n",
      "Completed batch 53 of 3699\n",
      "Completed batch 54 of 3699\n",
      "Completed batch 55 of 3699\n",
      "Completed batch 56 of 3699\n",
      "Completed batch 57 of 3699\n",
      "Completed batch 58 of 3699\n",
      "Completed batch 59 of 3699\n",
      "Completed batch 60 of 3699\n",
      "Completed batch 61 of 3699\n",
      "Completed batch 62 of 3699\n",
      "Completed batch 63 of 3699\n",
      "Completed batch 64 of 3699\n",
      "Completed batch 65 of 3699\n",
      "Completed batch 66 of 3699\n",
      "Completed batch 67 of 3699\n",
      "Completed batch 68 of 3699\n",
      "Completed batch 69 of 3699\n",
      "Completed batch 70 of 3699\n",
      "Completed batch 71 of 3699\n",
      "Completed batch 72 of 3699\n",
      "Completed batch 73 of 3699\n",
      "Completed batch 74 of 3699\n",
      "Completed batch 75 of 3699\n",
      "Completed batch 76 of 3699\n",
      "Completed batch 77 of 3699\n",
      "Completed batch 78 of 3699\n",
      "Completed batch 79 of 3699\n",
      "Completed batch 80 of 3699\n",
      "Completed batch 81 of 3699\n",
      "Completed batch 82 of 3699\n",
      "Completed batch 83 of 3699\n",
      "Completed batch 84 of 3699\n",
      "Completed batch 85 of 3699\n",
      "Completed batch 86 of 3699\n",
      "Completed batch 87 of 3699\n",
      "Completed batch 88 of 3699\n",
      "Completed batch 89 of 3699\n",
      "Completed batch 90 of 3699\n",
      "Completed batch 91 of 3699\n",
      "Completed batch 92 of 3699\n",
      "Completed batch 93 of 3699\n",
      "Completed batch 94 of 3699\n",
      "Completed batch 95 of 3699\n",
      "Completed batch 96 of 3699\n",
      "Completed batch 97 of 3699\n",
      "Completed batch 98 of 3699\n",
      "Completed batch 99 of 3699\n",
      "Completed batch 100 of 3699\n",
      "Completed batch 101 of 3699\n",
      "Completed batch 102 of 3699\n",
      "Completed batch 103 of 3699\n",
      "Completed batch 104 of 3699\n",
      "Completed batch 105 of 3699\n",
      "Completed batch 106 of 3699\n",
      "Completed batch 107 of 3699\n",
      "Completed batch 108 of 3699\n",
      "Completed batch 109 of 3699\n",
      "Completed batch 110 of 3699\n",
      "Completed batch 111 of 3699\n",
      "Completed batch 112 of 3699\n",
      "Completed batch 113 of 3699\n",
      "Completed batch 114 of 3699\n",
      "Completed batch 115 of 3699\n",
      "Completed batch 116 of 3699\n",
      "Completed batch 117 of 3699\n",
      "Completed batch 118 of 3699\n",
      "Completed batch 119 of 3699\n",
      "Completed batch 120 of 3699\n",
      "Completed batch 121 of 3699\n",
      "Completed batch 122 of 3699\n",
      "Completed batch 123 of 3699\n",
      "Completed batch 124 of 3699\n",
      "Completed batch 125 of 3699\n",
      "Completed batch 126 of 3699\n",
      "Completed batch 127 of 3699\n",
      "Completed batch 128 of 3699\n",
      "Completed batch 129 of 3699\n",
      "Completed batch 130 of 3699\n",
      "Completed batch 131 of 3699\n",
      "Completed batch 132 of 3699\n",
      "Completed batch 133 of 3699\n",
      "Completed batch 134 of 3699\n",
      "Completed batch 135 of 3699\n",
      "Completed batch 136 of 3699\n",
      "Completed batch 137 of 3699\n",
      "Completed batch 138 of 3699\n",
      "Completed batch 139 of 3699\n",
      "Completed batch 140 of 3699\n",
      "Completed batch 141 of 3699\n",
      "Completed batch 142 of 3699\n",
      "Completed batch 143 of 3699\n",
      "Completed batch 144 of 3699\n",
      "Completed batch 145 of 3699\n",
      "Completed batch 146 of 3699\n",
      "Completed batch 147 of 3699\n",
      "Completed batch 148 of 3699\n",
      "Completed batch 149 of 3699\n",
      "Completed batch 150 of 3699\n",
      "Completed batch 151 of 3699\n",
      "Completed batch 152 of 3699\n",
      "Completed batch 153 of 3699\n",
      "Completed batch 154 of 3699\n",
      "Completed batch 155 of 3699\n",
      "Completed batch 156 of 3699\n",
      "Completed batch 157 of 3699\n",
      "Completed batch 158 of 3699\n",
      "Completed batch 159 of 3699\n",
      "Completed batch 160 of 3699\n",
      "Completed batch 161 of 3699\n",
      "Completed batch 162 of 3699\n",
      "Completed batch 163 of 3699\n",
      "Completed batch 164 of 3699\n",
      "Completed batch 165 of 3699\n",
      "Completed batch 166 of 3699\n",
      "Completed batch 167 of 3699\n",
      "Completed batch 168 of 3699\n",
      "Completed batch 169 of 3699\n",
      "Completed batch 170 of 3699\n",
      "Completed batch 171 of 3699\n",
      "Completed batch 172 of 3699\n",
      "Completed batch 173 of 3699\n",
      "Completed batch 174 of 3699\n",
      "Completed batch 175 of 3699\n",
      "Completed batch 176 of 3699\n",
      "Completed batch 177 of 3699\n",
      "Completed batch 178 of 3699\n",
      "Completed batch 179 of 3699\n",
      "Completed batch 180 of 3699\n",
      "Completed batch 181 of 3699\n",
      "Completed batch 182 of 3699\n",
      "Completed batch 183 of 3699\n",
      "Completed batch 184 of 3699\n",
      "Completed batch 185 of 3699\n",
      "Completed batch 186 of 3699\n",
      "Completed batch 187 of 3699\n",
      "Completed batch 188 of 3699\n",
      "Completed batch 189 of 3699\n",
      "Completed batch 190 of 3699\n",
      "Completed batch 191 of 3699\n",
      "Completed batch 192 of 3699\n",
      "Completed batch 193 of 3699\n",
      "Completed batch 194 of 3699\n",
      "Completed batch 195 of 3699\n",
      "Completed batch 196 of 3699\n",
      "Completed batch 197 of 3699\n",
      "Completed batch 198 of 3699\n",
      "Completed batch 199 of 3699\n",
      "Completed batch 200 of 3699\n",
      "Completed batch 201 of 3699\n",
      "Completed batch 202 of 3699\n",
      "Completed batch 203 of 3699\n",
      "Completed batch 204 of 3699\n",
      "Completed batch 205 of 3699\n",
      "Completed batch 206 of 3699\n",
      "Completed batch 207 of 3699\n",
      "Completed batch 208 of 3699\n",
      "Completed batch 209 of 3699\n",
      "Completed batch 210 of 3699\n",
      "Completed batch 211 of 3699\n",
      "Completed batch 212 of 3699\n",
      "Completed batch 213 of 3699\n",
      "Completed batch 214 of 3699\n",
      "Completed batch 215 of 3699\n",
      "Completed batch 216 of 3699\n",
      "Completed batch 217 of 3699\n",
      "Completed batch 218 of 3699\n",
      "Completed batch 219 of 3699\n",
      "Completed batch 220 of 3699\n",
      "Completed batch 221 of 3699\n",
      "Completed batch 222 of 3699\n",
      "Completed batch 223 of 3699\n",
      "Completed batch 224 of 3699\n",
      "Completed batch 225 of 3699\n",
      "Completed batch 226 of 3699\n",
      "Completed batch 227 of 3699\n",
      "Completed batch 228 of 3699\n",
      "Completed batch 229 of 3699\n",
      "Completed batch 230 of 3699\n",
      "Completed batch 231 of 3699\n",
      "Completed batch 232 of 3699\n",
      "Completed batch 233 of 3699\n",
      "Completed batch 234 of 3699\n",
      "Completed batch 235 of 3699\n",
      "Completed batch 236 of 3699\n",
      "Completed batch 237 of 3699\n",
      "Completed batch 238 of 3699\n",
      "Completed batch 239 of 3699\n",
      "Completed batch 240 of 3699\n",
      "Completed batch 241 of 3699\n",
      "Completed batch 242 of 3699\n",
      "Completed batch 243 of 3699\n",
      "Completed batch 244 of 3699\n",
      "Completed batch 245 of 3699\n",
      "Completed batch 246 of 3699\n",
      "Completed batch 247 of 3699\n",
      "Completed batch 248 of 3699\n",
      "Completed batch 249 of 3699\n",
      "Completed batch 250 of 3699\n",
      "Completed batch 251 of 3699\n",
      "Completed batch 252 of 3699\n",
      "Completed batch 253 of 3699\n",
      "Completed batch 254 of 3699\n",
      "Completed batch 255 of 3699\n",
      "Completed batch 256 of 3699\n",
      "Completed batch 257 of 3699\n",
      "Completed batch 258 of 3699\n",
      "Completed batch 259 of 3699\n",
      "Completed batch 260 of 3699\n",
      "Completed batch 261 of 3699\n",
      "Completed batch 262 of 3699\n",
      "Completed batch 263 of 3699\n",
      "Completed batch 264 of 3699\n",
      "Completed batch 265 of 3699\n",
      "Completed batch 266 of 3699\n",
      "Completed batch 267 of 3699\n",
      "Completed batch 268 of 3699\n",
      "Completed batch 269 of 3699\n",
      "Completed batch 270 of 3699\n",
      "Completed batch 271 of 3699\n",
      "Completed batch 272 of 3699\n",
      "Completed batch 273 of 3699\n",
      "Completed batch 274 of 3699\n",
      "Completed batch 275 of 3699\n",
      "Completed batch 276 of 3699\n",
      "Completed batch 277 of 3699\n",
      "Completed batch 278 of 3699\n",
      "Completed batch 279 of 3699\n",
      "Completed batch 280 of 3699\n",
      "Completed batch 281 of 3699\n",
      "Completed batch 282 of 3699\n",
      "Completed batch 283 of 3699\n",
      "Completed batch 284 of 3699\n",
      "Completed batch 285 of 3699\n",
      "Completed batch 286 of 3699\n",
      "Completed batch 287 of 3699\n",
      "Completed batch 288 of 3699\n",
      "Completed batch 289 of 3699\n",
      "Completed batch 290 of 3699\n",
      "Completed batch 291 of 3699\n",
      "Completed batch 292 of 3699\n",
      "Completed batch 293 of 3699\n",
      "Completed batch 294 of 3699\n",
      "Completed batch 295 of 3699\n",
      "Completed batch 296 of 3699\n",
      "Completed batch 297 of 3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 298 of 3699\n",
      "Completed batch 299 of 3699\n",
      "Completed batch 300 of 3699\n",
      "Completed batch 301 of 3699\n",
      "Completed batch 302 of 3699\n",
      "Completed batch 303 of 3699\n",
      "Completed batch 304 of 3699\n",
      "Completed batch 305 of 3699\n",
      "Completed batch 306 of 3699\n",
      "Completed batch 307 of 3699\n",
      "Completed batch 308 of 3699\n",
      "Completed batch 309 of 3699\n",
      "Completed batch 310 of 3699\n",
      "Completed batch 311 of 3699\n",
      "Completed batch 312 of 3699\n",
      "Completed batch 313 of 3699\n",
      "Completed batch 314 of 3699\n",
      "Completed batch 315 of 3699\n",
      "Completed batch 316 of 3699\n",
      "Completed batch 317 of 3699\n",
      "Completed batch 318 of 3699\n",
      "Completed batch 319 of 3699\n",
      "Completed batch 320 of 3699\n",
      "Completed batch 321 of 3699\n",
      "Completed batch 322 of 3699\n",
      "Completed batch 323 of 3699\n",
      "Completed batch 324 of 3699\n",
      "Completed batch 325 of 3699\n",
      "Completed batch 326 of 3699\n",
      "Completed batch 327 of 3699\n",
      "Completed batch 328 of 3699\n",
      "Completed batch 329 of 3699\n",
      "Completed batch 330 of 3699\n",
      "Completed batch 331 of 3699\n",
      "Completed batch 332 of 3699\n",
      "Completed batch 333 of 3699\n",
      "Completed batch 334 of 3699\n",
      "Completed batch 335 of 3699\n",
      "Completed batch 336 of 3699\n",
      "Completed batch 337 of 3699\n",
      "Completed batch 338 of 3699\n",
      "Completed batch 339 of 3699\n",
      "Completed batch 340 of 3699\n",
      "Completed batch 341 of 3699\n",
      "Completed batch 342 of 3699\n",
      "Completed batch 343 of 3699\n",
      "Completed batch 344 of 3699\n",
      "Completed batch 345 of 3699\n",
      "Completed batch 346 of 3699\n",
      "Completed batch 347 of 3699\n",
      "Completed batch 348 of 3699\n",
      "Completed batch 349 of 3699\n",
      "Completed batch 350 of 3699\n",
      "Completed batch 351 of 3699\n",
      "Completed batch 352 of 3699\n",
      "Completed batch 353 of 3699\n",
      "Completed batch 354 of 3699\n",
      "Completed batch 355 of 3699\n",
      "Completed batch 356 of 3699\n",
      "Completed batch 357 of 3699\n",
      "Completed batch 358 of 3699\n",
      "Completed batch 359 of 3699\n",
      "Completed batch 360 of 3699\n",
      "Completed batch 361 of 3699\n",
      "Completed batch 362 of 3699\n",
      "Completed batch 363 of 3699\n",
      "Completed batch 364 of 3699\n",
      "Completed batch 365 of 3699\n",
      "Completed batch 366 of 3699\n",
      "Completed batch 367 of 3699\n",
      "Completed batch 368 of 3699\n",
      "Completed batch 369 of 3699\n",
      "Completed batch 370 of 3699\n",
      "Completed batch 371 of 3699\n",
      "Completed batch 372 of 3699\n",
      "Completed batch 373 of 3699\n",
      "Completed batch 374 of 3699\n",
      "Completed batch 375 of 3699\n",
      "Completed batch 376 of 3699\n",
      "Completed batch 377 of 3699\n",
      "Completed batch 378 of 3699\n",
      "Completed batch 379 of 3699\n",
      "Completed batch 380 of 3699\n",
      "Completed batch 381 of 3699\n",
      "Completed batch 382 of 3699\n",
      "Completed batch 383 of 3699\n",
      "Completed batch 384 of 3699\n",
      "Completed batch 385 of 3699\n",
      "Completed batch 386 of 3699\n",
      "Completed batch 387 of 3699\n",
      "Completed batch 388 of 3699\n",
      "Completed batch 389 of 3699\n",
      "Completed batch 390 of 3699\n",
      "Completed batch 391 of 3699\n",
      "Completed batch 392 of 3699\n",
      "Completed batch 393 of 3699\n",
      "Completed batch 394 of 3699\n",
      "Completed batch 395 of 3699\n",
      "Completed batch 396 of 3699\n",
      "Completed batch 397 of 3699\n",
      "Completed batch 398 of 3699\n",
      "Completed batch 399 of 3699\n",
      "Completed batch 400 of 3699\n",
      "Completed batch 401 of 3699\n",
      "Completed batch 402 of 3699\n",
      "Completed batch 403 of 3699\n",
      "Completed batch 404 of 3699\n",
      "Completed batch 405 of 3699\n",
      "Completed batch 406 of 3699\n",
      "Completed batch 407 of 3699\n",
      "Completed batch 408 of 3699\n",
      "Completed batch 409 of 3699\n",
      "Completed batch 410 of 3699\n",
      "Completed batch 411 of 3699\n",
      "Completed batch 412 of 3699\n",
      "Completed batch 413 of 3699\n",
      "Completed batch 414 of 3699\n",
      "Completed batch 415 of 3699\n",
      "Completed batch 416 of 3699\n",
      "Completed batch 417 of 3699\n",
      "Completed batch 418 of 3699\n",
      "Completed batch 419 of 3699\n",
      "Completed batch 420 of 3699\n",
      "Completed batch 421 of 3699\n",
      "Completed batch 422 of 3699\n",
      "Completed batch 423 of 3699\n",
      "Completed batch 424 of 3699\n",
      "Completed batch 425 of 3699\n",
      "Completed batch 426 of 3699\n",
      "Completed batch 427 of 3699\n",
      "Completed batch 428 of 3699\n",
      "Completed batch 429 of 3699\n",
      "Completed batch 430 of 3699\n",
      "Completed batch 431 of 3699\n",
      "Completed batch 432 of 3699\n",
      "Completed batch 433 of 3699\n",
      "Completed batch 434 of 3699\n",
      "Completed batch 435 of 3699\n",
      "Completed batch 436 of 3699\n",
      "Completed batch 437 of 3699\n",
      "Completed batch 438 of 3699\n",
      "Completed batch 439 of 3699\n",
      "Completed batch 440 of 3699\n",
      "Completed batch 441 of 3699\n",
      "Completed batch 442 of 3699\n",
      "Completed batch 443 of 3699\n",
      "Completed batch 444 of 3699\n",
      "Completed batch 445 of 3699\n",
      "Completed batch 446 of 3699\n",
      "Completed batch 447 of 3699\n",
      "Completed batch 448 of 3699\n",
      "Completed batch 449 of 3699\n",
      "Completed batch 450 of 3699\n",
      "Completed batch 451 of 3699\n",
      "Completed batch 452 of 3699\n",
      "Completed batch 453 of 3699\n",
      "Completed batch 454 of 3699\n",
      "Completed batch 455 of 3699\n",
      "Completed batch 456 of 3699\n",
      "Completed batch 457 of 3699\n",
      "Completed batch 458 of 3699\n",
      "Completed batch 459 of 3699\n",
      "Completed batch 460 of 3699\n",
      "Completed batch 461 of 3699\n",
      "Completed batch 462 of 3699\n",
      "Completed batch 463 of 3699\n",
      "Completed batch 464 of 3699\n",
      "Completed batch 465 of 3699\n",
      "Completed batch 466 of 3699\n",
      "Completed batch 467 of 3699\n",
      "Completed batch 468 of 3699\n",
      "Completed batch 469 of 3699\n",
      "Completed batch 470 of 3699\n",
      "Completed batch 471 of 3699\n",
      "Completed batch 472 of 3699\n",
      "Completed batch 473 of 3699\n",
      "Completed batch 474 of 3699\n",
      "Completed batch 475 of 3699\n",
      "Completed batch 476 of 3699\n",
      "Completed batch 477 of 3699\n",
      "Completed batch 478 of 3699\n",
      "Completed batch 479 of 3699\n",
      "Completed batch 480 of 3699\n",
      "Completed batch 481 of 3699\n",
      "Completed batch 482 of 3699\n",
      "Completed batch 483 of 3699\n",
      "Completed batch 484 of 3699\n",
      "Completed batch 485 of 3699\n",
      "Completed batch 486 of 3699\n",
      "Completed batch 487 of 3699\n",
      "Completed batch 488 of 3699\n",
      "Completed batch 489 of 3699\n",
      "Completed batch 490 of 3699\n",
      "Completed batch 491 of 3699\n",
      "Completed batch 492 of 3699\n",
      "Completed batch 493 of 3699\n",
      "Completed batch 494 of 3699\n",
      "Completed batch 495 of 3699\n",
      "Completed batch 496 of 3699\n",
      "Completed batch 497 of 3699\n",
      "Completed batch 498 of 3699\n",
      "Completed batch 499 of 3699\n",
      "Completed batch 500 of 3699\n",
      "Completed batch 501 of 3699\n",
      "Completed batch 502 of 3699\n",
      "Completed batch 503 of 3699\n",
      "Completed batch 504 of 3699\n",
      "Completed batch 505 of 3699\n",
      "Completed batch 506 of 3699\n",
      "Completed batch 507 of 3699\n",
      "Completed batch 508 of 3699\n",
      "Completed batch 509 of 3699\n",
      "Completed batch 510 of 3699\n",
      "Completed batch 511 of 3699\n",
      "Completed batch 512 of 3699\n",
      "Completed batch 513 of 3699\n",
      "Completed batch 514 of 3699\n",
      "Completed batch 515 of 3699\n",
      "Completed batch 516 of 3699\n",
      "Completed batch 517 of 3699\n",
      "Completed batch 518 of 3699\n",
      "Completed batch 519 of 3699\n",
      "Completed batch 520 of 3699\n",
      "Completed batch 521 of 3699\n",
      "Completed batch 522 of 3699\n",
      "Completed batch 523 of 3699\n",
      "Completed batch 524 of 3699\n",
      "Completed batch 525 of 3699\n",
      "Completed batch 526 of 3699\n",
      "Completed batch 527 of 3699\n",
      "Completed batch 528 of 3699\n",
      "Completed batch 529 of 3699\n",
      "Completed batch 530 of 3699\n",
      "Completed batch 531 of 3699\n",
      "Completed batch 532 of 3699\n",
      "Completed batch 533 of 3699\n",
      "Completed batch 534 of 3699\n",
      "Completed batch 535 of 3699\n",
      "Completed batch 536 of 3699\n",
      "Completed batch 537 of 3699\n",
      "Completed batch 538 of 3699\n",
      "Completed batch 539 of 3699\n",
      "Completed batch 540 of 3699\n",
      "Completed batch 541 of 3699\n",
      "Completed batch 542 of 3699\n",
      "Completed batch 543 of 3699\n",
      "Completed batch 544 of 3699\n",
      "Completed batch 545 of 3699\n",
      "Completed batch 546 of 3699\n",
      "Completed batch 547 of 3699\n",
      "Completed batch 548 of 3699\n",
      "Completed batch 549 of 3699\n",
      "Completed batch 550 of 3699\n",
      "Completed batch 551 of 3699\n",
      "Completed batch 552 of 3699\n",
      "Completed batch 553 of 3699\n",
      "Completed batch 554 of 3699\n",
      "Completed batch 555 of 3699\n",
      "Completed batch 556 of 3699\n",
      "Completed batch 557 of 3699\n",
      "Completed batch 558 of 3699\n",
      "Completed batch 559 of 3699\n",
      "Completed batch 560 of 3699\n",
      "Completed batch 561 of 3699\n",
      "Completed batch 562 of 3699\n",
      "Completed batch 563 of 3699\n",
      "Completed batch 564 of 3699\n",
      "Completed batch 565 of 3699\n",
      "Completed batch 566 of 3699\n",
      "Completed batch 567 of 3699\n",
      "Completed batch 568 of 3699\n",
      "Completed batch 569 of 3699\n",
      "Completed batch 570 of 3699\n",
      "Completed batch 571 of 3699\n",
      "Completed batch 572 of 3699\n",
      "Completed batch 573 of 3699\n",
      "Completed batch 574 of 3699\n",
      "Completed batch 575 of 3699\n",
      "Completed batch 576 of 3699\n",
      "Completed batch 577 of 3699\n",
      "Completed batch 578 of 3699\n",
      "Completed batch 579 of 3699\n",
      "Completed batch 580 of 3699\n",
      "Completed batch 581 of 3699\n",
      "Completed batch 582 of 3699\n",
      "Completed batch 583 of 3699\n",
      "Completed batch 584 of 3699\n",
      "Completed batch 585 of 3699\n",
      "Completed batch 586 of 3699\n",
      "Completed batch 587 of 3699\n",
      "Completed batch 588 of 3699\n",
      "Completed batch 589 of 3699\n",
      "Completed batch 590 of 3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 591 of 3699\n",
      "Completed batch 592 of 3699\n",
      "Completed batch 593 of 3699\n",
      "Completed batch 594 of 3699\n",
      "Completed batch 595 of 3699\n",
      "Completed batch 596 of 3699\n",
      "Completed batch 597 of 3699\n",
      "Completed batch 598 of 3699\n",
      "Completed batch 599 of 3699\n",
      "Completed batch 600 of 3699\n",
      "Completed batch 601 of 3699\n",
      "Completed batch 602 of 3699\n",
      "Completed batch 603 of 3699\n",
      "Completed batch 604 of 3699\n",
      "Completed batch 605 of 3699\n",
      "Completed batch 606 of 3699\n",
      "Completed batch 607 of 3699\n",
      "Completed batch 608 of 3699\n",
      "Completed batch 609 of 3699\n",
      "Completed batch 610 of 3699\n",
      "Completed batch 611 of 3699\n",
      "Completed batch 612 of 3699\n",
      "Completed batch 613 of 3699\n",
      "Completed batch 614 of 3699\n",
      "Completed batch 615 of 3699\n",
      "Completed batch 616 of 3699\n",
      "Completed batch 617 of 3699\n",
      "Completed batch 618 of 3699\n",
      "Completed batch 619 of 3699\n",
      "Completed batch 620 of 3699\n",
      "Completed batch 621 of 3699\n",
      "Completed batch 622 of 3699\n",
      "Completed batch 623 of 3699\n",
      "Completed batch 624 of 3699\n",
      "Completed batch 625 of 3699\n",
      "Completed batch 626 of 3699\n",
      "Completed batch 627 of 3699\n",
      "Completed batch 628 of 3699\n",
      "Completed batch 629 of 3699\n",
      "Completed batch 630 of 3699\n",
      "Completed batch 631 of 3699\n",
      "Completed batch 632 of 3699\n",
      "Completed batch 633 of 3699\n",
      "Completed batch 634 of 3699\n",
      "Completed batch 635 of 3699\n",
      "Completed batch 636 of 3699\n",
      "Completed batch 637 of 3699\n",
      "Completed batch 638 of 3699\n",
      "Completed batch 639 of 3699\n",
      "Completed batch 640 of 3699\n",
      "Completed batch 641 of 3699\n",
      "Completed batch 642 of 3699\n",
      "Completed batch 643 of 3699\n",
      "Completed batch 644 of 3699\n",
      "Completed batch 645 of 3699\n",
      "Completed batch 646 of 3699\n",
      "Completed batch 647 of 3699\n",
      "Completed batch 648 of 3699\n",
      "Completed batch 649 of 3699\n",
      "Completed batch 650 of 3699\n",
      "Completed batch 651 of 3699\n",
      "Completed batch 652 of 3699\n",
      "Completed batch 653 of 3699\n",
      "Completed batch 654 of 3699\n",
      "Completed batch 655 of 3699\n",
      "Completed batch 656 of 3699\n",
      "Completed batch 657 of 3699\n",
      "Completed batch 658 of 3699\n",
      "Completed batch 659 of 3699\n",
      "Completed batch 660 of 3699\n",
      "Completed batch 661 of 3699\n",
      "Completed batch 662 of 3699\n",
      "Completed batch 663 of 3699\n",
      "Completed batch 664 of 3699\n",
      "Completed batch 665 of 3699\n",
      "Completed batch 666 of 3699\n",
      "Completed batch 667 of 3699\n",
      "Completed batch 668 of 3699\n",
      "Completed batch 669 of 3699\n",
      "Completed batch 670 of 3699\n",
      "Completed batch 671 of 3699\n",
      "Completed batch 672 of 3699\n",
      "Completed batch 673 of 3699\n",
      "Completed batch 674 of 3699\n",
      "Completed batch 675 of 3699\n",
      "Completed batch 676 of 3699\n",
      "Completed batch 677 of 3699\n",
      "Completed batch 678 of 3699\n",
      "Completed batch 679 of 3699\n",
      "Completed batch 680 of 3699\n",
      "Completed batch 681 of 3699\n",
      "Completed batch 682 of 3699\n",
      "Completed batch 683 of 3699\n",
      "Completed batch 684 of 3699\n",
      "Completed batch 685 of 3699\n",
      "Completed batch 686 of 3699\n",
      "Completed batch 687 of 3699\n",
      "Completed batch 688 of 3699\n",
      "Completed batch 689 of 3699\n",
      "Completed batch 690 of 3699\n",
      "Completed batch 691 of 3699\n",
      "Completed batch 692 of 3699\n",
      "Completed batch 693 of 3699\n",
      "Completed batch 694 of 3699\n",
      "Completed batch 695 of 3699\n",
      "Completed batch 696 of 3699\n",
      "Completed batch 697 of 3699\n",
      "Completed batch 698 of 3699\n",
      "Completed batch 699 of 3699\n",
      "Completed batch 700 of 3699\n",
      "Completed batch 701 of 3699\n",
      "Completed batch 702 of 3699\n",
      "Completed batch 703 of 3699\n",
      "Completed batch 704 of 3699\n",
      "Completed batch 705 of 3699\n",
      "Completed batch 706 of 3699\n",
      "Completed batch 707 of 3699\n",
      "Completed batch 708 of 3699\n",
      "Completed batch 709 of 3699\n",
      "Completed batch 710 of 3699\n",
      "Completed batch 711 of 3699\n",
      "Completed batch 712 of 3699\n",
      "Completed batch 713 of 3699\n",
      "Completed batch 714 of 3699\n",
      "Completed batch 715 of 3699\n",
      "Completed batch 716 of 3699\n",
      "Completed batch 717 of 3699\n",
      "Completed batch 718 of 3699\n",
      "Completed batch 719 of 3699\n",
      "Completed batch 720 of 3699\n",
      "Completed batch 721 of 3699\n",
      "Completed batch 722 of 3699\n",
      "Completed batch 723 of 3699\n",
      "Completed batch 724 of 3699\n",
      "Completed batch 725 of 3699\n",
      "Completed batch 726 of 3699\n",
      "Completed batch 727 of 3699\n",
      "Completed batch 728 of 3699\n",
      "Completed batch 729 of 3699\n",
      "Completed batch 730 of 3699\n",
      "Completed batch 731 of 3699\n",
      "Completed batch 732 of 3699\n",
      "Completed batch 733 of 3699\n",
      "Completed batch 734 of 3699\n",
      "Completed batch 735 of 3699\n",
      "Completed batch 736 of 3699\n",
      "Completed batch 737 of 3699\n",
      "Completed batch 738 of 3699\n",
      "Completed batch 739 of 3699\n",
      "Completed batch 740 of 3699\n",
      "Completed batch 741 of 3699\n",
      "Completed batch 742 of 3699\n",
      "Completed batch 743 of 3699\n",
      "Completed batch 744 of 3699\n",
      "Completed batch 745 of 3699\n",
      "Completed batch 746 of 3699\n",
      "Completed batch 747 of 3699\n",
      "Completed batch 748 of 3699\n",
      "Completed batch 749 of 3699\n",
      "Completed batch 750 of 3699\n",
      "Completed batch 751 of 3699\n",
      "Completed batch 752 of 3699\n",
      "Completed batch 753 of 3699\n",
      "Completed batch 754 of 3699\n",
      "Completed batch 755 of 3699\n",
      "Completed batch 756 of 3699\n",
      "Completed batch 757 of 3699\n",
      "Completed batch 758 of 3699\n",
      "Completed batch 759 of 3699\n",
      "Completed batch 760 of 3699\n",
      "Completed batch 761 of 3699\n",
      "Completed batch 762 of 3699\n",
      "Completed batch 763 of 3699\n",
      "Completed batch 764 of 3699\n",
      "Completed batch 765 of 3699\n",
      "Completed batch 766 of 3699\n",
      "Completed batch 767 of 3699\n",
      "Completed batch 768 of 3699\n",
      "Completed batch 769 of 3699\n",
      "Completed batch 770 of 3699\n",
      "Completed batch 771 of 3699\n",
      "Completed batch 772 of 3699\n",
      "Completed batch 773 of 3699\n",
      "Completed batch 774 of 3699\n",
      "Completed batch 775 of 3699\n",
      "Completed batch 776 of 3699\n",
      "Completed batch 777 of 3699\n",
      "Completed batch 778 of 3699\n",
      "Completed batch 779 of 3699\n",
      "Completed batch 780 of 3699\n",
      "Completed batch 781 of 3699\n",
      "Completed batch 782 of 3699\n",
      "Completed batch 783 of 3699\n",
      "Completed batch 784 of 3699\n",
      "Completed batch 785 of 3699\n",
      "Completed batch 786 of 3699\n",
      "Completed batch 787 of 3699\n",
      "Completed batch 788 of 3699\n",
      "Completed batch 789 of 3699\n",
      "Completed batch 790 of 3699\n",
      "Completed batch 791 of 3699\n",
      "Completed batch 792 of 3699\n",
      "Completed batch 793 of 3699\n",
      "Completed batch 794 of 3699\n",
      "Completed batch 795 of 3699\n",
      "Completed batch 796 of 3699\n",
      "Completed batch 797 of 3699\n",
      "Completed batch 798 of 3699\n",
      "Completed batch 799 of 3699\n",
      "Completed batch 800 of 3699\n",
      "Completed batch 801 of 3699\n",
      "Completed batch 802 of 3699\n",
      "Completed batch 803 of 3699\n",
      "Completed batch 804 of 3699\n",
      "Completed batch 805 of 3699\n",
      "Completed batch 806 of 3699\n",
      "Completed batch 807 of 3699\n",
      "Completed batch 808 of 3699\n",
      "Completed batch 809 of 3699\n",
      "Completed batch 810 of 3699\n",
      "Completed batch 811 of 3699\n",
      "Completed batch 812 of 3699\n",
      "Completed batch 813 of 3699\n",
      "Completed batch 814 of 3699\n",
      "Completed batch 815 of 3699\n",
      "Completed batch 816 of 3699\n",
      "Completed batch 817 of 3699\n",
      "Completed batch 818 of 3699\n",
      "Completed batch 819 of 3699\n",
      "Completed batch 820 of 3699\n",
      "Completed batch 821 of 3699\n",
      "Completed batch 822 of 3699\n",
      "Completed batch 823 of 3699\n",
      "Completed batch 824 of 3699\n",
      "Completed batch 825 of 3699\n",
      "Completed batch 826 of 3699\n",
      "Completed batch 827 of 3699\n",
      "Completed batch 828 of 3699\n",
      "Completed batch 829 of 3699\n",
      "Completed batch 830 of 3699\n",
      "Completed batch 831 of 3699\n",
      "Completed batch 832 of 3699\n",
      "Completed batch 833 of 3699\n",
      "Completed batch 834 of 3699\n",
      "Completed batch 835 of 3699\n",
      "Completed batch 836 of 3699\n",
      "Completed batch 837 of 3699\n",
      "Completed batch 838 of 3699\n",
      "Completed batch 839 of 3699\n",
      "Completed batch 840 of 3699\n",
      "Completed batch 841 of 3699\n",
      "Completed batch 842 of 3699\n",
      "Completed batch 843 of 3699\n",
      "Completed batch 844 of 3699\n",
      "Completed batch 845 of 3699\n",
      "Completed batch 846 of 3699\n",
      "Completed batch 847 of 3699\n",
      "Completed batch 848 of 3699\n",
      "Completed batch 849 of 3699\n",
      "Completed batch 850 of 3699\n",
      "Completed batch 851 of 3699\n",
      "Completed batch 852 of 3699\n",
      "Completed batch 853 of 3699\n",
      "Completed batch 854 of 3699\n",
      "Completed batch 855 of 3699\n",
      "Completed batch 856 of 3699\n",
      "Completed batch 857 of 3699\n",
      "Completed batch 858 of 3699\n",
      "Completed batch 859 of 3699\n",
      "Completed batch 860 of 3699\n",
      "Completed batch 861 of 3699\n",
      "Completed batch 862 of 3699\n",
      "Completed batch 863 of 3699\n",
      "Completed batch 864 of 3699\n",
      "Completed batch 865 of 3699\n",
      "Completed batch 866 of 3699\n",
      "Completed batch 867 of 3699\n",
      "Completed batch 868 of 3699\n",
      "Completed batch 869 of 3699\n",
      "Completed batch 870 of 3699\n",
      "Completed batch 871 of 3699\n",
      "Completed batch 872 of 3699\n",
      "Completed batch 873 of 3699\n",
      "Completed batch 874 of 3699\n",
      "Completed batch 875 of 3699\n",
      "Completed batch 876 of 3699\n",
      "Completed batch 877 of 3699\n",
      "Completed batch 878 of 3699\n",
      "Completed batch 879 of 3699\n",
      "Completed batch 880 of 3699\n",
      "Completed batch 881 of 3699\n",
      "Completed batch 882 of 3699\n",
      "Completed batch 883 of 3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 884 of 3699\n",
      "Completed batch 885 of 3699\n",
      "Completed batch 886 of 3699\n",
      "Completed batch 887 of 3699\n",
      "Completed batch 888 of 3699\n",
      "Completed batch 889 of 3699\n",
      "Completed batch 890 of 3699\n",
      "Completed batch 891 of 3699\n",
      "Completed batch 892 of 3699\n",
      "Completed batch 893 of 3699\n",
      "Completed batch 894 of 3699\n",
      "Completed batch 895 of 3699\n",
      "Completed batch 896 of 3699\n",
      "Completed batch 897 of 3699\n",
      "Completed batch 898 of 3699\n",
      "Completed batch 899 of 3699\n",
      "Completed batch 900 of 3699\n",
      "Completed batch 901 of 3699\n",
      "Completed batch 902 of 3699\n",
      "Completed batch 903 of 3699\n",
      "Completed batch 904 of 3699\n",
      "Completed batch 905 of 3699\n",
      "Completed batch 906 of 3699\n",
      "Completed batch 907 of 3699\n",
      "Completed batch 908 of 3699\n",
      "Completed batch 909 of 3699\n",
      "Completed batch 910 of 3699\n",
      "Completed batch 911 of 3699\n",
      "Completed batch 912 of 3699\n",
      "Completed batch 913 of 3699\n",
      "Completed batch 914 of 3699\n",
      "Completed batch 915 of 3699\n",
      "Completed batch 916 of 3699\n",
      "Completed batch 917 of 3699\n",
      "Completed batch 918 of 3699\n",
      "Completed batch 919 of 3699\n",
      "Completed batch 920 of 3699\n",
      "Completed batch 921 of 3699\n",
      "Completed batch 922 of 3699\n",
      "Completed batch 923 of 3699\n",
      "Completed batch 924 of 3699\n",
      "Completed batch 925 of 3699\n",
      "Completed batch 926 of 3699\n",
      "Completed batch 927 of 3699\n",
      "Completed batch 928 of 3699\n",
      "Completed batch 929 of 3699\n",
      "Completed batch 930 of 3699\n",
      "Completed batch 931 of 3699\n",
      "Completed batch 932 of 3699\n",
      "Completed batch 933 of 3699\n",
      "Completed batch 934 of 3699\n",
      "Completed batch 935 of 3699\n",
      "Completed batch 936 of 3699\n",
      "Completed batch 937 of 3699\n",
      "Completed batch 938 of 3699\n",
      "Completed batch 939 of 3699\n",
      "Completed batch 940 of 3699\n",
      "Completed batch 941 of 3699\n",
      "Completed batch 942 of 3699\n",
      "Completed batch 943 of 3699\n",
      "Completed batch 944 of 3699\n",
      "Completed batch 945 of 3699\n",
      "Completed batch 946 of 3699\n",
      "Completed batch 947 of 3699\n",
      "Completed batch 948 of 3699\n",
      "Completed batch 949 of 3699\n",
      "Completed batch 950 of 3699\n",
      "Completed batch 951 of 3699\n",
      "Completed batch 952 of 3699\n",
      "Completed batch 953 of 3699\n",
      "Completed batch 954 of 3699\n",
      "Completed batch 955 of 3699\n",
      "Completed batch 956 of 3699\n",
      "Completed batch 957 of 3699\n",
      "Completed batch 958 of 3699\n",
      "Completed batch 959 of 3699\n",
      "Completed batch 960 of 3699\n",
      "Completed batch 961 of 3699\n",
      "Completed batch 962 of 3699\n",
      "Completed batch 963 of 3699\n",
      "Completed batch 964 of 3699\n",
      "Completed batch 965 of 3699\n",
      "Completed batch 966 of 3699\n",
      "Completed batch 967 of 3699\n",
      "Completed batch 968 of 3699\n",
      "Completed batch 969 of 3699\n",
      "Completed batch 970 of 3699\n",
      "Completed batch 971 of 3699\n",
      "Completed batch 972 of 3699\n",
      "Completed batch 973 of 3699\n",
      "Completed batch 974 of 3699\n",
      "Completed batch 975 of 3699\n",
      "Completed batch 976 of 3699\n",
      "Completed batch 977 of 3699\n",
      "Completed batch 978 of 3699\n",
      "Completed batch 979 of 3699\n",
      "Completed batch 980 of 3699\n",
      "Completed batch 981 of 3699\n",
      "Completed batch 982 of 3699\n",
      "Completed batch 983 of 3699\n",
      "Completed batch 984 of 3699\n",
      "Completed batch 985 of 3699\n",
      "Completed batch 986 of 3699\n",
      "Completed batch 987 of 3699\n",
      "Completed batch 988 of 3699\n",
      "Completed batch 989 of 3699\n",
      "Completed batch 990 of 3699\n",
      "Completed batch 991 of 3699\n",
      "Completed batch 992 of 3699\n",
      "Completed batch 993 of 3699\n",
      "Completed batch 994 of 3699\n",
      "Completed batch 995 of 3699\n",
      "Completed batch 996 of 3699\n",
      "Completed batch 997 of 3699\n",
      "Completed batch 998 of 3699\n",
      "Completed batch 999 of 3699\n",
      "Completed batch 1000 of 3699\n",
      "Completed batch 1001 of 3699\n",
      "Completed batch 1002 of 3699\n",
      "Completed batch 1003 of 3699\n",
      "Completed batch 1004 of 3699\n",
      "Completed batch 1005 of 3699\n",
      "Completed batch 1006 of 3699\n",
      "Completed batch 1007 of 3699\n",
      "Completed batch 1008 of 3699\n",
      "Completed batch 1009 of 3699\n",
      "Completed batch 1010 of 3699\n",
      "Completed batch 1011 of 3699\n",
      "Completed batch 1012 of 3699\n",
      "Completed batch 1013 of 3699\n",
      "Completed batch 1014 of 3699\n",
      "Completed batch 1015 of 3699\n",
      "Completed batch 1016 of 3699\n",
      "Completed batch 1017 of 3699\n",
      "Completed batch 1018 of 3699\n",
      "Completed batch 1019 of 3699\n",
      "Completed batch 1020 of 3699\n",
      "Completed batch 1021 of 3699\n",
      "Completed batch 1022 of 3699\n",
      "Completed batch 1023 of 3699\n",
      "Completed batch 1024 of 3699\n",
      "Completed batch 1025 of 3699\n",
      "Completed batch 1026 of 3699\n",
      "Completed batch 1027 of 3699\n",
      "Completed batch 1028 of 3699\n",
      "Completed batch 1029 of 3699\n",
      "Completed batch 1030 of 3699\n",
      "Completed batch 1031 of 3699\n",
      "Completed batch 1032 of 3699\n",
      "Completed batch 1033 of 3699\n",
      "Completed batch 1034 of 3699\n",
      "Completed batch 1035 of 3699\n",
      "Completed batch 1036 of 3699\n",
      "Completed batch 1037 of 3699\n",
      "Completed batch 1038 of 3699\n",
      "Completed batch 1039 of 3699\n",
      "Completed batch 1040 of 3699\n",
      "Completed batch 1041 of 3699\n",
      "Completed batch 1042 of 3699\n",
      "Completed batch 1043 of 3699\n",
      "Completed batch 1044 of 3699\n",
      "Completed batch 1045 of 3699\n",
      "Completed batch 1046 of 3699\n",
      "Completed batch 1047 of 3699\n",
      "Completed batch 1048 of 3699\n",
      "Completed batch 1049 of 3699\n",
      "Completed batch 1050 of 3699\n",
      "Completed batch 1051 of 3699\n",
      "Completed batch 1052 of 3699\n",
      "Completed batch 1053 of 3699\n",
      "Completed batch 1054 of 3699\n",
      "Completed batch 1055 of 3699\n",
      "Completed batch 1056 of 3699\n",
      "Completed batch 1057 of 3699\n",
      "Completed batch 1058 of 3699\n",
      "Completed batch 1059 of 3699\n",
      "Completed batch 1060 of 3699\n",
      "Completed batch 1061 of 3699\n",
      "Completed batch 1062 of 3699\n",
      "Completed batch 1063 of 3699\n",
      "Completed batch 1064 of 3699\n",
      "Completed batch 1065 of 3699\n",
      "Completed batch 1066 of 3699\n",
      "Completed batch 1067 of 3699\n",
      "Completed batch 1068 of 3699\n",
      "Completed batch 1069 of 3699\n",
      "Completed batch 1070 of 3699\n",
      "Completed batch 1071 of 3699\n",
      "Completed batch 1072 of 3699\n",
      "Completed batch 1073 of 3699\n",
      "Completed batch 1074 of 3699\n",
      "Completed batch 1075 of 3699\n",
      "Completed batch 1076 of 3699\n",
      "Completed batch 1077 of 3699\n",
      "Completed batch 1078 of 3699\n",
      "Completed batch 1079 of 3699\n",
      "Completed batch 1080 of 3699\n",
      "Completed batch 1081 of 3699\n",
      "Completed batch 1082 of 3699\n",
      "Completed batch 1083 of 3699\n",
      "Completed batch 1084 of 3699\n",
      "Completed batch 1085 of 3699\n",
      "Completed batch 1086 of 3699\n",
      "Completed batch 1087 of 3699\n",
      "Completed batch 1088 of 3699\n",
      "Completed batch 1089 of 3699\n",
      "Completed batch 1090 of 3699\n",
      "Completed batch 1091 of 3699\n",
      "Completed batch 1092 of 3699\n",
      "Completed batch 1093 of 3699\n",
      "Completed batch 1094 of 3699\n",
      "Completed batch 1095 of 3699\n",
      "Completed batch 1096 of 3699\n",
      "Completed batch 1097 of 3699\n",
      "Completed batch 1098 of 3699\n",
      "Completed batch 1099 of 3699\n",
      "Completed batch 1100 of 3699\n",
      "Completed batch 1101 of 3699\n",
      "Completed batch 1102 of 3699\n",
      "Completed batch 1103 of 3699\n",
      "Completed batch 1104 of 3699\n",
      "Completed batch 1105 of 3699\n",
      "Completed batch 1106 of 3699\n",
      "Completed batch 1107 of 3699\n",
      "Completed batch 1108 of 3699\n",
      "Completed batch 1109 of 3699\n",
      "Completed batch 1110 of 3699\n",
      "Completed batch 1111 of 3699\n",
      "Completed batch 1112 of 3699\n",
      "Completed batch 1113 of 3699\n",
      "Completed batch 1114 of 3699\n",
      "Completed batch 1115 of 3699\n",
      "Completed batch 1116 of 3699\n",
      "Completed batch 1117 of 3699\n",
      "Completed batch 1118 of 3699\n",
      "Completed batch 1119 of 3699\n",
      "Completed batch 1120 of 3699\n",
      "Completed batch 1121 of 3699\n",
      "Completed batch 1122 of 3699\n",
      "Completed batch 1123 of 3699\n",
      "Completed batch 1124 of 3699\n",
      "Completed batch 1125 of 3699\n",
      "Completed batch 1126 of 3699\n",
      "Completed batch 1127 of 3699\n",
      "Completed batch 1128 of 3699\n",
      "Completed batch 1129 of 3699\n",
      "Completed batch 1130 of 3699\n",
      "Completed batch 1131 of 3699\n",
      "Completed batch 1132 of 3699\n",
      "Completed batch 1133 of 3699\n",
      "Completed batch 1134 of 3699\n",
      "Completed batch 1135 of 3699\n",
      "Completed batch 1136 of 3699\n",
      "Completed batch 1137 of 3699\n",
      "Completed batch 1138 of 3699\n",
      "Completed batch 1139 of 3699\n",
      "Completed batch 1140 of 3699\n",
      "Completed batch 1141 of 3699\n",
      "Completed batch 1142 of 3699\n",
      "Completed batch 1143 of 3699\n",
      "Completed batch 1144 of 3699\n",
      "Completed batch 1145 of 3699\n",
      "Completed batch 1146 of 3699\n",
      "Completed batch 1147 of 3699\n",
      "Completed batch 1148 of 3699\n",
      "Completed batch 1149 of 3699\n",
      "Completed batch 1150 of 3699\n",
      "Completed batch 1151 of 3699\n",
      "Completed batch 1152 of 3699\n",
      "Completed batch 1153 of 3699\n",
      "Completed batch 1154 of 3699\n",
      "Completed batch 1155 of 3699\n",
      "Completed batch 1156 of 3699\n",
      "Completed batch 1157 of 3699\n",
      "Completed batch 1158 of 3699\n",
      "Completed batch 1159 of 3699\n",
      "Completed batch 1160 of 3699\n",
      "Completed batch 1161 of 3699\n",
      "Completed batch 1162 of 3699\n",
      "Completed batch 1163 of 3699\n",
      "Completed batch 1164 of 3699\n",
      "Completed batch 1165 of 3699\n",
      "Completed batch 1166 of 3699\n",
      "Completed batch 1167 of 3699\n",
      "Completed batch 1168 of 3699\n",
      "Completed batch 1169 of 3699\n",
      "Completed batch 1170 of 3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1171 of 3699\n",
      "Completed batch 1172 of 3699\n",
      "Completed batch 1173 of 3699\n",
      "Completed batch 1174 of 3699\n",
      "Completed batch 1175 of 3699\n",
      "Completed batch 1176 of 3699\n",
      "Completed batch 1177 of 3699\n",
      "Completed batch 1178 of 3699\n",
      "Completed batch 1179 of 3699\n",
      "Completed batch 1180 of 3699\n",
      "Completed batch 1181 of 3699\n",
      "Completed batch 1182 of 3699\n",
      "Completed batch 1183 of 3699\n",
      "Completed batch 1184 of 3699\n",
      "Completed batch 1185 of 3699\n",
      "Completed batch 1186 of 3699\n",
      "Completed batch 1187 of 3699\n",
      "Completed batch 1188 of 3699\n",
      "Completed batch 1189 of 3699\n",
      "Completed batch 1190 of 3699\n",
      "Completed batch 1191 of 3699\n",
      "Completed batch 1192 of 3699\n",
      "Completed batch 1193 of 3699\n",
      "Completed batch 1194 of 3699\n",
      "Completed batch 1195 of 3699\n",
      "Completed batch 1196 of 3699\n",
      "Completed batch 1197 of 3699\n",
      "Completed batch 1198 of 3699\n",
      "Completed batch 1199 of 3699\n",
      "Completed batch 1200 of 3699\n",
      "Completed batch 1201 of 3699\n",
      "Completed batch 1202 of 3699\n",
      "Completed batch 1203 of 3699\n",
      "Completed batch 1204 of 3699\n",
      "Completed batch 1205 of 3699\n",
      "Completed batch 1206 of 3699\n",
      "Completed batch 1207 of 3699\n",
      "Completed batch 1208 of 3699\n",
      "Completed batch 1209 of 3699\n",
      "Completed batch 1210 of 3699\n",
      "Completed batch 1211 of 3699\n",
      "Completed batch 1212 of 3699\n",
      "Completed batch 1213 of 3699\n",
      "Completed batch 1214 of 3699\n",
      "Completed batch 1215 of 3699\n",
      "Completed batch 1216 of 3699\n",
      "Completed batch 1217 of 3699\n",
      "Completed batch 1218 of 3699\n",
      "Completed batch 1219 of 3699\n",
      "Completed batch 1220 of 3699\n",
      "Completed batch 1221 of 3699\n",
      "Completed batch 1222 of 3699\n",
      "Completed batch 1223 of 3699\n",
      "Completed batch 1224 of 3699\n",
      "Completed batch 1225 of 3699\n",
      "Completed batch 1226 of 3699\n",
      "Completed batch 1227 of 3699\n",
      "Completed batch 1228 of 3699\n",
      "Completed batch 1229 of 3699\n",
      "Completed batch 1230 of 3699\n",
      "Completed batch 1231 of 3699\n",
      "Completed batch 1232 of 3699\n",
      "Completed batch 1233 of 3699\n",
      "Completed batch 1234 of 3699\n",
      "Completed batch 1235 of 3699\n",
      "Completed batch 1236 of 3699\n",
      "Completed batch 1237 of 3699\n",
      "Completed batch 1238 of 3699\n",
      "Completed batch 1239 of 3699\n",
      "Completed batch 1240 of 3699\n",
      "Completed batch 1241 of 3699\n",
      "Completed batch 1242 of 3699\n",
      "Completed batch 1243 of 3699\n",
      "Completed batch 1244 of 3699\n",
      "Completed batch 1245 of 3699\n",
      "Completed batch 1246 of 3699\n",
      "Completed batch 1247 of 3699\n",
      "Completed batch 1248 of 3699\n",
      "Completed batch 1249 of 3699\n",
      "Completed batch 1250 of 3699\n",
      "Completed batch 1251 of 3699\n",
      "Completed batch 1252 of 3699\n",
      "Completed batch 1253 of 3699\n",
      "Completed batch 1254 of 3699\n",
      "Completed batch 1255 of 3699\n",
      "Completed batch 1256 of 3699\n",
      "Completed batch 1257 of 3699\n",
      "Completed batch 1258 of 3699\n",
      "Completed batch 1259 of 3699\n",
      "Completed batch 1260 of 3699\n",
      "Completed batch 1261 of 3699\n",
      "Completed batch 1262 of 3699\n",
      "Completed batch 1263 of 3699\n",
      "Completed batch 1264 of 3699\n",
      "Completed batch 1265 of 3699\n",
      "Completed batch 1266 of 3699\n",
      "Completed batch 1267 of 3699\n",
      "Completed batch 1268 of 3699\n",
      "Completed batch 1269 of 3699\n",
      "Completed batch 1270 of 3699\n",
      "Completed batch 1271 of 3699\n",
      "Completed batch 1272 of 3699\n",
      "Completed batch 1273 of 3699\n",
      "Completed batch 1274 of 3699\n",
      "Completed batch 1275 of 3699\n",
      "Completed batch 1276 of 3699\n",
      "Completed batch 1277 of 3699\n",
      "Completed batch 1278 of 3699\n",
      "Completed batch 1279 of 3699\n",
      "Completed batch 1280 of 3699\n",
      "Completed batch 1281 of 3699\n",
      "Completed batch 1282 of 3699\n",
      "Completed batch 1283 of 3699\n",
      "Completed batch 1284 of 3699\n",
      "Completed batch 1285 of 3699\n",
      "Completed batch 1286 of 3699\n",
      "Completed batch 1287 of 3699\n",
      "Completed batch 1288 of 3699\n",
      "Completed batch 1289 of 3699\n",
      "Completed batch 1290 of 3699\n",
      "Completed batch 1291 of 3699\n",
      "Completed batch 1292 of 3699\n",
      "Completed batch 1293 of 3699\n",
      "Completed batch 1294 of 3699\n",
      "Completed batch 1295 of 3699\n",
      "Completed batch 1296 of 3699\n",
      "Completed batch 1297 of 3699\n",
      "Completed batch 1298 of 3699\n",
      "Completed batch 1299 of 3699\n",
      "Completed batch 1300 of 3699\n",
      "Completed batch 1301 of 3699\n",
      "Completed batch 1302 of 3699\n",
      "Completed batch 1303 of 3699\n",
      "Completed batch 1304 of 3699\n",
      "Completed batch 1305 of 3699\n",
      "Completed batch 1306 of 3699\n",
      "Completed batch 1307 of 3699\n",
      "Completed batch 1308 of 3699\n",
      "Completed batch 1309 of 3699\n",
      "Completed batch 1310 of 3699\n",
      "Completed batch 1311 of 3699\n",
      "Completed batch 1312 of 3699\n",
      "Completed batch 1313 of 3699\n",
      "Completed batch 1314 of 3699\n",
      "Completed batch 1315 of 3699\n",
      "Completed batch 1316 of 3699\n",
      "Completed batch 1317 of 3699\n",
      "Completed batch 1318 of 3699\n",
      "Completed batch 1319 of 3699\n",
      "Completed batch 1320 of 3699\n",
      "Completed batch 1321 of 3699\n",
      "Completed batch 1322 of 3699\n",
      "Completed batch 1323 of 3699\n",
      "Completed batch 1324 of 3699\n",
      "Completed batch 1325 of 3699\n",
      "Completed batch 1326 of 3699\n",
      "Completed batch 1327 of 3699\n",
      "Completed batch 1328 of 3699\n",
      "Completed batch 1329 of 3699\n",
      "Completed batch 1330 of 3699\n",
      "Completed batch 1331 of 3699\n",
      "Completed batch 1332 of 3699\n",
      "Completed batch 1333 of 3699\n",
      "Completed batch 1334 of 3699\n",
      "Completed batch 1335 of 3699\n",
      "Completed batch 1336 of 3699\n",
      "Completed batch 1337 of 3699\n",
      "Completed batch 1338 of 3699\n",
      "Completed batch 1339 of 3699\n",
      "Completed batch 1340 of 3699\n",
      "Completed batch 1341 of 3699\n",
      "Completed batch 1342 of 3699\n",
      "Completed batch 1343 of 3699\n",
      "Completed batch 1344 of 3699\n",
      "Completed batch 1345 of 3699\n",
      "Completed batch 1346 of 3699\n",
      "Completed batch 1347 of 3699\n",
      "Completed batch 1348 of 3699\n",
      "Completed batch 1349 of 3699\n",
      "Completed batch 1350 of 3699\n",
      "Completed batch 1351 of 3699\n",
      "Completed batch 1352 of 3699\n",
      "Completed batch 1353 of 3699\n",
      "Completed batch 1354 of 3699\n",
      "Completed batch 1355 of 3699\n",
      "Completed batch 1356 of 3699\n",
      "Completed batch 1357 of 3699\n",
      "Completed batch 1358 of 3699\n",
      "Completed batch 1359 of 3699\n",
      "Completed batch 1360 of 3699\n",
      "Completed batch 1361 of 3699\n",
      "Completed batch 1362 of 3699\n",
      "Completed batch 1363 of 3699\n",
      "Completed batch 1364 of 3699\n",
      "Completed batch 1365 of 3699\n",
      "Completed batch 1366 of 3699\n",
      "Completed batch 1367 of 3699\n",
      "Completed batch 1368 of 3699\n",
      "Completed batch 1369 of 3699\n",
      "Completed batch 1370 of 3699\n",
      "Completed batch 1371 of 3699\n",
      "Completed batch 1372 of 3699\n",
      "Completed batch 1373 of 3699\n",
      "Completed batch 1374 of 3699\n",
      "Completed batch 1375 of 3699\n",
      "Completed batch 1376 of 3699\n",
      "Completed batch 1377 of 3699\n",
      "Completed batch 1378 of 3699\n",
      "Completed batch 1379 of 3699\n",
      "Completed batch 1380 of 3699\n",
      "Completed batch 1381 of 3699\n",
      "Completed batch 1382 of 3699\n",
      "Completed batch 1383 of 3699\n",
      "Completed batch 1384 of 3699\n",
      "Completed batch 1385 of 3699\n",
      "Completed batch 1386 of 3699\n",
      "Completed batch 1387 of 3699\n",
      "Completed batch 1388 of 3699\n",
      "Completed batch 1389 of 3699\n",
      "Completed batch 1390 of 3699\n",
      "Completed batch 1391 of 3699\n",
      "Completed batch 1392 of 3699\n",
      "Completed batch 1393 of 3699\n",
      "Completed batch 1394 of 3699\n",
      "Completed batch 1395 of 3699\n",
      "Completed batch 1396 of 3699\n",
      "Completed batch 1397 of 3699\n",
      "Completed batch 1398 of 3699\n",
      "Completed batch 1399 of 3699\n",
      "Completed batch 1400 of 3699\n",
      "Completed batch 1401 of 3699\n",
      "Completed batch 1402 of 3699\n",
      "Completed batch 1403 of 3699\n",
      "Completed batch 1404 of 3699\n",
      "Completed batch 1405 of 3699\n",
      "Completed batch 1406 of 3699\n",
      "Completed batch 1407 of 3699\n",
      "Completed batch 1408 of 3699\n",
      "Completed batch 1409 of 3699\n",
      "Completed batch 1410 of 3699\n",
      "Completed batch 1411 of 3699\n",
      "Completed batch 1412 of 3699\n",
      "Completed batch 1413 of 3699\n",
      "Completed batch 1414 of 3699\n",
      "Completed batch 1415 of 3699\n",
      "Completed batch 1416 of 3699\n",
      "Completed batch 1417 of 3699\n",
      "Completed batch 1418 of 3699\n",
      "Completed batch 1419 of 3699\n",
      "Completed batch 1420 of 3699\n",
      "Completed batch 1421 of 3699\n",
      "Completed batch 1422 of 3699\n",
      "Completed batch 1423 of 3699\n",
      "Completed batch 1424 of 3699\n",
      "Completed batch 1425 of 3699\n",
      "Completed batch 1426 of 3699\n",
      "Completed batch 1427 of 3699\n",
      "Completed batch 1428 of 3699\n",
      "Completed batch 1429 of 3699\n",
      "Completed batch 1430 of 3699\n",
      "Completed batch 1431 of 3699\n",
      "Completed batch 1432 of 3699\n",
      "Completed batch 1433 of 3699\n",
      "Completed batch 1434 of 3699\n",
      "Completed batch 1435 of 3699\n",
      "Completed batch 1436 of 3699\n",
      "Completed batch 1437 of 3699\n",
      "Completed batch 1438 of 3699\n",
      "Completed batch 1439 of 3699\n",
      "Completed batch 1440 of 3699\n",
      "Completed batch 1441 of 3699\n",
      "Completed batch 1442 of 3699\n",
      "Completed batch 1443 of 3699\n",
      "Completed batch 1444 of 3699\n",
      "Completed batch 1445 of 3699\n",
      "Completed batch 1446 of 3699\n",
      "Completed batch 1447 of 3699\n",
      "Completed batch 1448 of 3699\n",
      "Completed batch 1449 of 3699\n",
      "Completed batch 1450 of 3699\n",
      "Completed batch 1451 of 3699\n",
      "Completed batch 1452 of 3699\n",
      "Completed batch 1453 of 3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1454 of 3699\n",
      "Completed batch 1455 of 3699\n",
      "Completed batch 1456 of 3699\n",
      "Completed batch 1457 of 3699\n",
      "Completed batch 1458 of 3699\n",
      "Completed batch 1459 of 3699\n",
      "Completed batch 1460 of 3699\n",
      "Completed batch 1461 of 3699\n",
      "Completed batch 1462 of 3699\n",
      "Completed batch 1463 of 3699\n",
      "Completed batch 1464 of 3699\n",
      "Completed batch 1465 of 3699\n",
      "Completed batch 1466 of 3699\n",
      "Completed batch 1467 of 3699\n",
      "Completed batch 1468 of 3699\n",
      "Completed batch 1469 of 3699\n",
      "Completed batch 1470 of 3699\n",
      "Completed batch 1471 of 3699\n",
      "Completed batch 1472 of 3699\n",
      "Completed batch 1473 of 3699\n",
      "Completed batch 1474 of 3699\n",
      "Completed batch 1475 of 3699\n",
      "Completed batch 1476 of 3699\n",
      "Completed batch 1477 of 3699\n",
      "Completed batch 1478 of 3699\n",
      "Completed batch 1479 of 3699\n",
      "Completed batch 1480 of 3699\n",
      "Completed batch 1481 of 3699\n",
      "Completed batch 1482 of 3699\n",
      "Completed batch 1483 of 3699\n",
      "Completed batch 1484 of 3699\n",
      "Completed batch 1485 of 3699\n",
      "Completed batch 1486 of 3699\n",
      "Completed batch 1487 of 3699\n",
      "Completed batch 1488 of 3699\n",
      "Completed batch 1489 of 3699\n",
      "Completed batch 1490 of 3699\n",
      "Completed batch 1491 of 3699\n",
      "Completed batch 1492 of 3699\n",
      "Completed batch 1493 of 3699\n",
      "Completed batch 1494 of 3699\n",
      "Completed batch 1495 of 3699\n",
      "Completed batch 1496 of 3699\n",
      "Completed batch 1497 of 3699\n",
      "Completed batch 1498 of 3699\n",
      "Completed batch 1499 of 3699\n",
      "Completed batch 1500 of 3699\n",
      "Completed batch 1501 of 3699\n",
      "Completed batch 1502 of 3699\n",
      "Completed batch 1503 of 3699\n",
      "Completed batch 1504 of 3699\n",
      "Completed batch 1505 of 3699\n",
      "Completed batch 1506 of 3699\n",
      "Completed batch 1507 of 3699\n",
      "Completed batch 1508 of 3699\n",
      "Completed batch 1509 of 3699\n",
      "Completed batch 1510 of 3699\n",
      "Completed batch 1511 of 3699\n",
      "Completed batch 1512 of 3699\n",
      "Completed batch 1513 of 3699\n",
      "Completed batch 1514 of 3699\n",
      "Completed batch 1515 of 3699\n",
      "Completed batch 1516 of 3699\n",
      "Completed batch 1517 of 3699\n",
      "Completed batch 1518 of 3699\n",
      "Completed batch 1519 of 3699\n",
      "Completed batch 1520 of 3699\n",
      "Completed batch 1521 of 3699\n",
      "Completed batch 1522 of 3699\n",
      "Completed batch 1523 of 3699\n",
      "Completed batch 1524 of 3699\n",
      "Completed batch 1525 of 3699\n",
      "Completed batch 1526 of 3699\n",
      "Completed batch 1527 of 3699\n",
      "Completed batch 1528 of 3699\n",
      "Completed batch 1529 of 3699\n",
      "Completed batch 1530 of 3699\n",
      "Completed batch 1531 of 3699\n",
      "Completed batch 1532 of 3699\n",
      "Completed batch 1533 of 3699\n",
      "Completed batch 1534 of 3699\n",
      "Completed batch 1535 of 3699\n",
      "Completed batch 1536 of 3699\n",
      "Completed batch 1537 of 3699\n",
      "Completed batch 1538 of 3699\n",
      "Completed batch 1539 of 3699\n",
      "Completed batch 1540 of 3699\n",
      "Completed batch 1541 of 3699\n",
      "Completed batch 1542 of 3699\n",
      "Completed batch 1543 of 3699\n",
      "Completed batch 1544 of 3699\n",
      "Completed batch 1545 of 3699\n",
      "Completed batch 1546 of 3699\n",
      "Completed batch 1547 of 3699\n",
      "Completed batch 1548 of 3699\n",
      "Completed batch 1549 of 3699\n",
      "Completed batch 1550 of 3699\n",
      "Completed batch 1551 of 3699\n",
      "Completed batch 1552 of 3699\n",
      "Completed batch 1553 of 3699\n",
      "Completed batch 1554 of 3699\n",
      "Completed batch 1555 of 3699\n",
      "Completed batch 1556 of 3699\n",
      "Completed batch 1557 of 3699\n",
      "Completed batch 1558 of 3699\n",
      "Completed batch 1559 of 3699\n",
      "Completed batch 1560 of 3699\n",
      "Completed batch 1561 of 3699\n",
      "Completed batch 1562 of 3699\n",
      "Completed batch 1563 of 3699\n",
      "Completed batch 1564 of 3699\n",
      "Completed batch 1565 of 3699\n",
      "Completed batch 1566 of 3699\n",
      "Completed batch 1567 of 3699\n",
      "Completed batch 1568 of 3699\n",
      "Completed batch 1569 of 3699\n",
      "Completed batch 1570 of 3699\n",
      "Completed batch 1571 of 3699\n",
      "Completed batch 1572 of 3699\n",
      "Completed batch 1573 of 3699\n",
      "Completed batch 1574 of 3699\n",
      "Completed batch 1575 of 3699\n",
      "Completed batch 1576 of 3699\n",
      "Completed batch 1577 of 3699\n",
      "Completed batch 1578 of 3699\n",
      "Completed batch 1579 of 3699\n",
      "Completed batch 1580 of 3699\n",
      "Completed batch 1581 of 3699\n",
      "Completed batch 1582 of 3699\n",
      "Completed batch 1583 of 3699\n",
      "Completed batch 1584 of 3699\n",
      "Completed batch 1585 of 3699\n",
      "Completed batch 1586 of 3699\n",
      "Completed batch 1587 of 3699\n",
      "Completed batch 1588 of 3699\n",
      "Completed batch 1589 of 3699\n",
      "Completed batch 1590 of 3699\n",
      "Completed batch 1591 of 3699\n",
      "Completed batch 1592 of 3699\n",
      "Completed batch 1593 of 3699\n",
      "Completed batch 1594 of 3699\n",
      "Completed batch 1595 of 3699\n",
      "Completed batch 1596 of 3699\n",
      "Completed batch 1597 of 3699\n",
      "Completed batch 1598 of 3699\n",
      "Completed batch 1599 of 3699\n",
      "Completed batch 1600 of 3699\n",
      "Completed batch 1601 of 3699\n",
      "Completed batch 1602 of 3699\n",
      "Completed batch 1603 of 3699\n",
      "Completed batch 1604 of 3699\n",
      "Completed batch 1605 of 3699\n",
      "Completed batch 1606 of 3699\n",
      "Completed batch 1607 of 3699\n",
      "Completed batch 1608 of 3699\n",
      "Completed batch 1609 of 3699\n",
      "Completed batch 1610 of 3699\n",
      "Completed batch 1611 of 3699\n",
      "Completed batch 1612 of 3699\n",
      "Completed batch 1613 of 3699\n",
      "Completed batch 1614 of 3699\n",
      "Completed batch 1615 of 3699\n",
      "Completed batch 1616 of 3699\n",
      "Completed batch 1617 of 3699\n",
      "Completed batch 1618 of 3699\n",
      "Completed batch 1619 of 3699\n",
      "Completed batch 1620 of 3699\n",
      "Completed batch 1621 of 3699\n",
      "Completed batch 1622 of 3699\n",
      "Completed batch 1623 of 3699\n",
      "Completed batch 1624 of 3699\n",
      "Completed batch 1625 of 3699\n",
      "Completed batch 1626 of 3699\n",
      "Completed batch 1627 of 3699\n",
      "Completed batch 1628 of 3699\n",
      "Completed batch 1629 of 3699\n",
      "Completed batch 1630 of 3699\n",
      "Completed batch 1631 of 3699\n",
      "Completed batch 1632 of 3699\n",
      "Completed batch 1633 of 3699\n",
      "Completed batch 1634 of 3699\n",
      "Completed batch 1635 of 3699\n",
      "Completed batch 1636 of 3699\n",
      "Completed batch 1637 of 3699\n",
      "Completed batch 1638 of 3699\n",
      "Completed batch 1639 of 3699\n",
      "Completed batch 1640 of 3699\n",
      "Completed batch 1641 of 3699\n",
      "Completed batch 1642 of 3699\n",
      "Completed batch 1643 of 3699\n",
      "Completed batch 1644 of 3699\n",
      "Completed batch 1645 of 3699\n",
      "Completed batch 1646 of 3699\n",
      "Completed batch 1647 of 3699\n",
      "Completed batch 1648 of 3699\n",
      "Completed batch 1649 of 3699\n",
      "Completed batch 1650 of 3699\n",
      "Completed batch 1651 of 3699\n",
      "Completed batch 1652 of 3699\n",
      "Completed batch 1653 of 3699\n",
      "Completed batch 1654 of 3699\n",
      "Completed batch 1655 of 3699\n",
      "Completed batch 1656 of 3699\n",
      "Completed batch 1657 of 3699\n",
      "Completed batch 1658 of 3699\n",
      "Completed batch 1659 of 3699\n",
      "Completed batch 1660 of 3699\n",
      "Completed batch 1661 of 3699\n",
      "Completed batch 1662 of 3699\n",
      "Completed batch 1663 of 3699\n",
      "Completed batch 1664 of 3699\n",
      "Completed batch 1665 of 3699\n",
      "Completed batch 1666 of 3699\n",
      "Completed batch 1667 of 3699\n",
      "Completed batch 1668 of 3699\n",
      "Completed batch 1669 of 3699\n",
      "Completed batch 1670 of 3699\n",
      "Completed batch 1671 of 3699\n",
      "Completed batch 1672 of 3699\n",
      "Completed batch 1673 of 3699\n",
      "Completed batch 1674 of 3699\n",
      "Completed batch 1675 of 3699\n",
      "Completed batch 1676 of 3699\n",
      "Completed batch 1677 of 3699\n",
      "Completed batch 1678 of 3699\n",
      "Completed batch 1679 of 3699\n",
      "Completed batch 1680 of 3699\n",
      "Completed batch 1681 of 3699\n",
      "Completed batch 1682 of 3699\n",
      "Completed batch 1683 of 3699\n",
      "Completed batch 1684 of 3699\n",
      "Completed batch 1685 of 3699\n",
      "Completed batch 1686 of 3699\n",
      "Completed batch 1687 of 3699\n",
      "Completed batch 1688 of 3699\n",
      "Completed batch 1689 of 3699\n",
      "Completed batch 1690 of 3699\n",
      "Completed batch 1691 of 3699\n",
      "Completed batch 1692 of 3699\n",
      "Completed batch 1693 of 3699\n",
      "Completed batch 1694 of 3699\n",
      "Completed batch 1695 of 3699\n",
      "Completed batch 1696 of 3699\n",
      "Completed batch 1697 of 3699\n",
      "Completed batch 1698 of 3699\n",
      "Completed batch 1699 of 3699\n",
      "Completed batch 1700 of 3699\n",
      "Completed batch 1701 of 3699\n",
      "Completed batch 1702 of 3699\n",
      "Completed batch 1703 of 3699\n",
      "Completed batch 1704 of 3699\n",
      "Completed batch 1705 of 3699\n",
      "Completed batch 1706 of 3699\n",
      "Completed batch 1707 of 3699\n",
      "Completed batch 1708 of 3699\n",
      "Completed batch 1709 of 3699\n",
      "Completed batch 1710 of 3699\n",
      "Completed batch 1711 of 3699\n",
      "Completed batch 1712 of 3699\n",
      "Completed batch 1713 of 3699\n",
      "Completed batch 1714 of 3699\n",
      "Completed batch 1715 of 3699\n",
      "Completed batch 1716 of 3699\n",
      "Completed batch 1717 of 3699\n",
      "Completed batch 1718 of 3699\n",
      "Completed batch 1719 of 3699\n",
      "Completed batch 1720 of 3699\n",
      "Completed batch 1721 of 3699\n",
      "Completed batch 1722 of 3699\n",
      "Completed batch 1723 of 3699\n",
      "Completed batch 1724 of 3699\n",
      "Completed batch 1725 of 3699\n",
      "Completed batch 1726 of 3699\n",
      "Completed batch 1727 of 3699\n",
      "Completed batch 1728 of 3699\n",
      "Completed batch 1729 of 3699\n",
      "Completed batch 1730 of 3699\n",
      "Completed batch 1731 of 3699\n",
      "Completed batch 1732 of 3699\n",
      "Completed batch 1733 of 3699\n",
      "Completed batch 1734 of 3699\n",
      "Completed batch 1735 of 3699\n",
      "Completed batch 1736 of 3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1737 of 3699\n",
      "Completed batch 1738 of 3699\n",
      "Completed batch 1739 of 3699\n",
      "Completed batch 1740 of 3699\n",
      "Completed batch 1741 of 3699\n",
      "Completed batch 1742 of 3699\n",
      "Completed batch 1743 of 3699\n",
      "Completed batch 1744 of 3699\n",
      "Completed batch 1745 of 3699\n",
      "Completed batch 1746 of 3699\n",
      "Completed batch 1747 of 3699\n",
      "Completed batch 1748 of 3699\n",
      "Completed batch 1749 of 3699\n",
      "Completed batch 1750 of 3699\n",
      "Completed batch 1751 of 3699\n",
      "Completed batch 1752 of 3699\n",
      "Completed batch 1753 of 3699\n",
      "Completed batch 1754 of 3699\n",
      "Completed batch 1755 of 3699\n",
      "Completed batch 1756 of 3699\n",
      "Completed batch 1757 of 3699\n",
      "Completed batch 1758 of 3699\n",
      "Completed batch 1759 of 3699\n",
      "Completed batch 1760 of 3699\n",
      "Completed batch 1761 of 3699\n",
      "Completed batch 1762 of 3699\n",
      "Completed batch 1763 of 3699\n",
      "Completed batch 1764 of 3699\n",
      "Completed batch 1765 of 3699\n",
      "Completed batch 1766 of 3699\n",
      "Completed batch 1767 of 3699\n",
      "Completed batch 1768 of 3699\n",
      "Completed batch 1769 of 3699\n",
      "Completed batch 1770 of 3699\n",
      "Completed batch 1771 of 3699\n",
      "Completed batch 1772 of 3699\n",
      "Completed batch 1773 of 3699\n",
      "Completed batch 1774 of 3699\n",
      "Completed batch 1775 of 3699\n",
      "Completed batch 1776 of 3699\n",
      "Completed batch 1777 of 3699\n",
      "Completed batch 1778 of 3699\n",
      "Completed batch 1779 of 3699\n",
      "Completed batch 1780 of 3699\n",
      "Completed batch 1781 of 3699\n",
      "Completed batch 1782 of 3699\n",
      "Completed batch 1783 of 3699\n",
      "Completed batch 1784 of 3699\n",
      "Completed batch 1785 of 3699\n",
      "Completed batch 1786 of 3699\n",
      "Completed batch 1787 of 3699\n",
      "Completed batch 1788 of 3699\n",
      "Completed batch 1789 of 3699\n",
      "Completed batch 1790 of 3699\n",
      "Completed batch 1791 of 3699\n",
      "Completed batch 1792 of 3699\n",
      "Completed batch 1793 of 3699\n",
      "Completed batch 1794 of 3699\n",
      "Completed batch 1795 of 3699\n",
      "Completed batch 1796 of 3699\n",
      "Completed batch 1797 of 3699\n",
      "Completed batch 1798 of 3699\n",
      "Completed batch 1799 of 3699\n",
      "Completed batch 1800 of 3699\n",
      "Completed batch 1801 of 3699\n",
      "Completed batch 1802 of 3699\n",
      "Completed batch 1803 of 3699\n",
      "Completed batch 1804 of 3699\n",
      "Completed batch 1805 of 3699\n",
      "Completed batch 1806 of 3699\n",
      "Completed batch 1807 of 3699\n",
      "Completed batch 1808 of 3699\n",
      "Completed batch 1809 of 3699\n",
      "Completed batch 1810 of 3699\n",
      "Completed batch 1811 of 3699\n",
      "Completed batch 1812 of 3699\n",
      "Completed batch 1813 of 3699\n",
      "Completed batch 1814 of 3699\n",
      "Completed batch 1815 of 3699\n",
      "Completed batch 1816 of 3699\n",
      "Completed batch 1817 of 3699\n",
      "Completed batch 1818 of 3699\n",
      "Completed batch 1819 of 3699\n",
      "Completed batch 1820 of 3699\n",
      "Completed batch 1821 of 3699\n",
      "Completed batch 1822 of 3699\n",
      "Completed batch 1823 of 3699\n",
      "Completed batch 1824 of 3699\n",
      "Completed batch 1825 of 3699\n",
      "Completed batch 1826 of 3699\n",
      "Completed batch 1827 of 3699\n",
      "Completed batch 1828 of 3699\n",
      "Completed batch 1829 of 3699\n",
      "Completed batch 1830 of 3699\n",
      "Completed batch 1831 of 3699\n",
      "Completed batch 1832 of 3699\n",
      "Completed batch 1833 of 3699\n",
      "Completed batch 1834 of 3699\n",
      "Completed batch 1835 of 3699\n",
      "Completed batch 1836 of 3699\n",
      "Completed batch 1837 of 3699\n",
      "Completed batch 1838 of 3699\n",
      "Completed batch 1839 of 3699\n",
      "Completed batch 1840 of 3699\n",
      "Completed batch 1841 of 3699\n",
      "Completed batch 1842 of 3699\n",
      "Completed batch 1843 of 3699\n",
      "Completed batch 1844 of 3699\n",
      "Completed batch 1845 of 3699\n",
      "Completed batch 1846 of 3699\n",
      "Completed batch 1847 of 3699\n",
      "Completed batch 1848 of 3699\n",
      "Completed batch 1849 of 3699\n",
      "Completed batch 1850 of 3699\n",
      "Completed batch 1851 of 3699\n",
      "Completed batch 1852 of 3699\n",
      "Completed batch 1853 of 3699\n",
      "Completed batch 1854 of 3699\n",
      "Completed batch 1855 of 3699\n",
      "Completed batch 1856 of 3699\n",
      "Completed batch 1857 of 3699\n",
      "Completed batch 1858 of 3699\n",
      "Completed batch 1859 of 3699\n",
      "Completed batch 1860 of 3699\n",
      "Completed batch 1861 of 3699\n",
      "Completed batch 1862 of 3699\n",
      "Completed batch 1863 of 3699\n",
      "Completed batch 1864 of 3699\n",
      "Completed batch 1865 of 3699\n",
      "Completed batch 1866 of 3699\n",
      "Completed batch 1867 of 3699\n",
      "Completed batch 1868 of 3699\n",
      "Completed batch 1869 of 3699\n",
      "Completed batch 1870 of 3699\n",
      "Completed batch 1871 of 3699\n",
      "Completed batch 1872 of 3699\n",
      "Completed batch 1873 of 3699\n",
      "Completed batch 1874 of 3699\n",
      "Completed batch 1875 of 3699\n",
      "Completed batch 1876 of 3699\n",
      "Completed batch 1877 of 3699\n",
      "Completed batch 1878 of 3699\n",
      "Completed batch 1879 of 3699\n",
      "Completed batch 1880 of 3699\n",
      "Completed batch 1881 of 3699\n",
      "Completed batch 1882 of 3699\n",
      "Completed batch 1883 of 3699\n",
      "Completed batch 1884 of 3699\n",
      "Completed batch 1885 of 3699\n",
      "Completed batch 1886 of 3699\n",
      "Completed batch 1887 of 3699\n",
      "Completed batch 1888 of 3699\n",
      "Completed batch 1889 of 3699\n",
      "Completed batch 1890 of 3699\n",
      "Completed batch 1891 of 3699\n",
      "Completed batch 1892 of 3699\n",
      "Completed batch 1893 of 3699\n",
      "Completed batch 1894 of 3699\n",
      "Completed batch 1895 of 3699\n",
      "Completed batch 1896 of 3699\n",
      "Completed batch 1897 of 3699\n",
      "Completed batch 1898 of 3699\n",
      "Completed batch 1899 of 3699\n",
      "Completed batch 1900 of 3699\n",
      "Completed batch 1901 of 3699\n",
      "Completed batch 1902 of 3699\n",
      "Completed batch 1903 of 3699\n",
      "Completed batch 1904 of 3699\n",
      "Completed batch 1905 of 3699\n",
      "Completed batch 1906 of 3699\n",
      "Completed batch 1907 of 3699\n",
      "Completed batch 1908 of 3699\n",
      "Completed batch 1909 of 3699\n",
      "Completed batch 1910 of 3699\n",
      "Completed batch 1911 of 3699\n",
      "Completed batch 1912 of 3699\n",
      "Completed batch 1913 of 3699\n",
      "Completed batch 1914 of 3699\n",
      "Completed batch 1915 of 3699\n",
      "Completed batch 1916 of 3699\n",
      "Completed batch 1917 of 3699\n",
      "Completed batch 1918 of 3699\n",
      "Completed batch 1919 of 3699\n",
      "Completed batch 1920 of 3699\n",
      "Completed batch 1921 of 3699\n"
     ]
    }
   ],
   "source": [
    "### Translating English sentences to French\n",
    "\n",
    "NUM_EN_SENTS = len(en_sents_only)\n",
    "BATCH_SIZE = 100\n",
    "NUM_BATCHES = (NUM_EN_SENTS // BATCH_SIZE) + 1\n",
    "i = 0\n",
    "transl_en_sents = []\n",
    "start = time.time()\n",
    "for i in range(NUM_BATCHES):\n",
    "    transl_en_sents.append(translateToFrench(en_sents_only[i*BATCH_SIZE : (i+1)*BATCH_SIZE]))\n",
    "    print(\"Completed batch {:} of {:}\".format(i+1, NUM_BATCHES))\n",
    "end = time.time()\n",
    "print(\"Time taken: {:.3f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/translated_english_sentences_all.txt', 'w') as f:\n",
    "    for batch in transl_en_sents:\n",
    "        for sent in batch:\n",
    "            f.write(\"%s\\n\" % sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/translated_english_sentences_all.txt', 'r') as f:\n",
    "    transl_en_sents = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445050\n"
     ]
    }
   ],
   "source": [
    "print(len(transl_en_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.54857420921326\n"
     ]
    }
   ],
   "source": [
    "# Using BLEU score as a filtering mechanism\n",
    "\n",
    "smoother = nltk.translate.bleu_score.SmoothingFunction()\n",
    "#W = 8 # Weights coefficient\n",
    "#weights = W*[1/W]\n",
    "start = time.time()\n",
    "bleu_scores = []\n",
    "# for i in range(len(all_sent_pairs[:10000])):\n",
    "#     fr_sent_id = all_sent_pairs[i][1]\n",
    "#     fr_sent = fr_sents_only[fr_sent_id-1]\n",
    "#     fr_sent = fr_sent.split()\n",
    "#     transl_en_sent = transl_en_sents[i].split()\n",
    "#     score = sentence_bleu(fr_sent, transl_en_sent) \n",
    "#     bleu_scores.append(score)\n",
    "    \n",
    "bleu_scores = [sentence_bleu([fr_sents_only[all_sent_pairs[i][1]-1].split()], transl_en_sents[i].split(), smoothing_function=smoother.method7) for i in range(len(all_sent_pairs))]\n",
    "    \n",
    "# bleu_scores = [sentence_bleu([fr_sents_only[pair[1]-1].split()], transl_en_sents[all_sent_pairs.index(pair)].split()) for pair in all_sent_pairs[:20000]]\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = fr_sents_only[all_sent_pairs[129541][1]-1].split()\n",
    "# y = transl_en_sents[129541].split()\n",
    "# W = 7\n",
    "# weights = W*[1/W]\n",
    "# print(sentence_bleu([x], y, smoothing_function=smoother.method0, weights=weights))\n",
    "# print(sentence_bleu([x], y, smoothing_function=smoother.method1, weights=weights))\n",
    "# print(sentence_bleu([x], y, smoothing_function=smoother.method2, weights=weights))\n",
    "# print(sentence_bleu([x], y, smoothing_function=smoother.method3, weights=weights))\n",
    "# print(sentence_bleu([x], y, smoothing_function=smoother.method4, weights=weights))\n",
    "# print(sentence_bleu([x], y, smoothing_function=smoother.method5, weights=weights))\n",
    "# print(sentence_bleu([x], y, smoothing_function=smoother.method7, weights=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplying BLEU scores by 100\n",
    "\n",
    "for i in range(len(bleu_scores)):\n",
    "    bleu_scores[i] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445050\n",
      "111.67470964180197\n",
      "0.0\n",
      "26.9367279381114\n"
     ]
    }
   ],
   "source": [
    "print(len(bleu_scores))\n",
    "print(max(bleu_scores))\n",
    "print(min(bleu_scores))\n",
    "print(np.average(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.31575117937724306, 0.9089005235554508, 0.19106317411402157)\n"
     ]
    }
   ],
   "source": [
    "BLEU_THRESHOLD = 60\n",
    "\n",
    "updated_sent_pairs = []\n",
    "for i in range(len(all_sent_pairs)):\n",
    "    if all_margin_scores[i] > THRESHOLD and bleu_scores[i] > BLEU_THRESHOLD:\n",
    "        updated_sent_pairs.append(all_sent_pairs[i])\n",
    "\n",
    "print(computeF1(updated_sent_pairs, gold_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing corpus-level BLEU score of translated English sentences\n",
    "transl_en_sents_gold = []\n",
    "fr_sents_gold = []\n",
    "for tup in gold_tuples:\n",
    "    if tup in all_sent_pairs:\n",
    "        transl_en_sents_gold.append(transl_en_sents[all_sent_pairs.index(tup)])\n",
    "        fr_sents_gold.append(fr_sents_only[tup[1]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mais en fin de compte, Merkel a accepté un fonds de sauvetage permanent pour la zone euro.', \"Mais le changement de régime coercitif n'a jamais été la clé de la transition démocratique.\", \"Dans d'autres cas, les sorties de capitaux ne font qu'aggraver des situations déjà difficiles.\", \"Si les humanités sont devenues obsolètes, alors il se peut que l'humanité perde sa salive.\", \"En conséquence, la politique mondiale n'est plus la seule province de gouvernement.\", \"Il a utilisé une partie de sa grande richesse pour verser de l'argent dans les zones rurales.\", \"Une technopole reconnaît également le rôle crucial de la coopération, de la réflexion stratégique et de l'adaptation.\", \"L'exploitation minière et l'agriculture des métaux ont connu des améliorations similaires dans l'ensemble du pays.\", 'La seule façon de sortir de ce dilemme est de fixer un objectif réalisable et réaliste.', 'Il est certain que les réserves sont importantes pour aplanir les déséquilibres dans un régime de taux de change fixes.']\n",
      "['Merkel a en fin de compte consenti à l’établissement d’un mécanisme permanent de sauvetage pour la zone euro.', 'Mais la transition démocratique n’a jamais reposé sur les changements de régime coercitifs.', 'Dans d’autres cas, la fuite des capitaux ne fait qu’aggraver une situation déjà difficile.', \"Si les sciences humaines deviennent obsolètes, il se peut que l'humanité perde de son importance.\", 'Il en résulte que la politique mondiale n’est plus le domaine réservé des gouvernements.', 'Il a déversé une partie de son immense fortune dans les régions rurales.', 'Le régime technopolitique reconnaît également le rôle crucial que jouent la coopération, la réflexion stratégique, et l’adaptation.', \"De tels progrès ont aussi été réalisés dans les domaines de l'extraction des métaux et de l'agriculture.\", 'La seule manière de se sortir de ce dilemme consiste à se donner un but atteignable et réaliste.', 'Bien sűr, les réserves sont importantes pour aplanir les déséquilibres des régimes de taux de changes fixes.']\n"
     ]
    }
   ],
   "source": [
    "print(transl_en_sents_gold[:10])\n",
    "print(fr_sents_gold[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_references_gold = [[fr_sent.split()] for fr_sent in fr_sents_gold]\n",
    "fr_hypotheses_gold = [fr_sent.split() for fr_sent in transl_en_sents_gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_bleu_gold = nltk.translate.bleu_score.corpus_bleu(fr_references_gold, fr_hypotheses_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_bleu_gold = [nltk.translate.bleu_score.sentence_bleu(fr_references_gold[i], fr_hypotheses_gold[i]) for i in range(len(fr_references_gold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.max([getNamedEntityOverlap(fetchNamedEntities(fr_sents_gold[i], fr=True), fetchNamedEntities(transl_en_sents_gold[i], fr=True)) for i in range(len(gold_tuples[:100]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f21b34226a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbad_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_sent_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sent_pairs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgold_tuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfr_references_similar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfr_sents_only\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_sent_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfr_hypotheses_similar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransl_en_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-f21b34226a20>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbad_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_sent_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_sent_pairs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgold_tuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfr_references_similar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfr_sents_only\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_sent_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfr_hypotheses_similar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransl_en_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "# bad_indices = [all_sent_pairs.index(pair) for pair in all_sent_pairs if pair not in gold_tuples]\n",
    "# fr_references_similar = [[fr_sents_only[all_sent_pairs[ind][1]-1].split()] for ind in bad_indices]\n",
    "# fr_hypotheses_similar = [transl_en_sents[ind].split() for ind in bad_indices]\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_bleu_similar = nltk.translate.bleu_score.corpus_bleu(fr_references_similar, fr_hypotheses_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_bleu_similar = [nltk.translate.bleu_score.sentence_bleu(fr_references_similar[i], fr_hypotheses_similar[i]) for i in range(len(fr_references_similar))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5943\n",
      "9029\n",
      "7863\n",
      "436021\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for score in sent_bleu_gold:\n",
    "    if score > 0.1:\n",
    "        k += 1\n",
    "print(k)\n",
    "\n",
    "print(len(sent_bleu_gold))\n",
    "\n",
    "t = 0\n",
    "for score in sent_bleu_similar:\n",
    "    if score > 0.1:\n",
    "        t += 1\n",
    "print(t)\n",
    "\n",
    "print(len(sent_bleu_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6865842782520066\n",
      "0.5931102795509575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6318110236220472"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(5389/(5389+2460))\n",
    "print(5389/(len(gold_tuples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_cos_sims = [normalize(embed(fr_sents_gold[i])).dot(normalize(embed(transl_en_sents_gold[i]))) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92529225\n"
     ]
    }
   ],
   "source": [
    "print(np.average(gold_cos_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fr_references_similar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-5daa1d48fd78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfr_sents_similar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlist_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfr_references_similar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfr_sents_similar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fr_references_similar' is not defined"
     ]
    }
   ],
   "source": [
    "fr_sents_similar = []\n",
    "for list_ in fr_references_similar:\n",
    "    for sublist in list_:\n",
    "        fr_sents_similar.append(\" \".join(sublist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_en_sents_similar = []\n",
    "for list_ in fr_hypotheses_similar:\n",
    "    transl_en_sents_similar.append(\" \".join(list_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436021\n",
      "436021\n"
     ]
    }
   ],
   "source": [
    "print(len(fr_sents_similar))\n",
    "print(len(transl_en_sents_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_cos_sims = [normalize(embed(fr_sents_similar[i])).dot(normalize(embed(transl_en_sents_similar[i]))) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23983933\n"
     ]
    }
   ],
   "source": [
    "print(np.average(similar_cos_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using LaBSE/SBERT cosine similarity as a filtering mechanism ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# cosine_similarities = [normalize(embed(fr_sents_only[all_sent_pairs[i][1]-1])).dot(normalize(embed(transl_en_sents[i]))) for i in range(len(transl_en_sents))]\n",
    "# pd.DataFrame({'cosine_similarities': cosine_similarities}).to_csv('Data/cosine_similarities_LaBSE.csv', index=False)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using post-translation margin score as a filtering mechanism ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cosine_similarities = pd.read_csv('Data/cosine_similarities_LaBSE.csv')\n",
    "all_cosine_similarities = [float(sim) for sim in all_cosine_similarities['cosine_similarities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using LASER cosine similarity as a filtering mechanism ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "from laserembeddings import Laser\n",
    "laser = Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5538535118103027\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "laser_cos_sims = [normalize(laser.embed_sentences(fr_sents_only[all_sent_pairs[i][1]-1], lang='fr')).T.dot(normalize(laser.embed_sentences(transl_en_sents[i], lang='fr'))) for i in range(len(transl_en_sents[:100]))]\n",
    "#pd.DataFrame({'cosine_similarities': laser_cos_sims}).to_csv('Data/cosine_similarities_LASER.csv', index=False)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "fr_laser_emb = laser.embed_sentences([fr_sents_only[all_sent_pairs[i][1]-1] for i in range(len(all_sent_pairs))], lang='fr')\n",
    "fr_laser_emb = [normalize(emb) for emb in fr_laser_emb]                \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "transl_en_laser_emb = laser.embed_sentences([transl_en_sents[i] for i in range(len(transl_en_sents))], lang='fr')\n",
    "transl_en_laser_emb = [normalize(emb) for emb in transl_en_laser_emb]\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5644221305847168\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "laser_cos_sims = [fr_laser_emb[i].dot(transl_en_laser_emb[i]) for i in range(len(transl_en_laser_emb))]\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'cosine_similarities': laser_cos_sims}).to_csv('Data/cosine_similarities_LASER.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser_cosine_similarities = pd.read_csv('Data/cosine_similarities_LASER.csv')\n",
    "laser_cosine_similarities = [float(sim) for sim in laser_cosine_similarities['cosine_similarities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07740991114879084\n"
     ]
    }
   ],
   "source": [
    "# Average absolute difference between LASER and LaBSE cosine similarities\n",
    "print(np.average([abs(laser_cosine_similarities[i]-all_cosine_similarities[i]) for i in range(len(laser_cosine_similarities))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_cosine_similarities = [((laser_cosine_similarities[i]+all_cosine_similarities[i])/2) for i in range(len(laser_cosine_similarities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8749247308910628, 0.8703038222793528, 0.8795949812898965)\n"
     ]
    }
   ],
   "source": [
    "### Combining margin/LaBSE/LASER similarity thresholds ###\n",
    "\n",
    "THRESHOLD = 0.0\n",
    "CS_THRESHOLD_LABSE = 0.84\n",
    "CS_THRESHOLD_LASER = 0.85\n",
    "CS_THRESHOLD_AVG = 0.84\n",
    "updated_sent_pairs = []\n",
    "\n",
    "for i in range(len(all_sent_pairs)):\n",
    "    if all_margin_scores[i] > THRESHOLD and averaged_cosine_similarities[i] > CS_THRESHOLD_AVG: \n",
    "        updated_sent_pairs.append(all_sent_pairs[i])\n",
    "\n",
    "print(computeF1(updated_sent_pairs, gold_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_overlap_gold = [len(list((Counter(fr_sents_gold[i].split()) & Counter(transl_en_sents_gold[i].split())).elements())) for i in range(len(fr_sents_gold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(lexical_overlap_gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_overlap_similar = [len(list((Counter(fr_sents_similar[i].split()) & Counter(transl_en_sents_similar[i].split())).elements())) for i in range(len(fr_sents_similar))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['«', 'comme', '».']\n"
     ]
    }
   ],
   "source": [
    "print(list((Counter(fr_sents_similar[200].split()) & Counter(transl_en_sents_similar[200].split())).elements()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7943929306157274\n",
      "436021\n",
      "8.948720788570164\n"
     ]
    }
   ],
   "source": [
    "print(np.average(lexical_overlap_similar))\n",
    "print(np.average(lexical_overlap_gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([l for l in lexical_overlap_similar if l >= 6]))\n",
    "print(len(lexical_overlap_similar))\n",
    "print(len([l for l in lexical_overlap_gold if l >= 6]))\n",
    "print(len(lexical_overlap_gold))\n",
    "print(len(lexical_overlap_gold)+len(lexical_overlap_similar))\n",
    "print(len(all_sent_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_inds = [all_sent_pairs.index(pair) for pair in all_sent_pairs if pair in gold_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_lex_overlap_similar = lexical_overlap_similar\n",
    "lexical_overlap_all = []\n",
    "for i in range(len(lexical_overlap_gold)):\n",
    "    copy_lex_overlap_similar.insert(gold_inds[i], lexical_overlap_gold[i])\n",
    "lexical_overlap_all = copy_lex_overlap_similar\n",
    "#lexical_overlap_all = [copy_lex_overlap_similar.insert(ind, lexical_overlap_gold[ind]) for ind in gold_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9218\n",
      "9086\n",
      "189\n",
      "189\n",
      "9029\n"
     ]
    }
   ],
   "source": [
    "print(len(lexical_overlap_all)-436021)\n",
    "print(len(gold_tuples))\n",
    "print(9218-len(gold_inds))\n",
    "print(9218-len(lexical_overlap_gold))\n",
    "print(len(gold_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [lexical_overlap_all[ind] for ind in gold_inds]\n",
    "b = [lexical_overlap_all[i] for i in range(len(all_sent_pairs)) if all_sent_pairs[i] not in gold_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9029\n",
      "436021\n"
     ]
    }
   ],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_overlap_similar = b\n",
    "los_copy = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436021\n"
     ]
    }
   ],
   "source": [
    "print(len(lexical_overlap_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_overlap_all = [len(list((Counter(fr_sents_only[all_sent_pairs[i][1]-1].split()) & Counter(transl_en_sents[i].split())).elements())) for i in range(len(transl_en_sents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords from lexical overlap list to create modified LO score #\n",
    "\n",
    "fr_stopwords = open('Data/stopwords-fr.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_stopwords = [word.strip('\\n') for word in fr_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining lexical overlap between French and translated English sentences\n",
    "# with stopwords removed\n",
    "\n",
    "lexical_overlap_all_mod = []\n",
    "for i in range(len(transl_en_sents)):\n",
    "    c1 = Counter(fr_sents_only[all_sent_pairs[i][1]-1].split())\n",
    "    c2 = Counter(transl_en_sents[i].split())\n",
    "    overlap = list((c1 & c2).elements())\n",
    "    mod_overlap = [e for e in overlap if e not in fr_stopwords]\n",
    "    lexical_overlap_all_mod.append(len(mod_overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4457324986560443, 0.3025296038395285, 0.8463570327977108)\n"
     ]
    }
   ],
   "source": [
    "### Using lexical overlap as a filtering mechanism ###\n",
    "\n",
    "LO_THRESHOLD = 9\n",
    "LO_THRESHOLD_MOD = 3\n",
    "CS_THRESHOLD = 0.0\n",
    "BLEU_THRESHOLD = 0.0\n",
    "THRESHOLD = 0.85\n",
    "\n",
    "updated_sent_pairs = []\n",
    "\n",
    "for i in range(len(all_sent_pairs)):\n",
    "    if all_margin_scores[i] > THRESHOLD and lexical_overlap_all_mod[i] >= LO_THRESHOLD_MOD and all_cosine_similarities[i] > CS_THRESHOLD:\n",
    "        updated_sent_pairs.append(all_sent_pairs[i])\n",
    "\n",
    "print(computeF1(updated_sent_pairs, gold_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Giving METEOR score a try instead of BLEU ###\n",
    "\n",
    "meteor_scores_gold = [nltk.translate.meteor_score.single_meteor_score(fr_sents_gold[i], transl_en_sents_gold[i]) for i in range(len(fr_sents_gold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0375\n",
      "0.5784231985366046\n",
      "0.0\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "print(np.max(meteor_scores_gold))\n",
    "print(np.average(meteor_scores_gold))\n",
    "print(np.min(meteor_scores_gold))\n",
    "print(len([score for score in meteor_scores_gold if score < 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meteor_scores = [nltk.translate.meteor_score.single_meteor_score(fr_sents_only[all_sent_pairs[i][1]-1], transl_en_sents[i]) for i in range(len(all_sent_pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13697937502251198\n",
      "361739\n"
     ]
    }
   ],
   "source": [
    "print(np.average(all_meteor_scores))\n",
    "print(len([score for score in all_meteor_scores if score < 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bleu_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c0c933b9fe14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_margin_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTHRESHOLD\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     (bleu_scores[i] > BLEU_THRESHOLD or \\\n\u001b[0m\u001b[1;32m     13\u001b[0m      \u001b[0mall_cosine_similarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mCS_THRESHOLD\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m      \u001b[0mlexical_overlap_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mLO_THRESHOLD\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bleu_scores' is not defined"
     ]
    }
   ],
   "source": [
    "METEOR_THRESHOLD = 10\n",
    "BLEU_THRESHOLD = 1000\n",
    "CS_THRESHOLD = 0.75\n",
    "LO_THRESHOD_MOD = 20\n",
    "LO_THRESHOLD = 20\n",
    "\n",
    "updated_sent_pairs = []\n",
    "\n",
    "for i in range(len(all_sent_pairs)):\n",
    "    if all_margin_scores[i] > THRESHOLD and \\\n",
    "    \\\n",
    "    (bleu_scores[i] > BLEU_THRESHOLD or \\\n",
    "     all_cosine_similarities[i] > CS_THRESHOLD or \\\n",
    "     lexical_overlap_all[i] > LO_THRESHOLD or \\\n",
    "     lexical_overlap_all_mod[i] > LO_THRESHOLD_MOD or \\\n",
    "     all_meteor_scores[i] > METEOR_THRESHOLD):\n",
    "        \n",
    "        updated_sent_pairs.append(all_sent_pairs[i])\n",
    "\n",
    "print(computeF1(updated_sent_pairs, gold_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9189\n",
      "7957.674\n",
      "1071\n"
     ]
    }
   ],
   "source": [
    "print(len(updated_sent_pairs))\n",
    "print(0.866*len(updated_sent_pairs))\n",
    "print(len(fr_sents_gold)-7958)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using named entities as a filtering mechanism ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fr_core_news_sm==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.3.0/fr_core_news_sm-2.3.0.tar.gz (14.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.7 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in ./.local/lib/python3.7/site-packages (from fr_core_news_sm==2.3.0) (2.3.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (50.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (4.54.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (2.24.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (7.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./.local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (3.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->fr_core_news_sm==2.3.0) (2.10)\n",
      "Building wheels for collected packages: fr-core-news-sm\n",
      "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.3.0-py3-none-any.whl size=14718366 sha256=db8c7b45c2724cf4324ec2bfec25510aa7c47df06f1b90bee6326da0c3b8ef95\n",
      "  Stored in directory: /scratch/1757124.1.csgpu/pip-ephem-wheel-cache-cqh4o4ox/wheels/cf/1e/f7/79485fa3afb8e3cb7490dedc3eb88b9fea2b4231a8209b77d1\n",
      "Successfully built fr-core-news-sm\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-2.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WhitespaceTokenizer' from 'spacy.tokenizer' (/usr2/collab/agjones/.local/lib/python3.7/site-packages/spacy/tokenizer.cpython-37m-x86_64-linux-gnu.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e215ec81e384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfr_core_news_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWhitespaceTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0men_ne_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tagger'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'textcat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'WhitespaceTokenizer' from 'spacy.tokenizer' (/usr2/collab/agjones/.local/lib/python3.7/site-packages/spacy/tokenizer.cpython-37m-x86_64-linux-gnu.so)"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "import fr_core_news_sm\n",
    "\n",
    "en_ne_model = en_core_web_sm.load(disable=['parser', 'tagger', 'textcat'])\n",
    "fr_ne_model = fr_core_news_sm.load(disable=['parser', 'tagger', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Copy-paste from SpaCy docs\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ne_model.tokenizer = WhitespaceTokenizer(en_ne_model.vocab)\n",
    "fr_ne_model.tokenizer = WhitespaceTokenizer(fr_ne_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne_model = 'usr2/collab/agjones/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "# ne_jar = 'usr2/collab/agjones/stanford-ner/stanford-ner.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchNamedEntities(sent, en=False, fr=False):\n",
    "    if en:\n",
    "        doc = en_ne_model(sent)\n",
    "    else:\n",
    "        doc = fr_ne_model(sent)\n",
    "    ne = [ent.text for ent in doc.ents]\n",
    "    return ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNamedEntityOverlap(ne_list_1, ne_list_2):\n",
    "    return len(list((Counter(ne_list_1) & Counter(ne_list_2)).elements()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(getNamedEntityOverlap(fetchNamedEntities(fr_sents_only[all_sent_pairs[220362][1]-1]), fetchNamedEntities(transl_en_sents[220362])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Merkel']\n",
      "['n’a']\n",
      "['qu’aggraver']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['d’une']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Etats-Unis']\n",
      "['Les Etats-Unis']\n",
      "['Ergogan.']\n",
      "['district de Warangal', 'Pradesh.']\n",
      "[\"l'Organisation mondiale du commerce\"]\n",
      "['L’Organisation mondiale du commerce aurait-elle']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['États']\n",
      "[]\n",
      "['La Chine']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Chine.']\n",
      "['«\\xa0promotion', 'sain\\xa0».']\n",
      "[]\n",
      "[]\n",
      "['Etats-Unis.']\n",
      "['Nigeria.']\n",
      "['l’échelle']\n",
      "[]\n",
      "[]\n",
      "['Songez', 'British Petroleum', 'golfe du Mexique']\n",
      "['États-nations']\n",
      "['Fonds monétaire international']\n",
      "[]\n",
      "['John Snow', 'Londres.']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\"l'exception.\"]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Montesquieu', 'France.']\n",
      "[]\n",
      "[]\n",
      "[\"l'épargne.\"]\n",
      "[]\n",
      "['Cependant,']\n",
      "[]\n",
      "[]\n",
      "['n’a']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Buffet']\n",
      "[]\n",
      "['2004,', 'Premier Ministre', 'Ahmed Nazif.']\n",
      "[]\n",
      "[]\n",
      "['s’est', 'd’une']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Malheureusement,']\n",
      "['Roms', 'Europe']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\"D'importants\"]\n",
      "['L’Europe', 'le Japon']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for sent in fr_sents_gold[:100]:\n",
    "    print(fetchNamedEntities(sent, fr=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Merkel']\n",
      "['Merkel']\n"
     ]
    }
   ],
   "source": [
    "print(fetchNamedEntities(fr_sents_only[all_sent_pairs[220362][1]-1], fr=True))\n",
    "print(fetchNamedEntities(en_sents_only[all_sent_pairs[220362][0]-1], en=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783.4920241832733\n",
      "(0.053263879441574825, 0.11253106141245109, 0.03488883997358574)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "NE_THRESHOLD = 0.4\n",
    "updated_sent_pairs = []\n",
    "for i in range(len(all_sent_pairs)):\n",
    "    ne_en_sent = fetchNamedEntities(en_sents_only[all_sent_pairs[i][0]-1], en=True)\n",
    "    ne_fr_sent = fetchNamedEntities(fr_sents_only[all_sent_pairs[i][1]-1], fr=True)\n",
    "    if all_margin_scores[i] > THRESHOLD and (getNamedEntityOverlap(ne_en_sent, ne_fr_sent) / (len(ne_en_sent)+1e-8) > NE_THRESHOLD):\n",
    "        updated_sent_pairs.append(all_sent_pairs[i])\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "print(computeF1(updated_sent_pairs, gold_tuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, trying mining on the Tatoeba test sets:\n",
    "\n",
    "# (1) en-kk, kk-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the test files\n",
    "# For every pair of lines i,j in zip(file_a_lines, file_b_lines),\n",
    "# a[i], b[j] represent a translation pair\n",
    "\n",
    "with open('Data/tatoeba.kaz-eng.eng', 'r') as f:\n",
    "    tatoeba_en_sents = f.read().splitlines()\n",
    "with open ('Data/tatoeba.kaz-eng.kaz', 'r') as f:\n",
    "    tatoeba_kk_sents = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tatoeba_en_embeddings = embed(tatoeba_en_sents)\n",
    "tatoeba_kk_embeddings = embed(tatoeba_kk_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing margin-based mining and obtaining accuracy scores\n",
    "\n",
    "fwd_means, fwd_inds = directedMeansAndInds(tatoeba_en_embeddings, tatoeba_kk_embeddings, forward=True, k=4, batch_size=100)\n",
    "bwd_means, bwd_inds = directedMeansAndInds(tatoeba_en_embeddings, tatoeba_kk_embeddings, backward=True, k=4, batch_size=100)\n",
    "\n",
    "fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "for i in range(fwd_inds.shape[0]):\n",
    "    for j in range(fwd_inds.shape[1]):\n",
    "        tgt_ind = fwd_inds[i,j]\n",
    "        margin_score = (tatoeba_en_embeddings[i].dot(tatoeba_kk_embeddings[tgt_ind]))# / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "        fwd_margin_scores[i,j] = margin_score\n",
    "best = np.zeros((fwd_inds.shape[0], 3))\n",
    "best_inds = fwd_inds[np.arange(tatoeba_en_embeddings.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "for i in range(fwd_inds.shape[0]):\n",
    "    best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "    best[i] = ((i+1, best_inds[i]+1, best_score))\n",
    "    \n",
    "bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "for i in range(bwd_inds.shape[0]):\n",
    "    for j in range(bwd_inds.shape[1]):\n",
    "        tgt_ind = bwd_inds[i,j]\n",
    "        margin_score = (tatoeba_kk_embeddings[i].dot(tatoeba_en_embeddings[tgt_ind]))# / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "        bwd_margin_scores[i,j] = margin_score\n",
    "bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "best_inds = bwd_inds[np.arange(tatoeba_kk_embeddings.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "for i in range(bwd_inds.shape[0]):\n",
    "    best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "    #best_ind = bwd_inds[i][ind]\n",
    "    bwd_best[i] = ((best_inds[i]+1, i+1, best_score))\n",
    "    \n",
    "fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "pairs_and_scores = []\n",
    "pairs_and_scores = list(set(fwd_best) & set(bwd_best))\n",
    "# for fwd_triplet, bwd_triplet in zip(fwd_best, bwd_best):\n",
    "#     pairs_and_scores.append(fwd_triplet)\n",
    "#     pairs_and_scores.append(bwd_triplet)\n",
    "\n",
    "pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "# #pairs_and_scores = list(set(pairs_and_scores))\n",
    "concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores]\n",
    "#concat_pairs = list(dict.fromkeys(concat_pairs))\n",
    "#concat_pairs = list(set(concat_pairs))\n",
    "concat_pairs_int = []\n",
    "for tup in concat_pairs:\n",
    "    concat_pairs_int.append((int(tup[0]), int(tup[1])))\n",
    "concat_pairs = concat_pairs_int\n",
    "\n",
    "margin_scores = [triplet[2] for triplet in pairs_and_scores]\n",
    "\n",
    "pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv('Data/sent_pairs_fwd_tat_en_kk_raw_cos.csv', index=False)\n",
    "pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv('Data/sent_pairs_bwd_tat_en_kk_raw_cos.csv', index=False)\n",
    "pd.DataFrame({'concat_pairs': concat_pairs}).to_csv('Data/concat_pairs_tat_en_kk_raw_cos.csv', index=False)\n",
    "pd.DataFrame({'margin_scores': margin_scores}).to_csv('Data/margin_scores_tat_en_kk_raw_cos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_margin_scores_tat_en_kk = pd.read_csv('Data/margin_scores_tat_en_kk_raw_cos.csv')\n",
    "all_margin_scores_tat_en_kk = [float(score) for score in all_margin_scores_tat_en_kk['margin_scores']]\n",
    "all_sent_pairs_tat_en_kk = pd.read_csv('Data/concat_pairs_tat_en_kk_raw_cos.csv')\n",
    "all_sent_pairs_tat_en_kk = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs_tat_en_kk['concat_pairs']]\n",
    "all_sent_pairs_tat_en_kk = [pair[0] for pair in all_sent_pairs_tat_en_kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracyTatoeba(sent_pairs):\n",
    "    return len([pair for pair in sent_pairs if pair[0]==pair[1]]) / len(sent_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9117647008902093, 0.9668615984216986, 0.8626086956521739)\n"
     ]
    }
   ],
   "source": [
    "print(computeF1(all_sent_pairs_tat_en_kk, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translates using Helsinki-NLP open-source NMT system\n",
    "def translateToKazakh(sents):\n",
    "    for i in range(len(sents)):\n",
    "        sents[i] = \">>kaz_Cyrl<< \" + sents[i]\n",
    "    tokenized = marian_tokenizer.prepare_seq2seq_batch(sents, return_tensors='pt')\n",
    "    tokenized.to(device)\n",
    "    translated = translation_model.generate(**tokenized)\n",
    "    decoded = [marian_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    tokenized.to('cpu')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> Мынау - сөйлем.']\n"
     ]
    }
   ],
   "source": [
    "print(translateToKazakh([\"This is a sentence. This is also a sentence. And this!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'marian_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d8b662c39691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_BATCHES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtransl_tat_en_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslateToKazakh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtatoeba_en_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Completed batch {:} of {:}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_BATCHES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4bd92a4c52f2>\u001b[0m in \u001b[0;36mtranslateToKazakh\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\">>kaz_Cyrl<< \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarian_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_seq2seq_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'marian_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_EN_SENTS = len(tatoeba_en_sents)\n",
    "BATCH_SIZE = 100\n",
    "NUM_BATCHES = (NUM_EN_SENTS // BATCH_SIZE) + 1\n",
    "i = 0\n",
    "transl_tat_en_sents = []\n",
    "start = time.time()\n",
    "for i in range(NUM_BATCHES):\n",
    "    transl_tat_en_sents.append(translateToKazakh(tatoeba_en_sents[i*BATCH_SIZE : (i+1)*BATCH_SIZE]))\n",
    "    print(\"Completed batch {:} of {:}\".format(i+1, NUM_BATCHES))\n",
    "end = time.time()\n",
    "print(\"Time taken: {:.3f}\".format(end-start))\n",
    "\n",
    "with open('Data/translated_english_sentences_tat.txt', 'w') as f:\n",
    "    for batch in transl_tat_en_sents:\n",
    "        for sent in batch:\n",
    "            f.write(\"%s\\n\" % sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/translated_english_sentences_tat.txt', 'r') as f:\n",
    "    transl_tat_en_sents = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mineSentencePairs(src_embs, tgt_embs, file_suffix, batch_size=100, num_neighbors=4):\n",
    "    \n",
    "    interval = 1e4\n",
    "\n",
    "    print(\"Retrieving means in forward direction\", flush=True)\n",
    "    start = time.time()\n",
    "    fwd_means, fwd_inds = directedMeansAndInds(src_embs, tgt_embs, forward=True, k=num_neighbors, batch_size=batch_size)\n",
    "    end = time.time()\n",
    "    print(\"Time taken to retrieve forward means: {:.2f} seconds\".format(end-start), flush=True)\n",
    "    \n",
    "    print(\"Retrieving means in backward direction\", flush=True)\n",
    "    start = time.time()\n",
    "    bwd_means, bwd_inds = directedMeansAndInds(src_embs, tgt_embs, backward=True, k=num_neighbors, batch_size=batch_size)\n",
    "    end = time.time()\n",
    "    print(\"Time taken to retrieve backward means: {:.2f} seconds\".format(end-start), flush=True)\n",
    "    \n",
    "    print(\"Beginning mining in the forward direction\", flush=True)\n",
    "    fwd_margin_scores = np.zeros(fwd_inds.shape)\n",
    "    for i in range(fwd_inds.shape[0]):\n",
    "        if i % interval == 0 and i > 0:\n",
    "            print(\"Completed pair {} of {} in the forward direction\".format(i, fwd_inds.shape[0]), flush=True)\n",
    "        for j in range(fwd_inds.shape[1]):\n",
    "            tgt_ind = fwd_inds[i,j]\n",
    "            margin_score = (src_embs[i].dot(tgt_embs[tgt_ind])) / np.average((fwd_means[i], bwd_means[tgt_ind]))\n",
    "            fwd_margin_scores[i,j] = margin_score\n",
    "    best = np.zeros((fwd_inds.shape[0], 3))\n",
    "    best_inds = fwd_inds[np.arange(src_embs.shape[0]), fwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(fwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(fwd_margin_scores[i]), np.argmax(fwd_margin_scores[i]))\n",
    "        best[i] = ((i+1, best_inds[i]+1, best_score))\n",
    "\n",
    "    print(\"Mining in the forward direction complete\", flush=True)\n",
    "    \n",
    "    print(\"Beginning mining in the backward direction\", flush=True)\n",
    "    bwd_margin_scores = np.zeros(bwd_inds.shape)\n",
    "    for i in range(bwd_inds.shape[0]):\n",
    "        if i % interval == 0 and i > 0:\n",
    "            print(\"Completed pair {} of {} in the backward direction\".format(i, bwd_inds.shape[0]), flush=True)\n",
    "        for j in range(bwd_inds.shape[1]):\n",
    "            tgt_ind = bwd_inds[i,j]\n",
    "            margin_score = (tgt_embs[i].dot(src_embs[tgt_ind])) / np.average((bwd_means[i], fwd_means[tgt_ind]))\n",
    "            bwd_margin_scores[i,j] = margin_score\n",
    "    bwd_best = np.zeros((bwd_inds.shape[0], 3))\n",
    "    best_inds = bwd_inds[np.arange(tgt_embs.shape[0]), bwd_margin_scores.argmax(axis=1)]\n",
    "    for i in range(bwd_inds.shape[0]):\n",
    "        best_score, ind = (np.max(bwd_margin_scores[i]), np.argmax(bwd_margin_scores[i]))\n",
    "        #best_ind = bwd_inds[i][ind]\n",
    "        bwd_best[i] = ((best_inds[i]+1, i+1, best_score))\n",
    "\n",
    "    print(\"Mining in the backward direction complete\", flush=True)\n",
    "    \n",
    "    fwd_best = [tuple(best[i]) for i in range(best.shape[0])]\n",
    "    bwd_best = [tuple(bwd_best[i]) for i in range(bwd_best.shape[0])]\n",
    "\n",
    "    pairs_and_scores = []\n",
    "    pairs_and_scores = list(set(fwd_best) & set(bwd_best))\n",
    "#     for fwd_triplet, bwd_triplet in zip(fwd_best, bwd_best):\n",
    "#         pairs_and_scores.append(fwd_triplet)\n",
    "#         pairs_and_scores.append(bwd_triplet)\n",
    "\n",
    "    pairs_and_scores = list(dict.fromkeys(pairs_and_scores))\n",
    "    # #pairs_and_scores = list(set(pairs_and_scores))\n",
    "    concat_pairs = [(triplet[0], triplet[1]) for triplet in pairs_and_scores]\n",
    "    #concat_pairs = list(dict.fromkeys(concat_pairs))\n",
    "    #concat_pairs = list(set(concat_pairs))\n",
    "    concat_pairs_int = []\n",
    "    for tup in concat_pairs:\n",
    "        concat_pairs_int.append((int(tup[0]), int(tup[1])))\n",
    "    concat_pairs = concat_pairs_int\n",
    "\n",
    "    margin_scores = [triplet[2] for triplet in pairs_and_scores]\n",
    "    \n",
    "    spf_path = 'Data/sent_pairs_fwd_{:}.csv'.format(file_suffix)\n",
    "    spb_path = 'Data/sent_pairs_bwd_{:}.csv'.format(file_suffix)\n",
    "    cct_path = 'Data/concat_pairs_{:}.csv'.format(file_suffix)\n",
    "    mgn_path = 'Data/margin_scores_{:}.csv'.format(file_suffix)\n",
    "\n",
    "    pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv(spf_path, index=False)\n",
    "    pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv(spb_path, index=False)\n",
    "    pd.DataFrame({'concat_pairs': concat_pairs}).to_csv(cct_path, index=False)\n",
    "    pd.DataFrame({'margin_scores': margin_scores}).to_csv(mgn_path, index=False)\n",
    "\n",
    "    all_margin_scores = pd.read_csv(mgn_path)\n",
    "    all_margin_scores = [float(score) for score in all_margin_scores['margin_scores']]\n",
    "    all_sent_pairs = pd.read_csv(cct_path)\n",
    "    all_sent_pairs = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs['concat_pairs']]\n",
    "    all_sent_pairs = [pair[0] for pair in all_sent_pairs]\n",
    "                                    \n",
    "    return concat_pairs, margin_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent_pairs_en_kk_T, all_margin_scores_en_kk_T = mineSentencePairs(embed(transl_tat_en_sents), \n",
    "                                                                      tatoeba_kk_embeddings,\n",
    "                                                                      'tat_en_kk_T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7910592759166485, 0.8964757709053639, 0.7078260869565217)\n"
     ]
    }
   ],
   "source": [
    "print(computeF1(all_sent_pairs_en_kk_T, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Өткен жылы ең жоғары мектепті біттім.', 'Жаңа жылың қуаныш!', 'Дегенім жоқ.', 'Жалғыз қарым-қатынас.', 'Сені сындырып кетмесе деймін.', 'Оны бір күні оқып алды.', 'Маған өлтір!', 'Занар Амантавенс балалары үшін мектеп саласындағы әдеби идеяларды қоғамдатады.', 'Бүгінгі Дүйсенбі.', 'Негізінде ол нағыз адам.']\n",
      "['Мен былтыр орта мектепті бітірдім.', 'Жаңа жылыңыз құтты болсын.', 'Бiлгiм келмейдi.', 'Қарыз қатынас бұзады.', 'Сізді бөлмедім деп үміттенемін.', 'Ол кітапты бір күнде оқып бітірді.', 'Өлтірдің ғой мені.', 'Жанар Амантаевна біздің балалардың жазғы демалысын ұйымдастыруда жақсы ой тастайды.', 'Бүгін Дүйсенбі.', 'Менің пікірімше, ол адал адам.']\n"
     ]
    }
   ],
   "source": [
    "print(transl_tat_en_sents[:10])\n",
    "print(tatoeba_kk_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4195dfaec048239e0c49f74fa2fb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/305M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(61709, 512, padding_idx=61708)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(61709, 512, padding_idx=61708)\n",
       "      (embed_positions): SinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): Identity()\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(61709, 512, padding_idx=61708)\n",
       "      (embed_positions): SinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (self_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Attention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying translating in the other direction . . .\n",
    "\n",
    "translation_model_name = 'Helsinki-NLP/opus-mt-trk-en'\n",
    "marian_tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(translation_model_name)\n",
    "translation_model.cuda().half() # Puts translation model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateFromKazakh(sents):\n",
    "    for i in range(len(sents)):\n",
    "        sents[i] = \">>kaz_Cyrl<< \" + sents[i]\n",
    "    tokenized = marian_tokenizer.prepare_seq2seq_batch(sents, return_tensors='pt')\n",
    "    tokenized.to(device)\n",
    "    translated = translation_model.generate(**tokenized)\n",
    "    decoded = [marian_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    tokenized.to('cpu')\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EN_SENTS = len(tatoeba_kk_sents)\n",
    "BATCH_SIZE = 100\n",
    "NUM_BATCHES = (NUM_EN_SENTS // BATCH_SIZE) + 1\n",
    "i = 0\n",
    "transl_tat_kk_sents = []\n",
    "start = time.time()\n",
    "for i in range(NUM_BATCHES):\n",
    "    transl_tat_kk_sents.append(translateFromKazakh(tatoeba_kk_sents[i*BATCH_SIZE : (i+1)*BATCH_SIZE]))\n",
    "    print(\"Completed batch {:} of {:}\".format(i+1, NUM_BATCHES))\n",
    "end = time.time()\n",
    "print(\"Time taken: {:.3f}\".format(end-start))\n",
    "\n",
    "with open('Data/translated_kazakh_sentences_tat.txt', 'w') as f:\n",
    "    for batch in transl_tat_kk_sents:\n",
    "        for sent in batch:\n",
    "            f.write(\"%s\\n\" % sent)\n",
    "\n",
    "with open('Data/translated_kazakh_sentences_tat.txt', 'r') as f:\n",
    "    transl_tat_kk_sents = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5583982156282659, 0.7746913580007811, 0.4365217391304348)\n"
     ]
    }
   ],
   "source": [
    "all_sent_pairs_en_kk_T2, all_margin_scores_en_kk_T2 = mineSentencePairs(embed(transl_tat_kk_sents), \n",
    "                                                                      tatoeba_kk_embeddings,\n",
    "                                                                      'tat_en_kk_T2')\n",
    "print(computeF1(all_sent_pairs_en_kk_T2, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7869198264423436, 0.9999999999731903, 0.648695652173913)\n"
     ]
    }
   ],
   "source": [
    "usp = []\n",
    "for i in range(len(all_sent_pairs_en_kk)):\n",
    "    if all_margin_scores_en_kk[i] > 1.25:\n",
    "        usp.append(all_sent_pairs_en_kk[i])\n",
    "        \n",
    "print(computeF1(usp, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "# Getting our Google Translate API key from Google Cloud (this is one-time and user-specific)\n",
    "APIKEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/__init__.py\", line 44, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    }
   ],
   "source": [
    "service = build('translate', 'v2', developerKey=APIKEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateGoogle(sentence):\n",
    "    return (((service.translations().list(source='kk', target='en', q=[sentence]).execute())['translations'])[0])['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_tat_kk_sents_goog = [translateGoogle(sent) for sent in tatoeba_kk_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9664804469273743\n"
     ]
    }
   ],
   "source": [
    "all_sent_pairs_en_kk_goog2, all_margin_scores_en_kk_goog2 = mineSentencePairs(embed(transl_tat_kk_sents_goog), \n",
    "                                                                      tatoeba_en_embeddings,\n",
    "                                                                      'tat_en_kk_goog2')\n",
    "print(getAccuracyTatoeba(all_sent_pairs_en_kk_goog2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9147826086956522\n"
     ]
    }
   ],
   "source": [
    "r = len(all_sent_pairs_en_kk_goog)/575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [(i+1,i+1) for i in range(len(tatoeba_en_sents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9334532324075165, 0.9664804469093765, 0.9026086956521739)\n"
     ]
    }
   ],
   "source": [
    "print(computeF1(all_sent_pairs_en_kk_goog2, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575\n"
     ]
    }
   ],
   "source": [
    "print(len(transl_tat_en_sents_goog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9664804469273743\n"
     ]
    }
   ],
   "source": [
    "all_sent_pairs_en_kk_goog2, all_margin_scores_en_kk_goog2 = mineSentencePairs(embed(transl_tat_kk_to_en), \n",
    "                                                                      tatoeba_en_embeddings,\n",
    "                                                                      'tat_en_kk_goog2')\n",
    "print(getAccuracyTatoeba(all_sent_pairs_en_kk_goog2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9334532324075165, 0.9664804469093765, 0.9026086956521739)\n"
     ]
    }
   ],
   "source": [
    "print(computeF1(all_sent_pairs_en_kk_goog2, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/transl_tat_kk_to_en.txt', 'w') as f:\n",
    "    for sent in transl_tat_en_sents_goog2:\n",
    "        f.write(\"%s\\n\" % sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/transl_tat_en_to_kk.txt', 'w') as f:\n",
    "    for sent in transl_tat_en_sents_goog:\n",
    "        f.write(\"%s\\n\" % sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/transl_tat_en_to_kk.txt', 'r') as f:\n",
    "    transl_tat_en_to_kk = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/transl_tat_kk_to_en.txt', 'r') as f:\n",
    "    transl_tat_kk_to_en = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9706144206398394, 0.9945255474271072, 0.9478260869565217)\n"
     ]
    }
   ],
   "source": [
    "print(computeF1(all_sent_pairs_en_kk_goog, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pairs = []\n",
    "# for pair1, pair2 in zip(all_sent_pairs_en_kk_goog2, all_sent_pairs_tat_en_kk):\n",
    "#     merged_pairs.append(pair1)\n",
    "#     merged_pairs.append(pair2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_set = set(merged_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pairs_new = []\n",
    "# for i in range(len(all_sent_pairs_tat_en_kk)):\n",
    "#     for j in range(len(all_sent_pairs_en_kk_goog2)):\n",
    "#         if all_sent_pairs_tat_en_kk[i][0]==all_sent_pairs_en_kk_goog2[j][0]:\n",
    "#             if all_margin_scores_tat_en_kk[i] >= all_margin_scores_en_kk_goog2[j]:\n",
    "#                 merged_pairs_new.append(all_sent_pairs_tat_en_kk[i])\n",
    "#             else:\n",
    "#                 merged_pairs_new.append(all_sent_pairs_en_kk_goog2[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5142118824782165, 0.9999999999497488, 0.34608695652173915)\n"
     ]
    }
   ],
   "source": [
    "print(computeF1(list(set(all_sent_pairs_en_kk)&set(all_sent_pairs_en_kk_T)&set(all_sent_pairs_en_kk_T2)), ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8622047194879371, 0.9931972788890431, 0.7617391304347826)\n"
     ]
    }
   ],
   "source": [
    "pairs1 = set(all_sent_pairs_en_kk)\n",
    "pairs2 = set(all_sent_pairs_en_kk_T)\n",
    "pairs3 = set(all_sent_pairs_en_kk_T2)\n",
    "int1,int2,int3 = pairs1&pairs2, pairs1&pairs3, pairs2&pairs3\n",
    "majority_vote = list(set(int1|int2|int3))\n",
    "print(computeF1(majority_vote, ground_truth))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(majority_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0468919277191162\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the minimum margin score for a recalled ground-truth pair\n",
    "ground_truth_ms = []\n",
    "for i in range(len(all_margin_scores_en_kk_goog2)):\n",
    "    if all_sent_pairs_en_kk_goog2[i][0]==all_sent_pairs_en_kk_goog2[i][1]:\n",
    "        ground_truth_ms.append(all_margin_scores_en_kk_goog2[i])\n",
    "print(min(ground_truth_ms)) # We'll set this as our threshold when mining sentence pairs for NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE ABOVE METHOD ON KHMER TATOEBA DATA\n",
    "# Benchmark to beat: 83.2 (https://arxiv.org/pdf/2004.09813.pdf)\n",
    "\n",
    "# Loading in the test files\n",
    "# For every pair of lines i,j in zip(file_a_lines, file_b_lines),\n",
    "# a[i], b[j] represent a translation pair\n",
    "with open('Data/tatoeba.khm-eng.eng', 'r') as f:\n",
    "    tatoeba_en_sents_2 = f.read().splitlines()\n",
    "with open ('Data/tatoeba.khm-eng.khm', 'r') as f:\n",
    "    tatoeba_khm_sents = f.read().splitlines()\n",
    "    \n",
    "tatoeba_en_embeddings_2 = embed(tatoeba_en_sents_2)\n",
    "tatoeba_khm_embeddings = embed(tatoeba_khm_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93026941362916\n"
     ]
    }
   ],
   "source": [
    "all_sent_pairs_en_khm, all_margin_scores_en_khm = mineSentencePairs(tatoeba_en_embeddings_2, \n",
    "                                                                      tatoeba_khm_embeddings,\n",
    "                                                                      'tat_en_khm')\n",
    "print(getAccuracyTatoeba(all_sent_pairs_en_khm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9764521193092621\n"
     ]
    }
   ],
   "source": [
    "print(getAccuracyTatoeba(list(set(all_sent_pairs_en_khm)&set(all_sent_pairs_en_km_goog2)&set(all_sent_pairs_en_km_goog))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3639547663963645\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the minimum margin score for a recalled ground-truth pair\n",
    "ground_truth_ms_2 = []\n",
    "for i in range(len(all_margin_scores_en_km_goog2)):\n",
    "    if all_sent_pairs_en_km_goog2[i][0]==all_sent_pairs_en_km_goog2[i][1]:\n",
    "        ground_truth_ms_2.append(all_margin_scores_en_km_goog2[i])\n",
    "print(np.average(ground_truth_ms_2)) # We'll set this as our threshold when mining sentence pairs for NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derry wants me to try using word-by-word translation again, this time on Tatoeba . . .\n",
    "\n",
    "kk_en_dict = pd.read_csv(\"Data/new_merged_dict.kk\", names=['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kk_en_dict = {str(entry).split(\"\\t\")[0]:str(entry).split(\"\\t\")[1] for entry in kk_en_dict['entries'] \n",
    "                  if len(str(entry).split(\"\\t\"))==2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbw_transl_tat_kk = []\n",
    "for sent in tatoeba_kk_sents:\n",
    "    transl = \"\"\n",
    "    for word in sent.split():\n",
    "        if word in new_kk_en_dict:\n",
    "            transl += new_kk_en_dict[word] + \" \"\n",
    "        else:\n",
    "            transl += word + \" \"\n",
    "    wbw_transl_tat_kk.append(transl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8494726700205084, 0.9465811965609705, 0.7704347826086957)\n"
     ]
    }
   ],
   "source": [
    "all_sent_pairs_tat_en_kk_wbw1, all_margin_scores_tat_en_kk_wbw1 = mineSentencePairs(embed(wbw_transl_tat_kk), \n",
    "                                                                                       tatoeba_en_embeddings,\n",
    "                                                                                       \"tat_en_kk_wbw1\")\n",
    "print(computeF1(all_sent_pairs_tat_en_kk_wbw1, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_kk_dict = {str(entry).split(\"\\t\")[1]:str(entry).split(\"\\t\")[0] for entry in kk_en_dict['entries'] \n",
    "                  if len(str(entry).split(\"\\t\"))==2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbw_transl_tat_en = []\n",
    "for sent in tatoeba_en_sents:\n",
    "    transl = \"\"\n",
    "    for word in sent.split():\n",
    "        if word in en_kk_dict:\n",
    "            transl += en_kk_dict[word] + \" \"\n",
    "        else:\n",
    "            transl += word + \" \"\n",
    "    wbw_transl_tat_en.append(transl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.876064328048394, 0.9605809128431415, 0.8052173913043478)\n"
     ]
    }
   ],
   "source": [
    "all_sent_pairs_tat_en_kk_wbw2, all_margin_scores_tat_en_kk_wbw2 = mineSentencePairs(embed(wbw_transl_tat_en), \n",
    "                                                                                       tatoeba_kk_embeddings,\n",
    "                                                                                       \"tat_en_kk_wbw2\")\n",
    "print(computeF1(all_sent_pairs_tat_en_kk_wbw2, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AT LAST, mining pseudoparallel sentences from the en-kk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in and cleaning the data\n",
    "\n",
    "# Importing the English-Kazakh comparable corpus (Wikipedia)\n",
    "column_names = ['Kazakh', 'English']\n",
    "en_kk_comparable_corpus = pd.read_csv('/project/statnlp/ajones/kazakh_full_paragraph.csv', names=column_names).drop_duplicates().dropna()\n",
    "\n",
    "# Putting the sentences into lists\n",
    "kazakh_sentences = en_kk_comparable_corpus.Kazakh.to_list()\n",
    "english_sentences = en_kk_comparable_corpus.English.to_list()\n",
    "\n",
    "kazakh_sentences = [str(sentence) for sentence in kazakh_sentences]\n",
    "english_sentences = [str(sentence) for sentence in english_sentences]\n",
    "\n",
    "# Preprocessing the data\n",
    "split_english_sentences = [nltk.tokenize.sent_tokenize(sentence) for sentence in english_sentences]\n",
    "split_kazakh_sentences = [nltk.tokenize.sent_tokenize(sentence) for sentence in kazakh_sentences]\n",
    "\n",
    "def clean_english_sentence(data):\n",
    "    data = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", data) # gets rid of URLs\n",
    "    data = re.sub(r\"[^A-Za-z.!?',;:—0-9]\", \" \", data) # gets rid of non-standard characters\n",
    "    data = re.sub(r\" +\", \" \", data) # eliminates superfluous white space\n",
    "    return data\n",
    "\n",
    "def clean_english_data(data):\n",
    "    new_data = []\n",
    "    for paragraph in data:\n",
    "        paragraph = [clean_english_sentence(sentence) for sentence in paragraph]\n",
    "        paragraph = [sentence.replace('a href', '') for sentence in paragraph]\n",
    "        paragraph = [sentence.replace('href', '') for sentence in paragraph]\n",
    "        new_data.append(paragraph)\n",
    "    return new_data\n",
    "\n",
    "cleaned_english_sentences = clean_english_data(split_english_sentences)\n",
    "\n",
    "def clean_kazakh_sentence(data):\n",
    "    data = re.sub(r\"https?://[A-Za-z0-9./]+\", \" \", data) # gets rid of URLs\n",
    "    data = re.sub(r\"[^.!?',;:—АаӘәБбВвГгҒғДдЕеЁёЖжЗзИиЙйКкҚқЛлМмНнҢңОоӨөПпРрСсТтУуҰұҮүФфХхҺһЦцЧчШшЩщЪъЫыІіЬьЭэЮюЯя]\", \" \", data) # gets rid of non-standard characters\n",
    "    data = re.sub(r\" +\", \" \", data) # eliminates superfluous white space\n",
    "    return data\n",
    "\n",
    "def clean_kazakh_data(data):\n",
    "    new_data = []\n",
    "    for paragraph in data:\n",
    "        paragraph = [clean_kazakh_sentence(sentence) for sentence in paragraph]\n",
    "        paragraph = [sentence.replace('a href', '') for sentence in paragraph]\n",
    "        paragraph = [sentence.replace('href', '') for sentence in paragraph]\n",
    "        new_data.append(paragraph)\n",
    "    return new_data\n",
    "\n",
    "cleaned_kazakh_sentences = clean_kazakh_data(split_kazakh_sentences)\n",
    "\n",
    "for paragraph in cleaned_kazakh_sentences:\n",
    "    for sentence in paragraph:\n",
    "        if sentence.find('Тағы қара') != -1:\n",
    "            paragraph.remove(sentence)\n",
    "\n",
    "# There are some empty lists in the Kazakh sentences, so we'll just insert an empty\n",
    "# string into them (which can be removed after mining—we want to preserve document-level\n",
    "# alignment beforehand)\n",
    "for arr in cleaned_kazakh_sentences:\n",
    "    if len(arr)==0:\n",
    "        arr.append('')\n",
    "            \n",
    "# new_cleaned_kazakh_sentences = []\n",
    "# for i in range(len(cleaned_kazakh_sentences)):\n",
    "#     if len(cleaned_kazakh_sentences[i]) > 1:\n",
    "#         new_cleaned_kazakh_sentences.append(cleaned_kazakh_sentences[i])\n",
    "\n",
    "# cleaned_kazakh_sentences = new_cleaned_kazakh_sentences\n",
    "\n",
    "# cleaned_english_sentences_new = []\n",
    "# for par in cleaned_english_sentences:\n",
    "#     for sent in par:\n",
    "#         cleaned_english_sentences_new.append(sent)\n",
    "# cleaned_english_sentences = cleaned_english_sentences_new\n",
    "\n",
    "# cleaned_kazakh_sentences_new = []\n",
    "# for par in cleaned_kazakh_sentences:\n",
    "#     for sent in par:\n",
    "#         cleaned_kazakh_sentences_new.append(sent)\n",
    "# cleaned_kazakh_sentences = cleaned_kazakh_sentences_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_english_sentences)==len(cleaned_kazakh_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, mining the \"standard\" way\n",
    "# start = time.time()\n",
    "# all_sent_pairs_en_kk_old_method, all_margin_scores_en_kk_old_method = mineSentencePairs(embed(cleaned_english_sentences),\n",
    "#                                                                                        embed(cleaned_kazakh_sentences),\n",
    "#                                                                                        'en_kk_old_method')\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "cleaned_english_embeddings = embed(cleaned_english_sentences)\n",
    "end = time.time()\n",
    "print(\"Time taken to embed {} sentences: {.2f} seconds\".format(len(cleaned_english_embeddings), end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "cleaned_kazakh_embeddings = embed(cleaned_kazakh_sentences)\n",
    "end = time.time()\n",
    "print(\"Time taken to embed {} sentences: {:.2f} seconds\".format(len(cleaned_kazakh_embeddings), end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whoops . . . converting our English embeddings back to a document-separated format\n",
    "cleaned_en_embs_by_doc = []\n",
    "j = 0\n",
    "for doc in cleaned_english_sentences:\n",
    "    i = 0\n",
    "    doc_embs = np.zeros((len(doc), cleaned_english_embeddings[0].shape[0]), dtype=np.float32)\n",
    "    while i < len(doc):\n",
    "        doc_embs[i] = cleaned_english_embeddings[j]\n",
    "        j += 1\n",
    "        i += 1\n",
    "    cleaned_en_embs_by_doc.append(doc_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_en_embs_by_doc)==len(cleaned_english_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, mining the \"standard\" way\n",
    "# Because the search space is so large, and our computational resources are limited, we'll restrict the \n",
    "# search space to the document level, as opposed to mining globally\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 1e3\n",
    "all_sent_pairs_en_kk_old_method, all_margin_scores_en_kk_old_method = [], []\n",
    "for en_doc, kk_doc in zip(cleaned_en_embs_by_doc, cleaned_kazakh_sentences):\n",
    "    i += 1\n",
    "    if i % MOD == 0:\n",
    "        print(\"Completed document {} of {}\".format(i, len(cleaned_kazakh_sentences)), flush=True)\n",
    "    doc_sent_pairs, doc_margin_scores = mineSentencePairs(en_doc, embed(kk_doc), '')\n",
    "    all_sent_pairs_en_kk_old_method.append(doc_sent_pairs)\n",
    "    all_margin_scores_en_kk_old_method.append(doc_margin_scores)\n",
    "    \n",
    "# file_suffix = 'en_kk_old_method'\n",
    "    \n",
    "# spf_path = 'Data/sent_pairs_fwd_{:}.csv'.format(file_suffix)\n",
    "# spb_path = 'Data/sent_pairs_bwd_{:}.csv'.format(file_suffix)\n",
    "# cct_path = 'Data/concat_pairs_{:}.csv'.format(file_suffix)\n",
    "# mgn_path = 'Data/margin_scores_{:}.csv'.format(file_suffix)\n",
    "    \n",
    "# pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv(spf_path, index=False)\n",
    "# pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv(spb_path, index=False)\n",
    "# pd.DataFrame({'concat_pairs': concat_pairs}).to_csv(cct_path, index=False)\n",
    "# pd.DataFrame({'margin_scores': margin_scores}).to_csv(mgn_path, index=False)\n",
    "\n",
    "# all_margin_scores = pd.read_csv(mgn_path)\n",
    "# all_margin_scores = [float(score) for score in all_margin_scores['margin_scores']]\n",
    "# all_sent_pairs = pd.read_csv(cct_path)\n",
    "# all_sent_pairs = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs['concat_pairs']]\n",
    "# all_sent_pairs = [pair[0] for pair in all_sent_pairs]\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken to mine all sentence pairs: {:.2f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering using a set threshold\n",
    "\n",
    "MARGIN_THRESHOLD = 1.35\n",
    "updated_sent_pairs_en_kk_old_method = []\n",
    "for i in range(len(all_sent_pairs_en_kk_old_method)):\n",
    "    doc_pairs = []\n",
    "    for j in range(len(all_sent_pairs_en_kk_old_method[i])):\n",
    "        if all_margin_scores_en_kk_old_method[i][j] > MARGIN_THRESHOLD:\n",
    "            doc_pairs.append(all_sent_pairs_en_kk_old_method[i][j])\n",
    "    updated_sent_pairs_en_kk_old_method.append(doc_pairs)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(updated_sent_pairs_en_kk_old_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([len(l) for l in updated_sent_pairs_en_kk_old_method]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mined_en_sents_old, mined_kk_sents_old = [], []\n",
    "for i in range(len(updated_sent_pairs_en_kk_old_method)):\n",
    "    for j in range(len(updated_sent_pairs_en_kk_old_method[i])):\n",
    "        pair = updated_sent_pairs_en_kk_old_method[i][j]\n",
    "        mined_en_sents_old.append(cleaned_english_sentences[i][pair[0]-1])\n",
    "        mined_kk_sents_old.append(cleaned_kazakh_sentences[i][pair[1]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mined_en_sents_old)==len(mined_kk_sents_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In August 634 the dying Caliph  Abu 20Bakr Abu Bakr a called in 'Abd al Rahman and  Uthman Uthman a to inform them that he had designated  Umar 20ibn 20al Khattab Umar ibn al Khattab a as successor.\", 'He was buried on a hill to the north east of present day  Amman Amman a ,  Jordan Jordan a .', 'He is known for being one of the  Hadith 20of 20the 20ten 20promised 20paradise Ten Promised Paradise a .', 'At the time of the decision, therefore, the melody of O sole mio had not yet—as had been widely supposed—entered into the  public 20domain public domain a in any country that was a party to the  Berne 20Convention Berne Convention a during the relevant period.', \"When performing it in concert in the mid 1970s, Elvis would explain the origin of It's Now Or Never and have singer Sherrill Nielsen perform a few lines of the original Neapolitan version before commencing with his version.\", \"is the Neapolitan equivalent of standard Italian ' and translates literally as my sun or my sunshine .\", 'In 1915,  Charles 20W. 20Harrison Charles W. Harrison a recorded the first English translation of O sole mio .', 'In October 2002, Maria Alvau, a judge in  Turin Turin a , upheld the declaration, ruling that Mazzucchi had indeed been a legitimate co composer of the 18 songs, because they included melodies he had composed and then sold to di Capua in June of 1897, with a written authorisation for the latter to make free use of them.', \"The rewritten version was entitled  It 27s 20Now 20or 20Never 20 28song 29 It's Now or Never a and was a worldwide hit for Presley.\", ' O sole mio has been performed and covered by many artists, including  Enrico 20Caruso Enrico Caruso a ,  Rosa 20Ponselle Rosa Ponselle a and her sister Carmela,  Beniamino 20Gigli Beniamino Gigli a , and  Mario 20Lanza Mario Lanza a .']\n"
     ]
    }
   ],
   "source": [
    "print(mined_en_sents_old[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['халифа Әбу Бәкір Аллаһның оған игілігі мен сәлемі болсын жылы қайтыс болғанға дейін кейін Абдуррахманды шақырып Умар ибн әл Хаттаб ты өзінің орынбасары ретінде тағайындайтынын мәлімдейді.', 'Абдуррахман ибн Ауф Ислам дінін қабылдаған ең алғашқы сегіз адамдардың бірі болған.', 'Абдуррахман ибн Ауф басқа аты Абдел Рахман ибн Ғауф.', 'Абдуррахман ибн Ауф Абдуррахман ибн Ауф, көрнекті сахабалардың бірі болған.', 'Хиджраның жылы Осман ибн Аффан ның билігі кезінде Левант қайтыс болады, бүгінгі күнгі Амман , Иордания қаласының солтүстік беткейінде жерленеді.', 'Жаннатқа қабылданған он адамның бірі.', \"Шешім кезінде, 'О соле мио мелодиясы емес — бұл кеңінен деп болжам бойынша, енді кез келген елде ол қоғамдық игілік болып табылады, Берн Конвенциясына қатысушы кезеңге тиісті.\", \" жылдардың ортасында орындау кезінде оның концертінде Элвис ' Қазір немесе ешқашан шығу тегін түсіндіріп еді мен әнші Шерил Нильсен өз нұсқасының алдында итальяндық нұсқасының түпнұсқаның бірнеше жолдарын орындады.\", \"бұл стандартты итальян ' неаполитан баламасы, сөзбе сөз менің күнім деп аударылады.\", ' жылы Чарльз У. Харрисон О соле мионың бірінші ағылшын аудармасын жазып алған.']\n"
     ]
    }
   ],
   "source": [
    "print(mined_kk_sents_old[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217667\n"
     ]
    }
   ],
   "source": [
    "print(len(mined_en_sents_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the sentence pairs mined with the \"old\" method to a file\n",
    "pd.DataFrame({'source': mined_en_sents_old, 'target': mined_kk_sents_old}).to_csv('Data/en_kk_mined_old_method-1.35.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, mining using the new method (bidirectional translation + intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/__init__.py\", line 44, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    }
   ],
   "source": [
    "# Translating English sentences to Kazakh\n",
    "\n",
    "APIKEY = getpass.getpass()\n",
    "service = build('translate', 'v2', developerKey=APIKEY)\n",
    "\n",
    "def translateGoogle(sentence):\n",
    "    return (((service.translations().list(source='en', target='kk', q=[sentence]).execute())['translations'])[0])['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6862187385559082\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "en_to_kk_sents = []\n",
    "for doc in cleaned_english_sentences[3:5]:\n",
    "    doc_sents = []\n",
    "    for sent in doc:\n",
    "        doc_sents.append(translateGoogle(sent))\n",
    "    en_to_kk_sents.append(doc_sents)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_english_sentences[3])+len(cleaned_english_sentences[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4488396\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(doc) for doc in cleaned_english_sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.272161564327486"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4488396*2.294/19/3600/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for doc in cleaned_english_sentences:\n",
    "    for sent in doc:\n",
    "        x.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.82867455482483\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "a= [translateGoogle(sent) for sent in x[:100]]\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project/statnlp/ajones/english_sents_from_cc.txt', 'w') as f:\n",
    "    for sent in x:\n",
    "        f.write(\"%s\\n\" % sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project/statnlp/ajones/english_sents_from_cc.txt', 'r') as f:\n",
    "    eng_sents_from_cc = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4488396\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sents_from_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for doc in cleaned_kazakh_sentences:\n",
    "    for sent in doc:\n",
    "        y.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project/statnlp/ajones/kazakh_sents_from_cc.txt', 'w') as f:\n",
    "    for sent in y:\n",
    "        f.write(\"%s\\n\" % sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project/statnlp/ajones/kazakh_sents_from_cc.txt', 'r') as f:\n",
    "    kaz_sents_from_cc = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google.cloud\n",
      "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
      "Installing collected packages: google.cloud\n",
      "Successfully installed google.cloud\n"
     ]
    }
   ],
   "source": [
    "!pip install google.cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using batch translation for faster processing\n",
    "\n",
    "# Code source: https://cloud.google.com/translate/docs/advanced/batch-translation#translate_v3_batch_translate_text-python\n",
    "\n",
    "from google.cloud import translate\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=\"Data/mineral-weaver-301202-eeca7bc928a3.json\"\n",
    "\n",
    "def batch_translate_text(\n",
    "    input_uri=\"gs://YOUR_BUCKET_ID/path/to/your/file.txt\",\n",
    "    output_uri=\"gs://YOUR_BUCKET_ID/path/to/save/results/\",\n",
    "    project_id=\"YOUR_PROJECT_ID\",\n",
    "    timeout=180\n",
    "):\n",
    "    \"\"\"Translates a batch of texts on GCS and stores the result in a GCS location.\"\"\"\n",
    "\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    location = \"us-central1\"\n",
    "    # Supported file types: https://cloud.google.com/translate/docs/supported-formats\n",
    "    gcs_source = {\"input_uri\": input_uri}\n",
    "\n",
    "    input_configs_element = {\n",
    "        \"gcs_source\": gcs_source,\n",
    "        \"mime_type\": \"text/plain\",  # Can be \"text/plain\" or \"text/html\".\n",
    "    }\n",
    "    gcs_destination = {\"output_uri_prefix\": output_uri}\n",
    "    output_config = {\"gcs_destination\": gcs_destination}\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "\n",
    "    # Supported language codes: https://cloud.google.com/translate/docs/language\n",
    "    operation = client.batch_translate_text(\n",
    "        request={\n",
    "            \"parent\": parent,\n",
    "            \"source_language_code\": \"kk\",\n",
    "            \"target_language_codes\": [\"en\"],  # Up to 10 language codes here.\n",
    "            \"input_configs\": [input_configs_element],\n",
    "            \"output_config\": output_config,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = operation.result(timeout)\n",
    "\n",
    "    print(\"Total Characters: {}\".format(response.total_characters))\n",
    "    print(\"Translated Characters: {}\".format(response.translated_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "batch_translate_text(input_uri=\"gs://translation-for-bitext-mining/kazakh_sents_from_cc.txt/\",\n",
    "                    output_uri=\"gs://translated-cc-out/\",\n",
    "                    project_id=\"mineral-weaver-301202\",\n",
    "                    timeout=100000)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usr2/collab/agjones/Data/mineral-weaver-301202-eeca7bc928a3.json\n"
     ]
    }
   ],
   "source": [
    "print(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"usr2/collab/agjones/Data/mineral-weaver-301202-eeca7bc928a3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GOOGLE_APPLICATION_CREDENTIALS=\"usr2/collab/agjones/Data/mineral-weaver-301202-eeca7bc928a3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        'Data/mineral-weaver-301202-a2309a626c27.json')\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 GET https://storage.googleapis.com/storage/v1/b?project=mineral-weaver-301202&projection=noAcl&prettyPrint=false: alex-s-account@mineral-weaver-301202.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud project.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8c8965b1832d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstorage_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbuckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_items_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_items_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;34m\"\"\"Iterator for each item returned.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_results\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_page_iter\u001b[0;34m(self, increment)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mPage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meach\u001b[0m \u001b[0mpage\u001b[0m \u001b[0mof\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_page_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_to_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_get_next_page_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             return self.api_request(\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m             )\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             )\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 GET https://storage.googleapis.com/storage/v1/b?project=mineral-weaver-301202&projection=noAcl&prettyPrint=false: alex-s-account@mineral-weaver-301202.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud project."
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client()\n",
    "buckets = list(storage_client.list_buckets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 GET https://storage.googleapis.com/storage/v1/b?project=mineral-weaver-301202&projection=noAcl&prettyPrint=false: alex-s-account@mineral-weaver-301202.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud project.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-eb17ecf05263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplicit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-f4ac4f97ec7b>\u001b[0m in \u001b[0;36mexplicit\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Make an authenticated API request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbuckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_buckets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_items_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_items_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;34m\"\"\"Iterator for each item returned.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_results\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_page_iter\u001b[0;34m(self, increment)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mPage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meach\u001b[0m \u001b[0mpage\u001b[0m \u001b[0mof\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_page_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_to_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_page\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_get_next_page_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             return self.api_request(\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m             )\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/storage/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             )\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 GET https://storage.googleapis.com/storage/v1/b?project=mineral-weaver-301202&projection=noAcl&prettyPrint=false: alex-s-account@mineral-weaver-301202.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud project."
     ]
    }
   ],
   "source": [
    "explicit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(\"translation-for-bitext-mining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 3.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2.0dev,>=1.11.0 in ./.local/lib/python3.7/site-packages (from google-cloud-storage) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in ./.local/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in ./.local/lib/python3.7/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./.local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.2.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in ./.local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (51.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.6)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (1.15.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.52.0)\n",
      "Requirement already satisfied: pytz in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2020.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in ./.local/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (3.14.0)\n",
      "Collecting google-resumable-media<2.0dev,>=1.2.0\n",
      "  Downloading google_resumable_media-1.2.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.1.0-cp37-cp37m-manylinux2010_x86_64.whl (39 kB)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage) (1.14.3)\n",
      "Requirement already satisfied: pycparser in /share/pkg.7/python3/3.7.9/install/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage) (2.20)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./.local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Installing collected packages: google-crc32c, google-resumable-media, google-cloud-storage\n",
      "Successfully installed google-cloud-storage-1.35.0 google-crc32c-1.1.0 google-resumable-media-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1677.345908164978\n"
     ]
    }
   ],
   "source": [
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182022\n"
     ]
    }
   ],
   "source": [
    "# Mining from a subset of the en-kk data in order to obtain the \"pseudo-gold-standard\" threshold\n",
    "\n",
    "# Shoot for a subset of documents containing ~10K sentences\n",
    "print(sum([len(doc) for doc in cleaned_english_sentences[2000:7000]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateGoogle(sentences: list, src_code: str, tgt_code: str):\n",
    "    transl_doc = (((service.translations().list(source=src_code, target=tgt_code, q=sentences, format='text').execute())))#['translations'])[0])['translatedText']\n",
    "    return [sent['translatedText'] for sent in transl_doc['translations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"dit is 'n sin\"]\n"
     ]
    }
   ],
   "source": [
    "print(translateGoogle([\"this is a sentence\"], \"en\", \"af\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished doc at index 2200\n",
      "Finished doc at index 2400\n",
      "Finished doc at index 2600\n",
      "Finished doc at index 2800\n",
      "Finished doc at index 3000\n",
      "Finished doc at index 3200\n",
      "Finished doc at index 3400\n",
      "Finished doc at index 3800\n",
      "Finished doc at index 4000\n",
      "Finished doc at index 4400\n",
      "Finished doc at index 4600\n",
      "Finished doc at index 4800\n",
      "Finished doc at index 5000\n",
      "Finished doc at index 5200\n",
      "Finished doc at index 5400\n",
      "Finished doc at index 5600\n",
      "Finished doc at index 5800\n",
      "Finished doc at index 6000\n",
      "Finished doc at index 6200\n",
      "Finished doc at index 6400\n",
      "Finished doc at index 6600\n",
      "Finished doc at index 6800\n",
      "Finished doc at index 7000\n",
      "2403.6892714500427\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "transl_en_sents_goog = []\n",
    "bad_indices = []\n",
    "i = 2000\n",
    "for doc in cleaned_english_sentences[2000:7000]:\n",
    "    try:\n",
    "        transl_en_sents_goog.append(translateGoogle(doc, \"en\", \"kk\"))\n",
    "        i += 1\n",
    "        if i % 200 == 0:\n",
    "            print(\"Finished doc at index {}\".format(i), flush=True)\n",
    "    except:\n",
    "        bad_indices.append(i)\n",
    "        i += 1\n",
    "        continue\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84759\n"
     ]
    }
   ],
   "source": [
    "# Number of translated sents we ended up with\n",
    "print(sum([len(d) for d in transl_en_sents_goog]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished doc at index 2200\n",
      "Finished doc at index 2400\n",
      "Finished doc at index 2600\n",
      "Finished doc at index 2800\n",
      "Finished doc at index 3000\n",
      "Finished doc at index 3200\n",
      "Finished doc at index 3400\n",
      "Finished doc at index 3800\n",
      "Finished doc at index 4000\n",
      "Finished doc at index 4400\n",
      "Finished doc at index 4600\n",
      "Finished doc at index 4800\n",
      "Finished doc at index 5000\n",
      "Finished doc at index 5200\n",
      "Finished doc at index 5400\n",
      "Finished doc at index 5600\n",
      "Finished doc at index 5800\n",
      "Finished doc at index 6000\n",
      "Finished doc at index 6200\n",
      "Finished doc at index 6400\n",
      "Finished doc at index 6600\n",
      "Finished doc at index 6800\n",
      "Finished doc at index 7000\n",
      "850.3558962345123\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "transl_kk_sents_goog = []\n",
    "bad_indices_2 = []\n",
    "i = 2000\n",
    "for i in range(2000, 7000):\n",
    "    if i not in bad_indices:\n",
    "        try:\n",
    "            transl_kk_sents_goog.append(translateGoogle(cleaned_kazakh_sentences[i], \"kk\", \"en\"))\n",
    "            i += 1\n",
    "            if i % 200 == 0:\n",
    "                print(\"Finished doc at index {}\".format(i), flush=True)\n",
    "        except:\n",
    "            bad_indices_2.append(i)\n",
    "            i += 1\n",
    "            continue\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(len(transl_kk_sents_goog)==len(transl_en_sents_goog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sents_en = [cleaned_english_sentences[i] for i in range(2000,7000) if i not in bad_indices]\n",
    "subset_sents_kk = [cleaned_kazakh_sentences[i] for i in range(2000,7000) if i not in bad_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sents_en_new = []\n",
    "for doc in subset_sents_en:\n",
    "    for sent in doc:\n",
    "        subset_sents_en_new.append(sent)\n",
    "subset_sents_kk_new = []\n",
    "for doc in subset_sents_kk:\n",
    "    for sent in doc:\n",
    "        subset_sents_kk_new.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to mine all sentence pairs: 310.05 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now, we mine in three ways, do majority vote, and take minimum threshold\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "i = 0\n",
    "MOD = 50\n",
    "all_sent_pairs_en_kk_og, all_margin_scores_en_kk_og = mineSentencePairs(embed(subset_sents_en_new), embed(subset_sents_kk_new), '')\n",
    "#     all_sent_pairs_en_kk_og.append(doc_sent_pairs)\n",
    "#     all_margin_scores_en_kk_og.append(doc_margin_scores)\n",
    "# for en_doc, kk_doc in zip(subset_sents_en, subset_sents_kk):\n",
    "#     i += 1\n",
    "#     if i % MOD == 0:\n",
    "#         print(\"Completed document {} of {}\".format(i, len(subset_sents_en)), flush=True)\n",
    "#     doc_sent_pairs, doc_margin_scores = mineSentencePairs(embed(en_doc), embed(kk_doc), '')\n",
    "#     all_sent_pairs_en_kk_og.append(doc_sent_pairs)\n",
    "#     all_margin_scores_en_kk_og.append(doc_margin_scores)\n",
    "    \n",
    "# file_suffix = 'en_kk_old_method'\n",
    "    \n",
    "# spf_path = 'Data/sent_pairs_fwd_{:}.csv'.format(file_suffix)\n",
    "# spb_path = 'Data/sent_pairs_bwd_{:}.csv'.format(file_suffix)\n",
    "# cct_path = 'Data/concat_pairs_{:}.csv'.format(file_suffix)\n",
    "# mgn_path = 'Data/margin_scores_{:}.csv'.format(file_suffix)\n",
    "    \n",
    "# pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv(spf_path, index=False)\n",
    "# pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv(spb_path, index=False)\n",
    "# pd.DataFrame({'concat_pairs': concat_pairs}).to_csv(cct_path, index=False)\n",
    "# pd.DataFrame({'margin_scores': margin_scores}).to_csv(mgn_path, index=False)\n",
    "\n",
    "# all_margin_scores = pd.read_csv(mgn_path)\n",
    "# all_margin_scores = [float(score) for score in all_margin_scores['margin_scores']]\n",
    "# all_sent_pairs = pd.read_csv(cct_path)\n",
    "# all_sent_pairs = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs['concat_pairs']]\n",
    "# all_sent_pairs = [pair[0] for pair in all_sent_pairs]\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken to mine all sentence pairs: {:.2f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13907\n"
     ]
    }
   ],
   "source": [
    "# No. of sent pairs mined using original sents\n",
    "print(len(all_sent_pairs_en_kk_og))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_en_sents_goog_new = []\n",
    "for doc in transl_en_sents_goog:\n",
    "    for sent in doc:\n",
    "        transl_en_sents_goog_new.append(sent)\n",
    "transl_kk_sents_goog_new = []\n",
    "for doc in transl_kk_sents_goog:\n",
    "    for sent in doc:\n",
    "        transl_kk_sents_goog_new.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to mine all sentence pairs: 317.72 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now, we mine in three ways, do majority vote, and take minimum threshold\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# i = 0\n",
    "# MOD = 50\n",
    "all_sent_pairs_en_to_kk, all_margin_scores_en_to_kk = mineSentencePairs(embed(transl_en_sents_goog_new), embed(subset_sents_kk_new), '')\n",
    "# for transl_en_doc, kk_doc in zip(transl_en_sents_goog, subset_sents_kk):\n",
    "#     i += 1\n",
    "#     if i % MOD == 0:\n",
    "#         print(\"Completed document {} of {}\".format(i, len(subset_sents_kk)), flush=True)\n",
    "#     doc_sent_pairs, doc_margin_scores = mineSentencePairs(embed(transl_en_doc), embed(kk_doc), '')\n",
    "#     all_sent_pairs_en_to_kk.append(doc_sent_pairs)\n",
    "#     all_margin_scores_en_to_kk.append(doc_margin_scores)\n",
    "    \n",
    "# file_suffix = 'en_kk_old_method'\n",
    "    \n",
    "# spf_path = 'Data/sent_pairs_fwd_{:}.csv'.format(file_suffix)\n",
    "# spb_path = 'Data/sent_pairs_bwd_{:}.csv'.format(file_suffix)\n",
    "# cct_path = 'Data/concat_pairs_{:}.csv'.format(file_suffix)\n",
    "# mgn_path = 'Data/margin_scores_{:}.csv'.format(file_suffix)\n",
    "    \n",
    "# pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv(spf_path, index=False)\n",
    "# pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv(spb_path, index=False)\n",
    "# pd.DataFrame({'concat_pairs': concat_pairs}).to_csv(cct_path, index=False)\n",
    "# pd.DataFrame({'margin_scores': margin_scores}).to_csv(mgn_path, index=False)\n",
    "\n",
    "# all_margin_scores = pd.read_csv(mgn_path)\n",
    "# all_margin_scores = [float(score) for score in all_margin_scores['margin_scores']]\n",
    "# all_sent_pairs = pd.read_csv(cct_path)\n",
    "# all_sent_pairs = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs['concat_pairs']]\n",
    "# all_sent_pairs = [pair[0] for pair in all_sent_pairs]\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken to mine all sentence pairs: {:.2f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to mine all sentence pairs: 268.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now, we mine in three ways, do majority vote, and take minimum threshold\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# i = 0\n",
    "# MOD = 50\n",
    "all_sent_pairs_kk_to_en, all_margin_scores_kk_to_en = mineSentencePairs(embed(subset_sents_en_new), embed(transl_kk_sents_goog_new), '')\n",
    "# for en_doc, transl_kk_doc in zip(subset_sents_en, transl_kk_sents_goog):\n",
    "#     i += 1\n",
    "#     if i % MOD == 0:\n",
    "#         print(\"Completed document {} of {}\".format(i, len(subset_sents_en)), flush=True)\n",
    "#     doc_sent_pairs, doc_margin_scores = mineSentencePairs(embed(en_doc), embed(transl_kk_doc), '')\n",
    "#     all_sent_pairs_kk_to_en.append(doc_sent_pairs)\n",
    "#     all_margin_scores_kk_to_en.append(doc_margin_scores)\n",
    "    \n",
    "# file_suffix = 'en_kk_old_method'\n",
    "    \n",
    "# spf_path = 'Data/sent_pairs_fwd_{:}.csv'.format(file_suffix)\n",
    "# spb_path = 'Data/sent_pairs_bwd_{:}.csv'.format(file_suffix)\n",
    "# cct_path = 'Data/concat_pairs_{:}.csv'.format(file_suffix)\n",
    "# mgn_path = 'Data/margin_scores_{:}.csv'.format(file_suffix)\n",
    "    \n",
    "# pd.DataFrame({'sent_pairs_fwd': fwd_best}).to_csv(spf_path, index=False)\n",
    "# pd.DataFrame({'sent_pairs_bwd': bwd_best}).to_csv(spb_path, index=False)\n",
    "# pd.DataFrame({'concat_pairs': concat_pairs}).to_csv(cct_path, index=False)\n",
    "# pd.DataFrame({'margin_scores': margin_scores}).to_csv(mgn_path, index=False)\n",
    "\n",
    "# all_margin_scores = pd.read_csv(mgn_path)\n",
    "# all_margin_scores = [float(score) for score in all_margin_scores['margin_scores']]\n",
    "# all_sent_pairs = pd.read_csv(cct_path)\n",
    "# all_sent_pairs = [[tuple(int(id_) for id_ in pair.strip('()').split(','))] for pair in all_sent_pairs['concat_pairs']]\n",
    "# all_sent_pairs = [pair[0] for pair in all_sent_pairs]\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken to mine all sentence pairs: {:.2f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip1, trip2, trip3 = [], [], []\n",
    "for p,s in zip(all_sent_pairs_en_kk_og, all_margin_scores_en_kk_og):\n",
    "    trip1.append((p[0], p[1], s))\n",
    "for p,s in zip(all_sent_pairs_en_to_kk, all_margin_scores_en_to_kk):\n",
    "    trip2.append((p[0], p[1], s))\n",
    "for p,s in zip(all_sent_pairs_kk_to_en, all_margin_scores_kk_to_en):\n",
    "    trip3.append((p[0], p[1], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_vote_by_doc = []\n",
    "for doc1, doc2, doc3 in zip(trip1, trip2, trip3):\n",
    "    pairs1 = set(doc1)\n",
    "    pairs2 = set(doc2)\n",
    "    pairs3 = set(doc3)\n",
    "    int1,int2,int3 = pairs1&pairs2, pairs1&pairs3, pairs2&pairs3\n",
    "    majority_vote = list(set(int1|int2|int3))\n",
    "    maj_vote_by_doc.append(majority_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "for doc in maj_vote_by_doc:\n",
    "    if len(doc) > 0:\n",
    "        for score in doc:\n",
    "            all_scores.append(score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2551777978129939\n"
     ]
    }
   ],
   "source": [
    "print(np.average(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs1 = set(all_sent_pairs_en_kk_og)\n",
    "pairs2 = set(all_sent_pairs_en_to_kk)\n",
    "pairs3 = set(all_sent_pairs_kk_to_en)\n",
    "int1,int2,int3 = pairs1&pairs2, pairs1&pairs3, pairs2&pairs3\n",
    "majority_vote = list(set(int1|int2|int3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6997\n"
     ]
    }
   ],
   "source": [
    "print(len(majority_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "for pair in majority_vote:\n",
    "    if pair in pairs1:\n",
    "        all_scores.append(trip1[list(pairs1).index(pair)][2])\n",
    "    elif pair in pairs2:\n",
    "        all_scores.append(trip2[list(pairs2).index(pair)][2])\n",
    "    else:\n",
    "        all_scores.append(trip3[list(pairs3).index(pair)][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.072879764477287\n"
     ]
    }
   ],
   "source": [
    "print(np.average(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9488486647605896, 0.9579350352287292, 0.9591459631919861, 0.9594359993934631, 0.9634297490119934, 0.9634584784507751, 0.9643964171409607, 0.9648369550704956, 0.9649688005447388, 0.9679153561592102]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(all_scores)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6997\n"
     ]
    }
   ],
   "source": [
    "print(len(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents_out = [subset_sents_en_new[pair[0]-1] for pair in majority_vote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk_sents_out = [subset_sents_kk_new[pair[1]-1] for pair in majority_vote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(en_sents_out)==len(kk_sents_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Astana won in the nomination of the City of the Future by the National Geographic version.', 'He was elected a Member of the USSR Academy of Sciences in 1958.', \"'Yes,' I agreed.\", 'The club was founded in 1995.', 'Acquigny Acquigny is a  Communes 20of 20France commune a in the  Eure Eure a  Departments 20of 20France department a in  Normandy Normandy a in northern  France France a .', 'Ak Zhol Democratic Party The Ak Zhol Democratic Party , commonly referred to simply as Ak Zhol , is a  Liberalism liberal a  political 20party political party a in  Kazakhstan Kazakhstan a .', 'Despite this width, the Beluga cannot carry most fuselage parts of the  Airbus 20A380 Airbus A380 a , which are instead normally transported by ship and road.', 'The ancient Greeks used agrimony to treat eye ailments, and it was made into brews for diarrhea and disorders of the gallbladder, liver, and kidneys.', 'The new university would take its name some decades later.', 'Isgandarov got his first education in director actor field in Baku Technical School of Theatre.']\n",
      "[' журналының шешімімен, Астана Болашақ қаласы аталымын жеңіп алды.', ' жылдан КСРО ғылым академиясының Президиум і жанында кеңесші болды.', ' : Әрине сенемін, бірақ жүрегім орнықсын деді.', 'Клуб жылы құрылған.', 'Акиньи Акиньи — Франция дағы коммуна.', 'Ақ жол демократиялық партиясы Қазақстанның АҚ ЖОЛ Демократиялық партиясы қоғамдық б рлест г — Қазақстан ның саяси партия сы.', 'Бірақ Белуганың жүктемесі азырақ болып келеді: Белугада тонна, да , т, ал Ан те т. Осыған қарамастан Белуга ұшағының фюзеляж бөліктерін тасымалдай алмайды, оларды кемемен және автокөлікпен тасымалдайды.', 'Андыз ерте заманнан ақ бауыр, бүйрек ауруларына қарсы, жүрек қатты соққанда және несеп айдайтын дәрі есебінде қолданылып келген.', 'Ол университетің бірнеше жыл бұрын бітірген.', 'Алғашқыда Киев университеті нің медициналық факультетінде, одан кейін режиссер А. Н. Пагаваның театр студиясы нда Тбилиси оқыды.']\n"
     ]
    }
   ],
   "source": [
    "print(en_sents_out[:10])\n",
    "print(kk_sents_out[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_subset_pairs = pd.DataFrame({'en': en_sents_out, 'kk': kk_sents_out}).to_csv('Data/en_kk_7k_majority_vote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "usp = []\n",
    "for i in range(len(majority_vote)):\n",
    "    if all_scores[i] > 1.06:\n",
    "        usp.append((majority_vote[i][0], majority_vote[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3181\n"
     ]
    }
   ],
   "source": [
    "print(len(usp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(69386, 26088), (26050, 8897), (48393, 16823), (46946, 16151), (41847, 8756), (33162, 1345), (31862, 16862), (37659, 12154), (16647, 24768), (64845, 24173)]\n"
     ]
    }
   ],
   "source": [
    "print(usp[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents_out_106 = [subset_sents_en_new[pair[0]-1] for pair in usp]\n",
    "kk_sents_out_106 = [subset_sents_kk_new[pair[1]-1] for pair in usp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'en': en_sents_out_106, 'kk': kk_sents_out_106}).to_csv('Data/en_kk_7k_majority_vote_1.06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline for retrieving Tatoeba test data results\n",
    "\n",
    "def retrieveTatoebaComparisons(lang_id_3, lang_id_2):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # We'll store the results here and return them at the end\n",
    "    all_results = []\n",
    "    \n",
    "    print(\"Reading, translating, embedding . . .\")\n",
    "    with open('Data/tatoeba.{}-eng.eng'.format(lang_id_3), 'r') as f:\n",
    "        en_sents = f.read().splitlines()\n",
    "    with open('Data/tatoeba.{}-eng.{}'.format(lang_id_3, lang_id_3), 'r') as f:\n",
    "        xx_sents = f.read().splitlines()\n",
    "#     en_to_xx_sents = [translateGoogle([en_sent], 'en', lang_id_2)[0] for en_sent in en_sents]\n",
    "#     xx_to_en_sents = [translateGoogle([xx_sent], lang_id_2, 'en')[0] for xx_sent in xx_sents]\n",
    "    en_emb = embed(en_sents)\n",
    "    xx_emb = embed(xx_sents)\n",
    "#     en_to_xx_emb = embed(en_to_xx_sents)\n",
    "#     xx_to_en_emb = embed(xx_to_en_sents)\n",
    "    \n",
    "    ground_truth_pairs = [(i+1,i+1) for i in range(len(en_sents))]\n",
    "    \n",
    "    print(\"Mining and scoring . . .\")\n",
    "    # METHOD 1: Margin scoring, no threshold\n",
    "    sent_pairs_og, marg_scores_og = mineSentencePairs(en_emb, xx_emb, '')\n",
    "    F1, P, R = computeF1(sent_pairs_og, ground_truth_pairs)\n",
    "    print(\"F1, P, R for {} with no threshold: {}, {}, {}\".format(lang_id_3, F1, P, R))\n",
    "    all_results.append((P, R, F1))\n",
    "    \n",
    "    # METHOD 2: Margin scoring, threshold = 1.06\n",
    "    above_threshold = []\n",
    "    THRESHOLD = 1.06\n",
    "    for i in range(len(sent_pairs_og)):\n",
    "        if marg_scores_og[i] > THRESHOLD:\n",
    "            above_threshold.append(sent_pairs_og[i])\n",
    "    F1, P, R = computeF1(above_threshold, ground_truth_pairs)\n",
    "    print(\"F1, P, R for {} with threshold = 1.06: {}, {}, {}\".format(lang_id_3, F1, P, R))\n",
    "    all_results.append((P, R, F1))\n",
    "    \n",
    "    # METHOD 3: Margin scoring, threshold = 1.20\n",
    "    above_threshold = []    \n",
    "    THRESHOLD = 1.20\n",
    "    for i in range(len(sent_pairs_og)):\n",
    "        if marg_scores_og[i] > THRESHOLD:\n",
    "            above_threshold.append(sent_pairs_og[i])\n",
    "    F1, P, R = computeF1(above_threshold, ground_truth_pairs)\n",
    "    print(\"F1, P, R for {} with threshold = 1.20: {}, {}, {}\".format(lang_id_3, F1, P, R))\n",
    "    all_results.append((P, R, F1))\n",
    "    \n",
    "    # METHOD 4: Margin scoring, translation en->xx, no threshold\n",
    "    sent_pairs_en_xx, marg_scores_en_xx = mineSentencePairs(en_to_xx_emb, xx_emb, '')\n",
    "    F1, P, R = computeF1(sent_pairs_en_xx, ground_truth_pairs)\n",
    "    print(\"F1, P, R for {} using translation en-xx: {}, {}, {}\".format(lang_id_3, F1, P, R))\n",
    "    all_results.append((P, R, F1))\n",
    "    \n",
    "    # METHOD 5: Margin scoring, translation xx->en, no threshold\n",
    "    sent_pairs_xx_en, marg_scores_xx_en = mineSentencePairs(en_emb, xx_to_en_emb, '')\n",
    "    F1, P, R = computeF1(sent_pairs_xx_en, ground_truth_pairs)\n",
    "    print(\"F1, P, R for {} using translation xx_en: {}, {}, {}\".format(lang_id_3, F1, P, R))\n",
    "    all_results.append((P, R, F1))\n",
    "    \n",
    "    # METHOD 6: Intersection of methods 1, 4, and 5\n",
    "    sent_pairs_triple_intersection = list(set(sent_pairs_og)&set(sent_pairs_en_xx)&set(sent_pairs_xx_en))\n",
    "    F1, P, R = computeF1(sent_pairs_triple_intersection, ground_truth_pairs)\n",
    "    print(\"F1, P, R for {} using triple intersection: {}, {}, {}\".format(lang_id_3, F1, P, R))\n",
    "    all_results.append((P, R, F1))\n",
    "    \n",
    "    # METHOD 7: Majority vote intersection of methods 1, 4, and 5\n",
    "    int1 = set(sent_pairs_og)&set(sent_pairs_en_xx)\n",
    "    int2 = set(sent_pairs_en_xx)&set(sent_pairs_xx_en)\n",
    "    int3 = set(sent_pairs_og)&set(sent_pairs_xx_en)\n",
    "    sent_pairs_majority_vote = list(set(int1|int2|int3))\n",
    "    F1, P, R = computeF1(sent_pairs_majority_vote, ground_truth_pairs)\n",
    "    print(\"F1, P, R for {} using majority vote intersection: {}, {}, {}\".format(lang_id_3, F1, P, R))\n",
    "    all_results.append((P, R, F1))\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Time taken: {:.2f} seconds\".format(end-start))\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_codes_3c = ['afr', 'amh', 'ang', 'arq', 'arz', 'ast',\n",
    "                 'awa', 'aze', 'bel', 'ben', 'ber', 'bos',\n",
    "                 'bre', 'cbk', 'ceb', 'cha', 'cor', 'csb', \n",
    "                 'cym', 'dsb', 'dtp', 'epo', 'eus', 'fao', \n",
    "                 'fry', 'gla', 'gle', 'gsw', 'hsb', 'ido', \n",
    "                 'ile', 'ina', 'isl', 'jav', 'kab', 'kaz', \n",
    "                 'khm', 'kur', 'kzj', 'lat', 'lfn', 'mal', \n",
    "                 'maz', 'mhr', 'nds', 'nno', 'nov', 'oci', \n",
    "                 'orv', 'pam', 'pms', 'swg', 'swh', 'tam', \n",
    "                 'tat', 'tel', 'tgl', 'tuk', 'tzl', 'uig', \n",
    "                 'uzb', 'war', 'wuu', 'xho', 'yid'] \n",
    "lang_codes_2c = ['af', 'am', False, False, False, False, \n",
    "                 False, 'az', 'be', 'bn', False, 'bs',\n",
    "                 False, False, 'ceb', False, 'co', False,\n",
    "                 'cy', 'pl', False, 'eo', 'eu', False, \n",
    "                 'fy', 'gd', 'ga', 'pl', 'eo', 'la',\n",
    "                 'la', 'is', 'jv', False, 'kk', 'km',\n",
    "                 'ku', False, 'lv', False, 'mt', False,\n",
    "                 False, 'de', 'no', False, False, False,\n",
    "                 False, False, False, 'sw', 'ta', 'tt',\n",
    "                 'te', 'tl', 'tk', False, 'ug', 'uz',\n",
    "                 False, False, 'xh', 'yi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for kaz with no threshold: 0.925589831661704, 0.9677419354655077, 0.8869565217391304\n",
      "F1, P, R for kaz with threshold = 1.06: 0.921532841719058, 0.9692898272366739, 0.8782608695652174\n",
      "F1, P, R for kaz with threshold = 1.20: 0.8568608045597949, 0.9908675798860532, 0.7547826086956522\n",
      "F1, P, R for kaz using translation en-xx: 0.9264305127126607, 0.9695817490309965, 0.8869565217391304\n",
      "F1, P, R for kaz using translation xx_en: 0.9352517935585793, 0.9683426443022655, 0.9043478260869565\n",
      "F1, P, R for kaz using triple intersection: 0.9116541303624429, 0.9918200408795129, 0.8434782608695652\n",
      "F1, P, R for kaz using majority vote intersection: 0.9309090859109587, 0.9752380952195192, 0.8904347826086957\n",
      "Time taken: 275.78 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for khm with no threshold: 0.8677013993029753, 0.9302694136144173, 0.8130193905817175\n",
      "F1, P, R for khm with threshold = 1.06: 0.8644578263569414, 0.947194719456317, 0.7950138504155124\n",
      "F1, P, R for khm with threshold = 1.20: 0.7640067864199344, 0.9868421052415166, 0.6232686980609419\n",
      "F1, P, R for khm using translation en-xx: 0.8732394316380417, 0.939393939378957, 0.8157894736842105\n",
      "F1, P, R for khm using translation xx_en: 0.874631263451186, 0.9353312302691588, 0.8213296398891967\n",
      "F1, P, R for khm using triple intersection: 0.8562499950753907, 0.982078853028995, 0.7590027700831025\n",
      "F1, P, R for khm using majority vote intersection: 0.8779761854973381, 0.9485530546471294, 0.817174515235457\n",
      "Time taken: 194.72 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for kur with no threshold: 0.9210866702976425, 0.9807162534165091, 0.8682926829268293\n",
      "F1, P, R for kur with threshold = 1.06: 0.9138381151173571, 0.9831460673881138, 0.8536585365853658\n",
      "F1, P, R for kur with threshold = 1.20: 0.8290598241892922, 0.9965753424316242, 0.7097560975609756\n",
      "F1, P, R for kur using translation en-xx: 0.9201030877877231, 0.9754098360389232, 0.8707317073170732\n",
      "F1, P, R for kur using translation xx_en: 0.9557522073840186, 0.9921259842259285, 0.9219512195121952\n",
      "F1, P, R for kur using triple intersection: 0.9027962666680025, 0.9941348973315502, 0.8268292682926829\n",
      "F1, P, R for kur using majority vote intersection: 0.9396662337694389, 0.991869918672307, 0.8926829268292683\n",
      "Time taken: 113.86 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for lat with no threshold: 0.890329546621322, 0.9682726204351554, 0.824\n",
      "F1, P, R for lat with threshold = 1.06: 0.8715393084526477, 0.9764267989953297, 0.787\n",
      "F1, P, R for lat with threshold = 1.20: 0.7263492017086661, 0.9947826086783516, 0.572\n",
      "F1, P, R for lat using translation en-xx: 0.8350168300869778, 0.951406649604202, 0.744\n",
      "F1, P, R for lat using translation xx_en: 0.8609125843795731, 0.9560439560322828, 0.783\n",
      "F1, P, R for lat using triple intersection: 0.8165887801834563, 0.9817415730199194, 0.699\n",
      "F1, P, R for lat using majority vote intersection: 0.887561103125902, 0.9714625445782228, 0.817\n",
      "Time taken: 295.81 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for mal with no threshold: 0.9926900534723718, 0.9970631424229506, 0.9883551673944687\n",
      "F1, P, R for mal with threshold = 1.06: 0.9926900534723718, 0.9970631424229506, 0.9883551673944687\n",
      "F1, P, R for mal with threshold = 1.20: 0.9639097694343153, 0.9968895800778088, 0.9330422125181951\n",
      "F1, P, R for mal using translation en-xx: 0.9800443408919656, 0.995495495480548, 0.9650655021834061\n",
      "F1, P, R for mal using translation xx_en: 0.9823008799493892, 0.9955156950523839, 0.9694323144104804\n",
      "F1, P, R for mal using triple intersection: 0.970786511850928, 0.9999999999845679, 0.9432314410480349\n",
      "F1, P, R for mal using majority vote intersection: 0.9919413869343424, 0.9985250737315852, 0.9854439592430859\n",
      "Time taken: 213.53 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for nds with no threshold: 0.889610384639402, 0.9693396226300786, 0.822\n",
      "F1, P, R for nds with threshold = 1.06: 0.8764542886823091, 0.9826086956399676, 0.791\n",
      "F1, P, R for nds with threshold = 1.20: 0.7481296711105652, 0.993377483427262, 0.6\n",
      "F1, P, R for nds using translation en-xx: 0.8596981504588223, 0.9746514575288384, 0.769\n",
      "F1, P, R for nds using translation xx_en: 0.8382764360288524, 0.9517153748290761, 0.749\n",
      "F1, P, R for nds using triple intersection: 0.8014311221932284, 0.9926144756131076, 0.672\n",
      "F1, P, R for nds using majority vote intersection: 0.8844884438901295, 0.9828850855625564, 0.804\n",
      "Time taken: 268.71 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for nno with no threshold: 0.9750382018230956, 0.9937694703946649, 0.957\n",
      "F1, P, R for nno with threshold = 1.06: 0.9723926330343686, 0.9947698744665819, 0.951\n",
      "F1, P, R for nno with threshold = 1.20: 0.9213362019216857, 0.998831775689266, 0.855\n",
      "F1, P, R for nno using translation en-xx: 0.972972967969989, 0.9927159209053827, 0.954\n",
      "F1, P, R for nno using translation xx_en: 0.9771225166029472, 0.9937952430093713, 0.961\n",
      "F1, P, R for nno using triple intersection: 0.9664775607555083, 0.9978700745367639, 0.937\n",
      "F1, P, R for nno using majority vote intersection: 0.9775967363408978, 0.9958506223963086, 0.96\n",
      "Time taken: 295.76 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for swh with no threshold: 0.9315436191596055, 0.9774647887048601, 0.8897435897435897\n",
      "F1, P, R for swh with threshold = 1.06: 0.9249658885959327, 0.9883381923910105, 0.8692307692307693\n",
      "F1, P, R for swh with threshold = 1.20: 0.8181818133347107, 0.999999999962963, 0.6923076923076923\n",
      "F1, P, R for swh using translation en-xx: 0.949468080100689, 0.9861878452766246, 0.9153846153846154\n",
      "F1, P, R for swh using translation xx_en: 0.9525065912977144, 0.9809782608429082, 0.9256410256410257\n",
      "F1, P, R for swh using triple intersection: 0.9211618207444776, 0.9999999999699699, 0.8538461538461538\n",
      "F1, P, R for swh using majority vote intersection: 0.9546666616619377, 0.994444444416821, 0.9179487179487179\n",
      "Time taken: 105.63 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for tam with no threshold: 0.9301533169708824, 0.9749999999651785, 0.8892508143322475\n",
      "F1, P, R for tam with threshold = 1.06: 0.9280821867781244, 0.9783393501451862, 0.8827361563517915\n",
      "F1, P, R for tam with threshold = 1.20: 0.8888888839233181, 0.9880478087255757, 0.8078175895765473\n",
      "F1, P, R for tam using translation en-xx: 0.9296740944836108, 0.981884057935439, 0.8827361563517915\n",
      "F1, P, R for tam using translation xx_en: 0.9373942420306287, 0.975352112641713, 0.9022801302931596\n",
      "F1, P, R for tam using triple intersection: 0.9195804145913004, 0.9924528301512282, 0.8566775244299675\n",
      "F1, P, R for tam using majority vote intersection: 0.9367521317484112, 0.9856115107559132, 0.8925081433224755\n",
      "Time taken: 89.26 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for tat with no threshold: 0.9197465631206255, 0.9742729306378717, 0.871\n",
      "F1, P, R for tat with threshold = 1.06: 0.9129738337794591, 0.9793814432877505, 0.855\n",
      "F1, P, R for tat with threshold = 1.20: 0.8387096725301557, 0.9891304347691694, 0.728\n",
      "F1, P, R for tat using translation en-xx: 0.8978295344513681, 0.9538807648936571, 0.848\n",
      "F1, P, R for tat using translation xx_en: 0.9392207742234899, 0.9772972972867319, 0.904\n",
      "F1, P, R for tat using triple intersection: 0.898518919891356, 0.9951397326732061, 0.819\n",
      "F1, P, R for tat using majority vote intersection: 0.9245382535856699, 0.9787709497097344, 0.876\n",
      "Time taken: 294.57 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for tel with no threshold: 0.9914163039916928, 0.9956896551294961, 0.9871794871794872\n",
      "F1, P, R for tel with threshold = 1.06: 0.9914163039916928, 0.9956896551294961, 0.9871794871794872\n",
      "F1, P, R for tel with threshold = 1.20: 0.971428566411303, 0.9999999999547511, 0.9444444444444444\n",
      "F1, P, R for tel using translation en-xx: 0.9849462315381663, 0.9913419912990761, 0.9786324786324786\n",
      "F1, P, R for tel using translation xx_en: 0.9761388236133841, 0.9911894272691105, 0.9615384615384616\n",
      "F1, P, R for tel using triple intersection: 0.9737991216186763, 0.9955357142412707, 0.9529914529914529\n",
      "F1, P, R for tel using majority vote intersection: 0.9914163039916928, 0.9956896551294961, 0.9871794871794872\n",
      "Time taken: 68.39 seconds\n",
      "Reading, translating, embedding . . .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining and scoring . . .\n",
      "F1, P, R for tgl with no threshold: 0.9863567408268069, 0.9969356486108587, 0.976\n",
      "F1, P, R for tgl with threshold = 1.06: 0.9842878814632563, 0.9979445015313675, 0.971\n",
      "F1, P, R for tgl with threshold = 1.20: 0.9333333283505779, 0.9999999999885714, 0.875\n",
      "F1, P, R for tgl using translation en-xx: 0.9745676450474031, 0.9917184264907689, 0.958\n",
      "F1, P, R for tgl using translation xx_en: 0.9939698442413626, 0.9989898989798082, 0.989\n",
      "F1, P, R for tgl using triple intersection: 0.9759344548033262, 0.9999999999895068, 0.953\n",
      "F1, P, R for tgl using majority vote intersection: 0.9878542460078842, 0.9999999999897541, 0.976\n",
      "Time taken: 274.97 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for tuk with no threshold: 0.8679245233229925, 0.9583333332762897, 0.7931034482758621\n",
      "F1, P, R for tuk with threshold = 1.06: 0.8729281718453344, 0.9937106917614018, 0.7783251231527094\n",
      "F1, P, R for tuk with threshold = 1.20: 0.5882352899167875, 0.988372092908329, 0.4187192118226601\n",
      "F1, P, R for tuk using translation en-xx: 0.8586956472038929, 0.9575757575177226, 0.7783251231527094\n",
      "F1, P, R for tuk using translation xx_en: 0.9696969646756709, 0.9948186527981959, 0.9458128078817734\n",
      "F1, P, R for tuk using triple intersection: 0.7988165632260776, 0.9999999999259259, 0.6650246305418719\n",
      "F1, P, R for tuk using majority vote intersection: 0.9399477756723409, 0.9999999999444444, 0.8866995073891626\n",
      "Time taken: 57.51 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for uig with no threshold: 0.9541945395181776, 0.9830328737965744, 0.927\n",
      "F1, P, R for uig with threshold = 1.06: 0.9514963830291036, 0.9829424306931456, 0.922\n",
      "F1, P, R for uig with threshold = 1.20: 0.9150892324540328, 0.9964664310836694, 0.846\n",
      "F1, P, R for uig using translation en-xx: 0.9477496070031733, 0.9817792068490698, 0.916\n",
      "F1, P, R for uig using translation xx_en: 0.9545923582612857, 0.9861407249361819, 0.925\n",
      "F1, P, R for uig using triple intersection: 0.93749999501263, 0.9966216216103984, 0.885\n",
      "F1, P, R for uig using majority vote intersection: 0.9540051629593682, 0.9871657753905115, 0.923\n",
      "Time taken: 296.41 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for uzb with no threshold: 0.9108910841152829, 0.9684210526060941, 0.8598130841121495\n",
      "F1, P, R for uzb with threshold = 1.06: 0.906600244076494, 0.9706666666407822, 0.8504672897196262\n",
      "F1, P, R for uzb with threshold = 1.20: 0.8579017215097821, 0.993846153815574, 0.7546728971962616\n",
      "F1, P, R for uzb using translation en-xx: 0.9348093430959697, 0.9870129869873503, 0.8878504672897196\n",
      "F1, P, R for uzb using translation xx_en: 0.9520383642966203, 0.9778325122911864, 0.927570093457944\n",
      "F1, P, R for uzb using triple intersection: 0.9123252809226486, 0.9999999999721448, 0.8387850467289719\n",
      "F1, P, R for uzb using majority vote intersection: 0.935802464140375, 0.9921465968326663, 0.8855140186915887\n",
      "Time taken: 208.30 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for xho with no threshold: 0.9416058343883531, 0.9772727271986914, 0.9084507042253521\n",
      "F1, P, R for xho with threshold = 1.06: 0.9416058343883531, 0.9772727271986914, 0.9084507042253521\n",
      "F1, P, R for xho with threshold = 1.20: 0.875968987264888, 0.9741379309505054, 0.795774647887324\n",
      "F1, P, R for xho using translation en-xx: 0.9293680247207752, 0.9842519684264369, 0.8802816901408451\n",
      "F1, P, R for xho using translation xx_en: 0.9716312006395554, 0.9785714285015306, 0.9647887323943662\n",
      "F1, P, R for xho using triple intersection: 0.9132075421610538, 0.9837398373183952, 0.852112676056338\n",
      "F1, P, R for xho using majority vote intersection: 0.956521734099979, 0.9850746267921585, 0.9295774647887324\n",
      "Time taken: 38.02 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for yid with no threshold: 0.9523809473814071, 0.9873417721394008, 0.9198113207547169\n",
      "F1, P, R for yid with threshold = 1.06: 0.9431396736213656, 0.9909090908962219, 0.8997641509433962\n",
      "F1, P, R for yid with threshold = 1.20: 0.8758256225433515, 0.995495495480548, 0.7818396226415094\n",
      "F1, P, R for yid using translation en-xx: 0.9364589709457039, 0.9818887451360687, 0.8950471698113207\n",
      "F1, P, R for yid using translation xx_en: 0.972455084815748, 0.987834549866328, 0.9575471698113207\n",
      "F1, P, R for yid using triple intersection: 0.9268600202388888, 0.9959349593360984, 0.8667452830188679\n",
      "F1, P, R for yid using majority vote intersection: 0.9585870839155259, 0.9911838790807156, 0.9280660377358491\n",
      "Time taken: 255.21 seconds\n",
      "Total time taken: 8985.95 seconds\n"
     ]
    }
   ],
   "source": [
    "#start = time.time()\n",
    "results_by_lang = []\n",
    "for l3,l2 in zip(lang_codes_3c[35:], lang_codes_2c[34:]):\n",
    "    if l2:\n",
    "        res = retrieveTatoebaComparisons(l3, l2)\n",
    "        results_by_lang.append(res)\n",
    "    else:\n",
    "        results_by_lang.append((l3, \"NULL\"))\n",
    "end = time.time()\n",
    "print(\"Total time taken: {:.2f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_langs = ['ang','arq', 'arz', 'ast', 'awa', 'ber',\n",
    "                 'bre', 'cbk', 'cha', 'csb', 'dtp', 'fao',\n",
    "                 'jav', 'kzj','lfn', 'mhr', 'nov', \n",
    "                 'oci', 'orv', 'pam', 'pms', 'swg', 'tzl',\n",
    "                 'war', 'wuu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for mhr with no threshold: 0.26285713877363265, 0.4599999999885, 0.184\n",
      "F1, P, R for mhr with threshold = 1.06: 0.26303501600122337, 0.5929824561195445, 0.169\n",
      "F1, P, R for mhr with threshold = 1.20: 0.18001800000000007, 0.9009009008197387, 0.1\n",
      "Time taken: 1.33 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for nov with no threshold: 0.8541226165839922, 0.9351851851418895, 0.7859922178988327\n",
      "F1, P, R for nov with threshold = 1.06: 0.82969431820074, 0.9452736317937674, 0.7392996108949417\n",
      "F1, P, R for nov with threshold = 1.20: 0.7725118435605669, 0.9878787878189164, 0.6342412451361867\n",
      "Time taken: 0.32 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for oci with no threshold: 0.7873303118237239, 0.9062499999881999, 0.696\n",
      "F1, P, R for oci with threshold = 1.06: 0.7766763799732017, 0.9314685314555039, 0.666\n",
      "F1, P, R for oci with threshold = 1.20: 0.6577718434527355, 0.9879759518840084, 0.493\n",
      "Time taken: 1.27 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for orv with no threshold: 0.5744680804638146, 0.7858627858464478, 0.45269461077844314\n",
      "F1, P, R for orv with threshold = 1.06: 0.5593625453430772, 0.8357142856943878, 0.42035928143712575\n",
      "F1, P, R for orv with threshold = 1.20: 0.36979670551339205, 0.9646464645977451, 0.22874251497005987\n",
      "Time taken: 1.10 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for pam with no threshold: 0.17925925541705356, 0.34571428570440815, 0.121\n",
      "F1, P, R for pam with threshold = 1.06: 0.17369726761030624, 0.5023923444735697, 0.105\n",
      "F1, P, R for pam with threshold = 1.20: 0.11731843447001931, 0.8513513512363039, 0.063\n",
      "Time taken: 1.26 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for pms with no threshold: 0.7886089764840103, 0.9278350515224785, 0.6857142857142857\n",
      "F1, P, R for pms with threshold = 1.06: 0.7627695751980325, 0.9438202246925893, 0.64\n",
      "F1, P, R for pms with threshold = 1.20: 0.6295336743966954, 0.9838056679763642, 0.46285714285714286\n",
      "Time taken: 0.75 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for swg with no threshold: 0.8041237064183229, 0.9512195120791196, 0.6964285714285714\n",
      "F1, P, R for swg with threshold = 1.06: 0.7700534710903946, 0.9599999998720001, 0.6428571428571429\n",
      "F1, P, R for swg with threshold = 1.20: 0.7231638371196017, 0.9846153844639054, 0.5714285714285714\n",
      "Time taken: 0.15 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for tzl with no threshold: 0.7103825087156979, 0.8227848100224324, 0.625\n",
      "F1, P, R for tzl with threshold = 1.06: 0.7085714237087347, 0.8732394364967269, 0.5961538461538461\n",
      "F1, P, R for tzl with threshold = 1.20: 0.5599999957102223, 0.9130434780623818, 0.40384615384615385\n",
      "Time taken: 0.13 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for war with no threshold: 0.7577854622413193, 0.8950953678352167, 0.657\n",
      "F1, P, R for war with threshold = 1.06: 0.7377347014575973, 0.9354838709533719, 0.609\n",
      "F1, P, R for war with threshold = 1.20: 0.5761024140960102, 0.997536945788238, 0.405\n",
      "Time taken: 1.35 seconds\n",
      "Reading, translating, embedding . . .\n",
      "Mining and scoring . . .\n",
      "F1, P, R for wuu with no threshold: 0.9479708586869655, 0.9880694143059863, 0.911\n",
      "F1, P, R for wuu with threshold = 1.06: 0.939569096426866, 0.9900332225803983, 0.894\n",
      "F1, P, R for wuu with threshold = 1.20: 0.8661772960386183, 0.9948119325422203, 0.767\n",
      "Time taken: 1.29 seconds\n",
      "Total time taken: 10646.20 seconds\n"
     ]
    }
   ],
   "source": [
    "#start = time.time()\n",
    "missing_results = []\n",
    "for l3 in missing_langs[15:]:\n",
    "    res = retrieveTatoebaComparisons(l3, '')\n",
    "    missing_results.append(res)\n",
    "end = time.time()\n",
    "print(\"Total time taken: {:.2f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying majority vote mining on the entire en-kk corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we'll need for this method:\n",
    "# (1) The IDs of the sentence pairs we mined using vanilla (en-kk) margin scoring\n",
    "# (2) The en sentences translated to kk, and the original kk sentences\n",
    "# (3) The kk sentences translated to en, and the original en sentences\n",
    "# (4) The document IDs for (2)\n",
    "# (5) The document IDs for (3)\n",
    "\n",
    "# Steps:\n",
    "# 1. Tokenize and embed translated en sentences, then mine with kk sentences\n",
    "# 2. Repeat step (1) for the translated kk sentences\n",
    "# 3. Use the majority vote method to retrieve sentence pairs \n",
    "#    from the pairwise intersection of of the products of steps (1) \n",
    "#    and (2), plus the original sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in (1-5)\n",
    "\n",
    "# kk sentences translated to en\n",
    "with open('/project/statnlp/ajones/translation.kk-en', 'r') as f:\n",
    "    transl_kk_sents = f.read().splitlines()\n",
    "# original kk sentences\n",
    "with open('/project/statnlp/ajones/orig.kk-en', 'r') as f:\n",
    "    orig_kk_sents = f.read().splitlines()\n",
    "# kk doc IDs\n",
    "with open('/project/statnlp/ajones/id.kk-en', 'r') as f:\n",
    "    kk_ids = f.read().splitlines()\n",
    "    \n",
    "# en sentences translated to kk\n",
    "with open('/project/statnlp/ajones/translation.en-kk', 'r') as f:\n",
    "    transl_en_sents = f.read().splitlines()\n",
    "# original en sentences\n",
    "with open('/project/statnlp/ajones/orig.en-kk', 'r') as f:\n",
    "    orig_en_sents = f.read().splitlines()\n",
    "# en doc IDs\n",
    "with open('/project/statnlp/ajones/id.en-kk', 'r') as f:\n",
    "    en_ids = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_en_sents_by_doc = [[] for _ in range(max([literal_eval(id_) for id_ in en_ids]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "for i in range(len(orig_en_sents)):\n",
    "    orig_en_sents_by_doc[literal_eval(en_ids[i])-1].append(orig_en_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524102\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(d) for d in orig_en_sents_by_doc if len(d) < 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106671\n"
     ]
    }
   ],
   "source": [
    "print(len(orig_en_sents_by_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_en_sents_by_doc = [[] for _ in range(max([literal_eval(id_) for id_ in en_ids]))]\n",
    "for i in range(len(transl_en_sents)):\n",
    "    transl_en_sents_by_doc[literal_eval(en_ids[i])-1].append(transl_en_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_kk_sents_by_doc = [[] for _ in range(max([literal_eval(id_) for id_ in kk_ids]))]\n",
    "for i in range(len(orig_kk_sents)):\n",
    "    orig_kk_sents_by_doc[literal_eval(kk_ids[i])-1].append(orig_kk_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_kk_sents_by_doc = [[] for _ in range(max([literal_eval(id_) for id_ in kk_ids]))]\n",
    "for i in range(len(transl_kk_sents)):\n",
    "    transl_kk_sents_by_doc[literal_eval(kk_ids[i])-1].append(transl_kk_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(orig_kk_sents_by_doc)==len(orig_en_sents_by_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_en_inds = [i for i in range(len(orig_en_sents_by_doc)) if len(orig_en_sents_by_doc[i]) < 30]\n",
    "bad_kk_inds = [i for i in range(len(orig_kk_sents_by_doc)) if len(orig_kk_sents_by_doc[i]) < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_en_sents_by_doc = [orig_en_sents_by_doc[i] for i in range(len(orig_en_sents_by_doc)) \n",
    "                        if i not in bad_en_inds and i not in bad_kk_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200057\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(d) for d in orig_en_sents_by_doc])) # The number of en sentences after removing very short documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_kk_sents_by_doc = [orig_kk_sents_by_doc[i] for i in range(len(orig_kk_sents_by_doc)) \n",
    "                        if i not in bad_en_inds and i not in bad_kk_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410192\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(d) for d in orig_kk_sents_by_doc])) # The number of kk sentences after removing very short documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_en_sents_by_doc = [transl_en_sents_by_doc[i] for i in range(len(transl_en_sents_by_doc)) \n",
    "                        if i not in bad_en_inds and i not in bad_kk_inds]\n",
    "transl_kk_sents_by_doc = [transl_kk_sents_by_doc[i] for i in range(len(transl_kk_sents_by_doc)) \n",
    "                        if i not in bad_en_inds and i not in bad_kk_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining using vanilla margin scoring\n",
    "all_sent_pairs_en_kk_og, all_margin_scores_en_kk_og = [], []\n",
    "for en_doc,kk_doc in zip(orig_en_sents_by_doc, orig_kk_sents_by_doc):\n",
    "    doc_sp, doc_ms = mineSentencePairs(embed(en_doc), embed(kk_doc), '')\n",
    "    all_sent_pairs_en_kk_og.append(doc_sp)\n",
    "    all_margin_scores_en_kk_og.append(doc_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed document 100 of 14622\n",
      "Completed document 200 of 14622\n",
      "Completed document 300 of 14622\n",
      "Completed document 400 of 14622\n",
      "Completed document 500 of 14622\n",
      "Completed document 600 of 14622\n",
      "Completed document 700 of 14622\n"
     ]
    }
   ],
   "source": [
    "# Mining with en-kk\n",
    "i = 0\n",
    "all_sent_pairs_en_to_kk, all_margin_scores_en_to_kk = [], []\n",
    "for transl_en_doc,kk_doc in zip(transl_en_sents_by_doc, orig_kk_sents_by_doc):\n",
    "    i += 1\n",
    "    doc_sp, doc_ms = mineSentencePairs(embed(transl_en_doc), embed(kk_doc), '')\n",
    "    all_sent_pairs_en_to_kk.append(doc_sp)\n",
    "    all_margin_scores_en_to_kk.append(doc_ms)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Completed document {} of 14622\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining with kk-en\n",
    "i = 0\n",
    "all_sent_pairs_kk_to_en, all_margin_scores_kk_to_en = [], []\n",
    "for en_doc,transl_kk_doc in zip(orig_en_sents_by_doc, transl_kk_sents_by_doc):\n",
    "    doc_sp, doc_ms = mineSentencePairs(embed(en_doc), embed(transl_kk_doc), '')\n",
    "    all_sent_pairs_kk_to_en.append(doc_sp)\n",
    "    all_margin_scores_kk_to_en.append(doc_ms)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Completed document {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking pairwise intersection\n",
    "\n",
    "majority_vote_pairs = []\n",
    "for doc1,doc2,doc3 in zip(all_sent_pairs_en_kk_og,all_sent_pairs_en_to_kk,all_sent_pairs_kk_to_en):\n",
    "    int1 = set(doc1)&set(doc2)\n",
    "    int2 = set(doc1)&set(doc3)\n",
    "    int3 = set(doc2)&set(doc3)\n",
    "    maj_vote = list(set(int1|int2|int3))\n",
    "    majority_vote_pairs.append(maj_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_vote_en, maj_vote_kk = [],[]\n",
    "for i in range(len(majority_vote_pairs)):\n",
    "    doc = majority_vote_pairs[i]\n",
    "    if len(doc) > 0:\n",
    "        for pair in doc:\n",
    "            maj_vote_en.append(orig_en_sents_by_doc[i][pair[0]-1])\n",
    "            maj_vote_kk.append(orig_kk_sents_by_doc[i][pair[1]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'en': maj_vote_en, 'kk': maj_vote_kk}).to_csv('Data/en_kk_mined_majority_vote_ALL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154678\n"
     ]
    }
   ],
   "source": [
    "print(len(maj_vote_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_margins = []\n",
    "for i in range(len(majority_vote_pairs)):\n",
    "    doc = majority_vote_pairs[i]\n",
    "    margins = []\n",
    "    for j in range(len(doc)):\n",
    "        if doc[j] in all_sent_pairs_en_kk_og[i]:\n",
    "            ind = all_sent_pairs_en_kk_og[i].index(doc[j])\n",
    "            margins.append(all_margin_scores_en_kk_og[i][ind])\n",
    "        elif doc[j] in all_sent_pairs_en_to_kk[i]:\n",
    "            ind = all_sent_pairs_en_to_kk[i].index(doc[j])\n",
    "            margins.append(all_margin_scores_en_to_kk[i][ind])\n",
    "        else:\n",
    "            ind = all_sent_pairs_kk_to_en[i].index(doc[j])\n",
    "            margins.append(all_margin_scores_kk_to_en[i][ind])\n",
    "    majority_vote_margins.append(margins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19099\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for doc in majority_vote_margins:\n",
    "    for score in doc:\n",
    "        if score > 1.35:\n",
    "            i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_triples = []\n",
    "for i in range(len(majority_vote_pairs)):\n",
    "    doc = majority_vote_pairs[i]\n",
    "    triples = []\n",
    "    for j in range(len(doc)):\n",
    "        if doc[j] in all_sent_pairs_en_kk_og[i]:\n",
    "            ind = all_sent_pairs_en_kk_og[i].index(doc[j])\n",
    "            triples.append((doc[j][0], doc[j][1], all_margin_scores_en_kk_og[i][ind]))\n",
    "        elif doc[j] in all_sent_pairs_en_to_kk[i]:\n",
    "            ind = all_sent_pairs_en_to_kk[i].index(doc[j])\n",
    "            triples.append((doc[j][0], doc[j][1], all_margin_scores_en_to_kk[i][ind]))\n",
    "        else:\n",
    "            ind = all_sent_pairs_kk_to_en[i].index(doc[j])\n",
    "            triples.append((doc[j][0], doc[j][1], all_margin_scores_kk_to_en[i][ind]))\n",
    "    majority_vote_triples.append(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_vote_en_135, maj_vote_kk_135 = [],[]\n",
    "for i in range(len(majority_vote_triples)):\n",
    "    doc = majority_vote_triples[i]\n",
    "    for j in range(len(doc)):\n",
    "        triple = doc[j]\n",
    "        if triple[2] > 1.35:\n",
    "            maj_vote_en_135.append(orig_en_sents_by_doc[i][triple[0]-1])\n",
    "            maj_vote_kk_135.append(orig_kk_sents_by_doc[i][triple[1]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'en': maj_vote_en_135, 'kk': maj_vote_kk_135}).to_csv('Data/en_kk_mined_majority_vote_ALL_1.35.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_kk_old_method = pd.read_csv('/project/statnlp/ajones/en_kk_mined_old_method.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In August 634 the dying Caliph  Abu 20Bakr Abu...</td>\n",
       "      <td>халифа Әбу Бәкір Аллаһның оған игілігі мен сәл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was before the Muslims had entered the ho...</td>\n",
       "      <td>Абдуррахман ибн Ауф Ислам дінін қабылдаған ең ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>His name has also been transliterated as Abdel...</td>\n",
       "      <td>Абдуррахман ибн Ауф басқа аты Абдел Рахман ибн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Abd al Rahman ibn 'Awf 'Abd al Rahman ibn 'Aw...</td>\n",
       "      <td>Абдуррахман ибн Ауф Абдуррахман ибн Ауф, көрне...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He was buried on a hill to the north east of p...</td>\n",
       "      <td>Хиджраның жылы Осман ибн Аффан ның билігі кезі...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  In August 634 the dying Caliph  Abu 20Bakr Abu...   \n",
       "1  This was before the Muslims had entered the ho...   \n",
       "2  His name has also been transliterated as Abdel...   \n",
       "3  'Abd al Rahman ibn 'Awf 'Abd al Rahman ibn 'Aw...   \n",
       "4  He was buried on a hill to the north east of p...   \n",
       "\n",
       "                                              target  \n",
       "0  халифа Әбу Бәкір Аллаһның оған игілігі мен сәл...  \n",
       "1  Абдуррахман ибн Ауф Ислам дінін қабылдаған ең ...  \n",
       "2  Абдуррахман ибн Ауф басқа аты Абдел Рахман ибн...  \n",
       "3  Абдуррахман ибн Ауф Абдуррахман ибн Ауф, көрне...  \n",
       "4  Хиджраның жылы Осман ибн Аффан ның билігі кезі...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_kk_old_method.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430761, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_kk_old_method.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents, kk_sents = list(en_kk_old_method['source']), list(en_kk_old_method['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk_sents = [t for t in kk_sents if type(t)==str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embs, kk_embs = embed(en_sents), embed(kk_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving means in forward direction\n",
      "Time taken to retrieve forward means: 3976.96 seconds\n",
      "Retrieving means in backward direction\n",
      "Completed pair 310000 of 429327 in the backward direction\n",
      "Completed pair 320000 of 429327 in the backward direction\n",
      "Completed pair 330000 of 429327 in the backward direction\n",
      "Completed pair 340000 of 429327 in the backward direction\n",
      "Completed pair 350000 of 429327 in the backward direction\n",
      "Completed pair 360000 of 429327 in the backward direction\n",
      "Completed pair 370000 of 429327 in the backward direction\n",
      "Completed pair 380000 of 429327 in the backward direction\n",
      "Completed pair 390000 of 429327 in the backward direction\n",
      "Completed pair 400000 of 429327 in the backward direction\n",
      "Completed pair 410000 of 429327 in the backward direction\n",
      "Completed pair 420000 of 429327 in the backward direction\n",
      "Mining in the backward direction complete\n"
     ]
    }
   ],
   "source": [
    "old_sent_pairs, old_margin_scores = mineSentencePairs(en_embs, kk_embs, 'en_kk_old_method_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "usp = []\n",
    "for i in range(len(old_sent_pairs)):\n",
    "    if old_margin_scores[i] > 1.35:\n",
    "            usp.append(old_sent_pairs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2745"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(usp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
